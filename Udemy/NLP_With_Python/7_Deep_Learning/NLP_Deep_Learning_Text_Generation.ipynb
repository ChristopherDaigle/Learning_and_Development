{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use one-hot-encoding to use in Keras\n",
    "# Class_0 --> [1,0,0]\n",
    "# Class_1 --> [0,1,0]\n",
    "# Class_2 --> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([5,10,15,20])/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only fit to the training features\n",
    "scaler_obj.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_obj.transform(X_train)\n",
    "scaled_X_test = scaler_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Units: neurons/nodes\n",
    "# Input_dim: number of features/factors/x-variables to be input\n",
    "# activation: function for node\n",
    "model.add(Dense(units=8, input_dim=4, activation='sigmoid'))\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "100/100 - 0s - loss: 1.1119 - acc: 0.3100\n",
      "Epoch 2/700\n",
      "100/100 - 0s - loss: 1.1094 - acc: 0.3100\n",
      "Epoch 3/700\n",
      "100/100 - 0s - loss: 1.1083 - acc: 0.3100\n",
      "Epoch 4/700\n",
      "100/100 - 0s - loss: 1.1065 - acc: 0.3100\n",
      "Epoch 5/700\n",
      "100/100 - 0s - loss: 1.1057 - acc: 0.3100\n",
      "Epoch 6/700\n",
      "100/100 - 0s - loss: 1.1051 - acc: 0.3100\n",
      "Epoch 7/700\n",
      "100/100 - 0s - loss: 1.1041 - acc: 0.3100\n",
      "Epoch 8/700\n",
      "100/100 - 0s - loss: 1.1034 - acc: 0.3100\n",
      "Epoch 9/700\n",
      "100/100 - 0s - loss: 1.1023 - acc: 0.3100\n",
      "Epoch 10/700\n",
      "100/100 - 0s - loss: 1.1016 - acc: 0.3100\n",
      "Epoch 11/700\n",
      "100/100 - 0s - loss: 1.1008 - acc: 0.3000\n",
      "Epoch 12/700\n",
      "100/100 - 0s - loss: 1.1000 - acc: 0.1300\n",
      "Epoch 13/700\n",
      "100/100 - 0s - loss: 1.0996 - acc: 0.3400\n",
      "Epoch 14/700\n",
      "100/100 - 0s - loss: 1.0993 - acc: 0.3400\n",
      "Epoch 15/700\n",
      "100/100 - 0s - loss: 1.0988 - acc: 0.3400\n",
      "Epoch 16/700\n",
      "100/100 - 0s - loss: 1.0985 - acc: 0.3400\n",
      "Epoch 17/700\n",
      "100/100 - 0s - loss: 1.0982 - acc: 0.3400\n",
      "Epoch 18/700\n",
      "100/100 - 0s - loss: 1.0980 - acc: 0.3400\n",
      "Epoch 19/700\n",
      "100/100 - 0s - loss: 1.0978 - acc: 0.3400\n",
      "Epoch 20/700\n",
      "100/100 - 0s - loss: 1.0976 - acc: 0.3400\n",
      "Epoch 21/700\n",
      "100/100 - 0s - loss: 1.0974 - acc: 0.3400\n",
      "Epoch 22/700\n",
      "100/100 - 0s - loss: 1.0971 - acc: 0.3400\n",
      "Epoch 23/700\n",
      "100/100 - 0s - loss: 1.0970 - acc: 0.3400\n",
      "Epoch 24/700\n",
      "100/100 - 0s - loss: 1.0968 - acc: 0.3400\n",
      "Epoch 25/700\n",
      "100/100 - 0s - loss: 1.0966 - acc: 0.3400\n",
      "Epoch 26/700\n",
      "100/100 - 0s - loss: 1.0963 - acc: 0.3400\n",
      "Epoch 27/700\n",
      "100/100 - 0s - loss: 1.0962 - acc: 0.3400\n",
      "Epoch 28/700\n",
      "100/100 - 0s - loss: 1.0961 - acc: 0.3400\n",
      "Epoch 29/700\n",
      "100/100 - 0s - loss: 1.0959 - acc: 0.3400\n",
      "Epoch 30/700\n",
      "100/100 - 0s - loss: 1.0957 - acc: 0.3400\n",
      "Epoch 31/700\n",
      "100/100 - 0s - loss: 1.0956 - acc: 0.3400\n",
      "Epoch 32/700\n",
      "100/100 - 0s - loss: 1.0954 - acc: 0.3400\n",
      "Epoch 33/700\n",
      "100/100 - 0s - loss: 1.0953 - acc: 0.3400\n",
      "Epoch 34/700\n",
      "100/100 - 0s - loss: 1.0950 - acc: 0.3400\n",
      "Epoch 35/700\n",
      "100/100 - 0s - loss: 1.0948 - acc: 0.3400\n",
      "Epoch 36/700\n",
      "100/100 - 0s - loss: 1.0946 - acc: 0.3400\n",
      "Epoch 37/700\n",
      "100/100 - 0s - loss: 1.0942 - acc: 0.3400\n",
      "Epoch 38/700\n",
      "100/100 - 0s - loss: 1.0939 - acc: 0.3400\n",
      "Epoch 39/700\n",
      "100/100 - 0s - loss: 1.0935 - acc: 0.3400\n",
      "Epoch 40/700\n",
      "100/100 - 0s - loss: 1.0930 - acc: 0.3400\n",
      "Epoch 41/700\n",
      "100/100 - 0s - loss: 1.0928 - acc: 0.3400\n",
      "Epoch 42/700\n",
      "100/100 - 0s - loss: 1.0924 - acc: 0.3000\n",
      "Epoch 43/700\n",
      "100/100 - 0s - loss: 1.0917 - acc: 0.2500\n",
      "Epoch 44/700\n",
      "100/100 - 0s - loss: 1.0912 - acc: 0.2500\n",
      "Epoch 45/700\n",
      "100/100 - 0s - loss: 1.0906 - acc: 0.4900\n",
      "Epoch 46/700\n",
      "100/100 - 0s - loss: 1.0903 - acc: 0.5800\n",
      "Epoch 47/700\n",
      "100/100 - 0s - loss: 1.0894 - acc: 0.6600\n",
      "Epoch 48/700\n",
      "100/100 - 0s - loss: 1.0884 - acc: 0.6600\n",
      "Epoch 49/700\n",
      "100/100 - 0s - loss: 1.0875 - acc: 0.6600\n",
      "Epoch 50/700\n",
      "100/100 - 0s - loss: 1.0869 - acc: 0.6500\n",
      "Epoch 51/700\n",
      "100/100 - 0s - loss: 1.0861 - acc: 0.6600\n",
      "Epoch 52/700\n",
      "100/100 - 0s - loss: 1.0849 - acc: 0.6600\n",
      "Epoch 53/700\n",
      "100/100 - 0s - loss: 1.0838 - acc: 0.6600\n",
      "Epoch 54/700\n",
      "100/100 - 0s - loss: 1.0823 - acc: 0.6600\n",
      "Epoch 55/700\n",
      "100/100 - 0s - loss: 1.0807 - acc: 0.6600\n",
      "Epoch 56/700\n",
      "100/100 - 0s - loss: 1.0788 - acc: 0.6600\n",
      "Epoch 57/700\n",
      "100/100 - 0s - loss: 1.0767 - acc: 0.6600\n",
      "Epoch 58/700\n",
      "100/100 - 0s - loss: 1.0747 - acc: 0.6600\n",
      "Epoch 59/700\n",
      "100/100 - 0s - loss: 1.0723 - acc: 0.6600\n",
      "Epoch 60/700\n",
      "100/100 - 0s - loss: 1.0701 - acc: 0.6600\n",
      "Epoch 61/700\n",
      "100/100 - 0s - loss: 1.0679 - acc: 0.6500\n",
      "Epoch 62/700\n",
      "100/100 - 0s - loss: 1.0655 - acc: 0.6500\n",
      "Epoch 63/700\n",
      "100/100 - 0s - loss: 1.0629 - acc: 0.6400\n",
      "Epoch 64/700\n",
      "100/100 - 0s - loss: 1.0601 - acc: 0.6100\n",
      "Epoch 65/700\n",
      "100/100 - 0s - loss: 1.0570 - acc: 0.6000\n",
      "Epoch 66/700\n",
      "100/100 - 0s - loss: 1.0537 - acc: 0.5500\n",
      "Epoch 67/700\n",
      "100/100 - 0s - loss: 1.0505 - acc: 0.5300\n",
      "Epoch 68/700\n",
      "100/100 - 0s - loss: 1.0468 - acc: 0.5600\n",
      "Epoch 69/700\n",
      "100/100 - 0s - loss: 1.0434 - acc: 0.5900\n",
      "Epoch 70/700\n",
      "100/100 - 0s - loss: 1.0395 - acc: 0.5300\n",
      "Epoch 71/700\n",
      "100/100 - 0s - loss: 1.0353 - acc: 0.5100\n",
      "Epoch 72/700\n",
      "100/100 - 0s - loss: 1.0312 - acc: 0.5300\n",
      "Epoch 73/700\n",
      "100/100 - 0s - loss: 1.0270 - acc: 0.5100\n",
      "Epoch 74/700\n",
      "100/100 - 0s - loss: 1.0221 - acc: 0.4900\n",
      "Epoch 75/700\n",
      "100/100 - 0s - loss: 1.0170 - acc: 0.4700\n",
      "Epoch 76/700\n",
      "100/100 - 0s - loss: 1.0120 - acc: 0.4300\n",
      "Epoch 77/700\n",
      "100/100 - 0s - loss: 1.0064 - acc: 0.4200\n",
      "Epoch 78/700\n",
      "100/100 - 0s - loss: 1.0008 - acc: 0.4200\n",
      "Epoch 79/700\n",
      "100/100 - 0s - loss: 0.9956 - acc: 0.4400\n",
      "Epoch 80/700\n",
      "100/100 - 0s - loss: 0.9908 - acc: 0.4900\n",
      "Epoch 81/700\n",
      "100/100 - 0s - loss: 0.9843 - acc: 0.4600\n",
      "Epoch 82/700\n",
      "100/100 - 0s - loss: 0.9783 - acc: 0.4100\n",
      "Epoch 83/700\n",
      "100/100 - 0s - loss: 0.9724 - acc: 0.3800\n",
      "Epoch 84/700\n",
      "100/100 - 0s - loss: 0.9658 - acc: 0.3700\n",
      "Epoch 85/700\n",
      "100/100 - 0s - loss: 0.9585 - acc: 0.4000\n",
      "Epoch 86/700\n",
      "100/100 - 0s - loss: 0.9526 - acc: 0.4200\n",
      "Epoch 87/700\n",
      "100/100 - 0s - loss: 0.9472 - acc: 0.4400\n",
      "Epoch 88/700\n",
      "100/100 - 0s - loss: 0.9404 - acc: 0.4400\n",
      "Epoch 89/700\n",
      "100/100 - 0s - loss: 0.9337 - acc: 0.4200\n",
      "Epoch 90/700\n",
      "100/100 - 0s - loss: 0.9263 - acc: 0.4000\n",
      "Epoch 91/700\n",
      "100/100 - 0s - loss: 0.9193 - acc: 0.4000\n",
      "Epoch 92/700\n",
      "100/100 - 0s - loss: 0.9129 - acc: 0.4100\n",
      "Epoch 93/700\n",
      "100/100 - 0s - loss: 0.9070 - acc: 0.4200\n",
      "Epoch 94/700\n",
      "100/100 - 0s - loss: 0.9003 - acc: 0.4200\n",
      "Epoch 95/700\n",
      "100/100 - 0s - loss: 0.8940 - acc: 0.4200\n",
      "Epoch 96/700\n",
      "100/100 - 0s - loss: 0.8877 - acc: 0.4300\n",
      "Epoch 97/700\n",
      "100/100 - 0s - loss: 0.8812 - acc: 0.4200\n",
      "Epoch 98/700\n",
      "100/100 - 0s - loss: 0.8742 - acc: 0.4000\n",
      "Epoch 99/700\n",
      "100/100 - 0s - loss: 0.8680 - acc: 0.4000\n",
      "Epoch 100/700\n",
      "100/100 - 0s - loss: 0.8629 - acc: 0.4000\n",
      "Epoch 101/700\n",
      "100/100 - 0s - loss: 0.8560 - acc: 0.4100\n",
      "Epoch 102/700\n",
      "100/100 - 0s - loss: 0.8497 - acc: 0.4200\n",
      "Epoch 103/700\n",
      "100/100 - 0s - loss: 0.8442 - acc: 0.4500\n",
      "Epoch 104/700\n",
      "100/100 - 0s - loss: 0.8389 - acc: 0.4700\n",
      "Epoch 105/700\n",
      "100/100 - 0s - loss: 0.8341 - acc: 0.5000\n",
      "Epoch 106/700\n",
      "100/100 - 0s - loss: 0.8291 - acc: 0.5200\n",
      "Epoch 107/700\n",
      "100/100 - 0s - loss: 0.8237 - acc: 0.5200\n",
      "Epoch 108/700\n",
      "100/100 - 0s - loss: 0.8188 - acc: 0.5200\n",
      "Epoch 109/700\n",
      "100/100 - 0s - loss: 0.8137 - acc: 0.5000\n",
      "Epoch 110/700\n",
      "100/100 - 0s - loss: 0.8089 - acc: 0.5000\n",
      "Epoch 111/700\n",
      "100/100 - 0s - loss: 0.8044 - acc: 0.5000\n",
      "Epoch 112/700\n",
      "100/100 - 0s - loss: 0.8003 - acc: 0.4900\n",
      "Epoch 113/700\n",
      "100/100 - 0s - loss: 0.7960 - acc: 0.4800\n",
      "Epoch 114/700\n",
      "100/100 - 0s - loss: 0.7915 - acc: 0.5000\n",
      "Epoch 115/700\n",
      "100/100 - 0s - loss: 0.7873 - acc: 0.5100\n",
      "Epoch 116/700\n",
      "100/100 - 0s - loss: 0.7830 - acc: 0.5400\n",
      "Epoch 117/700\n",
      "100/100 - 0s - loss: 0.7792 - acc: 0.5600\n",
      "Epoch 118/700\n",
      "100/100 - 0s - loss: 0.7752 - acc: 0.5600\n",
      "Epoch 119/700\n",
      "100/100 - 0s - loss: 0.7712 - acc: 0.5800\n",
      "Epoch 120/700\n",
      "100/100 - 0s - loss: 0.7678 - acc: 0.5900\n",
      "Epoch 121/700\n",
      "100/100 - 0s - loss: 0.7641 - acc: 0.5900\n",
      "Epoch 122/700\n",
      "100/100 - 0s - loss: 0.7603 - acc: 0.5900\n",
      "Epoch 123/700\n",
      "100/100 - 0s - loss: 0.7570 - acc: 0.5900\n",
      "Epoch 124/700\n",
      "100/100 - 0s - loss: 0.7536 - acc: 0.5900\n",
      "Epoch 125/700\n",
      "100/100 - 0s - loss: 0.7507 - acc: 0.5800\n",
      "Epoch 126/700\n",
      "100/100 - 0s - loss: 0.7476 - acc: 0.5900\n",
      "Epoch 127/700\n",
      "100/100 - 0s - loss: 0.7442 - acc: 0.5900\n",
      "Epoch 128/700\n",
      "100/100 - 0s - loss: 0.7413 - acc: 0.6000\n",
      "Epoch 129/700\n",
      "100/100 - 0s - loss: 0.7383 - acc: 0.6000\n",
      "Epoch 130/700\n",
      "100/100 - 0s - loss: 0.7355 - acc: 0.6200\n",
      "Epoch 131/700\n",
      "100/100 - 0s - loss: 0.7329 - acc: 0.6300\n",
      "Epoch 132/700\n",
      "100/100 - 0s - loss: 0.7303 - acc: 0.6300\n",
      "Epoch 133/700\n",
      "100/100 - 0s - loss: 0.7276 - acc: 0.6300\n",
      "Epoch 134/700\n",
      "100/100 - 0s - loss: 0.7247 - acc: 0.6300\n",
      "Epoch 135/700\n",
      "100/100 - 0s - loss: 0.7224 - acc: 0.6100\n",
      "Epoch 136/700\n",
      "100/100 - 0s - loss: 0.7200 - acc: 0.6000\n",
      "Epoch 137/700\n",
      "100/100 - 0s - loss: 0.7177 - acc: 0.6000\n",
      "Epoch 138/700\n",
      "100/100 - 0s - loss: 0.7153 - acc: 0.6000\n",
      "Epoch 139/700\n",
      "100/100 - 0s - loss: 0.7128 - acc: 0.6200\n",
      "Epoch 140/700\n",
      "100/100 - 0s - loss: 0.7104 - acc: 0.6300\n",
      "Epoch 141/700\n",
      "100/100 - 0s - loss: 0.7082 - acc: 0.6300\n",
      "Epoch 142/700\n",
      "100/100 - 0s - loss: 0.7061 - acc: 0.6400\n",
      "Epoch 143/700\n",
      "100/100 - 0s - loss: 0.7041 - acc: 0.6400\n",
      "Epoch 144/700\n",
      "100/100 - 0s - loss: 0.7020 - acc: 0.6400\n",
      "Epoch 145/700\n",
      "100/100 - 0s - loss: 0.7001 - acc: 0.6400\n",
      "Epoch 146/700\n",
      "100/100 - 0s - loss: 0.6981 - acc: 0.6400\n",
      "Epoch 147/700\n",
      "100/100 - 0s - loss: 0.6961 - acc: 0.6400\n",
      "Epoch 148/700\n",
      "100/100 - 0s - loss: 0.6942 - acc: 0.6400\n",
      "Epoch 149/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 0s - loss: 0.6924 - acc: 0.6400\n",
      "Epoch 150/700\n",
      "100/100 - 0s - loss: 0.6907 - acc: 0.6400\n",
      "Epoch 151/700\n",
      "100/100 - 0s - loss: 0.6887 - acc: 0.6400\n",
      "Epoch 152/700\n",
      "100/100 - 0s - loss: 0.6871 - acc: 0.6400\n",
      "Epoch 153/700\n",
      "100/100 - 0s - loss: 0.6855 - acc: 0.6300\n",
      "Epoch 154/700\n",
      "100/100 - 0s - loss: 0.6839 - acc: 0.6300\n",
      "Epoch 155/700\n",
      "100/100 - 0s - loss: 0.6824 - acc: 0.6300\n",
      "Epoch 156/700\n",
      "100/100 - 0s - loss: 0.6807 - acc: 0.6400\n",
      "Epoch 157/700\n",
      "100/100 - 0s - loss: 0.6791 - acc: 0.6400\n",
      "Epoch 158/700\n",
      "100/100 - 0s - loss: 0.6774 - acc: 0.6400\n",
      "Epoch 159/700\n",
      "100/100 - 0s - loss: 0.6759 - acc: 0.6400\n",
      "Epoch 160/700\n",
      "100/100 - 0s - loss: 0.6744 - acc: 0.6400\n",
      "Epoch 161/700\n",
      "100/100 - 0s - loss: 0.6730 - acc: 0.7500\n",
      "Epoch 162/700\n",
      "100/100 - 0s - loss: 0.6716 - acc: 0.7500\n",
      "Epoch 163/700\n",
      "100/100 - 0s - loss: 0.6702 - acc: 0.7500\n",
      "Epoch 164/700\n",
      "100/100 - 0s - loss: 0.6688 - acc: 0.7500\n",
      "Epoch 165/700\n",
      "100/100 - 0s - loss: 0.6674 - acc: 0.6900\n",
      "Epoch 166/700\n",
      "100/100 - 0s - loss: 0.6661 - acc: 0.6400\n",
      "Epoch 167/700\n",
      "100/100 - 0s - loss: 0.6649 - acc: 0.6400\n",
      "Epoch 168/700\n",
      "100/100 - 0s - loss: 0.6636 - acc: 0.7100\n",
      "Epoch 169/700\n",
      "100/100 - 0s - loss: 0.6623 - acc: 0.7500\n",
      "Epoch 170/700\n",
      "100/100 - 0s - loss: 0.6610 - acc: 0.7400\n",
      "Epoch 171/700\n",
      "100/100 - 0s - loss: 0.6597 - acc: 0.7400\n",
      "Epoch 172/700\n",
      "100/100 - 0s - loss: 0.6585 - acc: 0.7400\n",
      "Epoch 173/700\n",
      "100/100 - 0s - loss: 0.6573 - acc: 0.6500\n",
      "Epoch 174/700\n",
      "100/100 - 0s - loss: 0.6561 - acc: 0.6500\n",
      "Epoch 175/700\n",
      "100/100 - 0s - loss: 0.6549 - acc: 0.6500\n",
      "Epoch 176/700\n",
      "100/100 - 0s - loss: 0.6537 - acc: 0.6500\n",
      "Epoch 177/700\n",
      "100/100 - 0s - loss: 0.6526 - acc: 0.6500\n",
      "Epoch 178/700\n",
      "100/100 - 0s - loss: 0.6514 - acc: 0.6500\n",
      "Epoch 179/700\n",
      "100/100 - 0s - loss: 0.6504 - acc: 0.6500\n",
      "Epoch 180/700\n",
      "100/100 - 0s - loss: 0.6493 - acc: 0.6500\n",
      "Epoch 181/700\n",
      "100/100 - 0s - loss: 0.6482 - acc: 0.6500\n",
      "Epoch 182/700\n",
      "100/100 - 0s - loss: 0.6471 - acc: 0.6500\n",
      "Epoch 183/700\n",
      "100/100 - 0s - loss: 0.6460 - acc: 0.6500\n",
      "Epoch 184/700\n",
      "100/100 - 0s - loss: 0.6450 - acc: 0.6500\n",
      "Epoch 185/700\n",
      "100/100 - 0s - loss: 0.6439 - acc: 0.6500\n",
      "Epoch 186/700\n",
      "100/100 - 0s - loss: 0.6429 - acc: 0.6500\n",
      "Epoch 187/700\n",
      "100/100 - 0s - loss: 0.6419 - acc: 0.6500\n",
      "Epoch 188/700\n",
      "100/100 - 0s - loss: 0.6410 - acc: 0.6500\n",
      "Epoch 189/700\n",
      "100/100 - 0s - loss: 0.6400 - acc: 0.6500\n",
      "Epoch 190/700\n",
      "100/100 - 0s - loss: 0.6390 - acc: 0.6500\n",
      "Epoch 191/700\n",
      "100/100 - 0s - loss: 0.6380 - acc: 0.6600\n",
      "Epoch 192/700\n",
      "100/100 - 0s - loss: 0.6371 - acc: 0.6600\n",
      "Epoch 193/700\n",
      "100/100 - 0s - loss: 0.6362 - acc: 0.6600\n",
      "Epoch 194/700\n",
      "100/100 - 0s - loss: 0.6353 - acc: 0.6600\n",
      "Epoch 195/700\n",
      "100/100 - 0s - loss: 0.6344 - acc: 0.6500\n",
      "Epoch 196/700\n",
      "100/100 - 0s - loss: 0.6336 - acc: 0.6800\n",
      "Epoch 197/700\n",
      "100/100 - 0s - loss: 0.6327 - acc: 0.7400\n",
      "Epoch 198/700\n",
      "100/100 - 0s - loss: 0.6319 - acc: 0.7300\n",
      "Epoch 199/700\n",
      "100/100 - 0s - loss: 0.6310 - acc: 0.7300\n",
      "Epoch 200/700\n",
      "100/100 - 0s - loss: 0.6301 - acc: 0.7300\n",
      "Epoch 201/700\n",
      "100/100 - 0s - loss: 0.6292 - acc: 0.7400\n",
      "Epoch 202/700\n",
      "100/100 - 0s - loss: 0.6283 - acc: 0.7300\n",
      "Epoch 203/700\n",
      "100/100 - 0s - loss: 0.6275 - acc: 0.7300\n",
      "Epoch 204/700\n",
      "100/100 - 0s - loss: 0.6266 - acc: 0.7200\n",
      "Epoch 205/700\n",
      "100/100 - 0s - loss: 0.6258 - acc: 0.7300\n",
      "Epoch 206/700\n",
      "100/100 - 0s - loss: 0.6250 - acc: 0.7300\n",
      "Epoch 207/700\n",
      "100/100 - 0s - loss: 0.6243 - acc: 0.7400\n",
      "Epoch 208/700\n",
      "100/100 - 0s - loss: 0.6235 - acc: 0.6800\n",
      "Epoch 209/700\n",
      "100/100 - 0s - loss: 0.6227 - acc: 0.6500\n",
      "Epoch 210/700\n",
      "100/100 - 0s - loss: 0.6218 - acc: 0.6600\n",
      "Epoch 211/700\n",
      "100/100 - 0s - loss: 0.6210 - acc: 0.6600\n",
      "Epoch 212/700\n",
      "100/100 - 0s - loss: 0.6202 - acc: 0.6500\n",
      "Epoch 213/700\n",
      "100/100 - 0s - loss: 0.6194 - acc: 0.6600\n",
      "Epoch 214/700\n",
      "100/100 - 0s - loss: 0.6186 - acc: 0.6600\n",
      "Epoch 215/700\n",
      "100/100 - 0s - loss: 0.6179 - acc: 0.6600\n",
      "Epoch 216/700\n",
      "100/100 - 0s - loss: 0.6171 - acc: 0.6600\n",
      "Epoch 217/700\n",
      "100/100 - 0s - loss: 0.6164 - acc: 0.6600\n",
      "Epoch 218/700\n",
      "100/100 - 0s - loss: 0.6157 - acc: 0.6600\n",
      "Epoch 219/700\n",
      "100/100 - 0s - loss: 0.6150 - acc: 0.6600\n",
      "Epoch 220/700\n",
      "100/100 - 0s - loss: 0.6142 - acc: 0.6600\n",
      "Epoch 221/700\n",
      "100/100 - 0s - loss: 0.6135 - acc: 0.6600\n",
      "Epoch 222/700\n",
      "100/100 - 0s - loss: 0.6129 - acc: 0.6600\n",
      "Epoch 223/700\n",
      "100/100 - 0s - loss: 0.6122 - acc: 0.6600\n",
      "Epoch 224/700\n",
      "100/100 - 0s - loss: 0.6115 - acc: 0.6600\n",
      "Epoch 225/700\n",
      "100/100 - 0s - loss: 0.6108 - acc: 0.7100\n",
      "Epoch 226/700\n",
      "100/100 - 0s - loss: 0.6101 - acc: 0.7100\n",
      "Epoch 227/700\n",
      "100/100 - 0s - loss: 0.6095 - acc: 0.7100\n",
      "Epoch 228/700\n",
      "100/100 - 0s - loss: 0.6088 - acc: 0.7100\n",
      "Epoch 229/700\n",
      "100/100 - 0s - loss: 0.6082 - acc: 0.7100\n",
      "Epoch 230/700\n",
      "100/100 - 0s - loss: 0.6075 - acc: 0.7100\n",
      "Epoch 231/700\n",
      "100/100 - 0s - loss: 0.6068 - acc: 0.7100\n",
      "Epoch 232/700\n",
      "100/100 - 0s - loss: 0.6062 - acc: 0.7100\n",
      "Epoch 233/700\n",
      "100/100 - 0s - loss: 0.6055 - acc: 0.7100\n",
      "Epoch 234/700\n",
      "100/100 - 0s - loss: 0.6049 - acc: 0.7100\n",
      "Epoch 235/700\n",
      "100/100 - 0s - loss: 0.6043 - acc: 0.7100\n",
      "Epoch 236/700\n",
      "100/100 - 0s - loss: 0.6036 - acc: 0.6900\n",
      "Epoch 237/700\n",
      "100/100 - 0s - loss: 0.6030 - acc: 0.6600\n",
      "Epoch 238/700\n",
      "100/100 - 0s - loss: 0.6024 - acc: 0.7100\n",
      "Epoch 239/700\n",
      "100/100 - 0s - loss: 0.6018 - acc: 0.7100\n",
      "Epoch 240/700\n",
      "100/100 - 0s - loss: 0.6012 - acc: 0.7100\n",
      "Epoch 241/700\n",
      "100/100 - 0s - loss: 0.6007 - acc: 0.7100\n",
      "Epoch 242/700\n",
      "100/100 - 0s - loss: 0.6000 - acc: 0.6600\n",
      "Epoch 243/700\n",
      "100/100 - 0s - loss: 0.5995 - acc: 0.6600\n",
      "Epoch 244/700\n",
      "100/100 - 0s - loss: 0.5989 - acc: 0.6600\n",
      "Epoch 245/700\n",
      "100/100 - 0s - loss: 0.5983 - acc: 0.6600\n",
      "Epoch 246/700\n",
      "100/100 - 0s - loss: 0.5977 - acc: 0.6600\n",
      "Epoch 247/700\n",
      "100/100 - 0s - loss: 0.5972 - acc: 0.6600\n",
      "Epoch 248/700\n",
      "100/100 - 0s - loss: 0.5966 - acc: 0.6600\n",
      "Epoch 249/700\n",
      "100/100 - 0s - loss: 0.5961 - acc: 0.6600\n",
      "Epoch 250/700\n",
      "100/100 - 0s - loss: 0.5956 - acc: 0.6600\n",
      "Epoch 251/700\n",
      "100/100 - 0s - loss: 0.5951 - acc: 0.6600\n",
      "Epoch 252/700\n",
      "100/100 - 0s - loss: 0.5945 - acc: 0.6600\n",
      "Epoch 253/700\n",
      "100/100 - 0s - loss: 0.5940 - acc: 0.6600\n",
      "Epoch 254/700\n",
      "100/100 - 0s - loss: 0.5935 - acc: 0.6600\n",
      "Epoch 255/700\n",
      "100/100 - 0s - loss: 0.5929 - acc: 0.6600\n",
      "Epoch 256/700\n",
      "100/100 - 0s - loss: 0.5924 - acc: 0.6600\n",
      "Epoch 257/700\n",
      "100/100 - 0s - loss: 0.5918 - acc: 0.6600\n",
      "Epoch 258/700\n",
      "100/100 - 0s - loss: 0.5914 - acc: 0.6600\n",
      "Epoch 259/700\n",
      "100/100 - 0s - loss: 0.5908 - acc: 0.6600\n",
      "Epoch 260/700\n",
      "100/100 - 0s - loss: 0.5903 - acc: 0.6600\n",
      "Epoch 261/700\n",
      "100/100 - 0s - loss: 0.5898 - acc: 0.6600\n",
      "Epoch 262/700\n",
      "100/100 - 0s - loss: 0.5892 - acc: 0.6600\n",
      "Epoch 263/700\n",
      "100/100 - 0s - loss: 0.5887 - acc: 0.6600\n",
      "Epoch 264/700\n",
      "100/100 - 0s - loss: 0.5882 - acc: 0.6600\n",
      "Epoch 265/700\n",
      "100/100 - 0s - loss: 0.5876 - acc: 0.6600\n",
      "Epoch 266/700\n",
      "100/100 - 0s - loss: 0.5871 - acc: 0.6600\n",
      "Epoch 267/700\n",
      "100/100 - 0s - loss: 0.5866 - acc: 0.6600\n",
      "Epoch 268/700\n",
      "100/100 - 0s - loss: 0.5861 - acc: 0.6600\n",
      "Epoch 269/700\n",
      "100/100 - 0s - loss: 0.5856 - acc: 0.6600\n",
      "Epoch 270/700\n",
      "100/100 - 0s - loss: 0.5852 - acc: 0.6600\n",
      "Epoch 271/700\n",
      "100/100 - 0s - loss: 0.5847 - acc: 0.6600\n",
      "Epoch 272/700\n",
      "100/100 - 0s - loss: 0.5842 - acc: 0.6600\n",
      "Epoch 273/700\n",
      "100/100 - 0s - loss: 0.5837 - acc: 0.6600\n",
      "Epoch 274/700\n",
      "100/100 - 0s - loss: 0.5833 - acc: 0.6600\n",
      "Epoch 275/700\n",
      "100/100 - 0s - loss: 0.5828 - acc: 0.6600\n",
      "Epoch 276/700\n",
      "100/100 - 0s - loss: 0.5823 - acc: 0.6600\n",
      "Epoch 277/700\n",
      "100/100 - 0s - loss: 0.5818 - acc: 0.6600\n",
      "Epoch 278/700\n",
      "100/100 - 0s - loss: 0.5814 - acc: 0.6600\n",
      "Epoch 279/700\n",
      "100/100 - 0s - loss: 0.5810 - acc: 0.6600\n",
      "Epoch 280/700\n",
      "100/100 - 0s - loss: 0.5806 - acc: 0.6600\n",
      "Epoch 281/700\n",
      "100/100 - 0s - loss: 0.5801 - acc: 0.6600\n",
      "Epoch 282/700\n",
      "100/100 - 0s - loss: 0.5797 - acc: 0.6600\n",
      "Epoch 283/700\n",
      "100/100 - 0s - loss: 0.5793 - acc: 0.6600\n",
      "Epoch 284/700\n",
      "100/100 - 0s - loss: 0.5789 - acc: 0.6600\n",
      "Epoch 285/700\n",
      "100/100 - 0s - loss: 0.5784 - acc: 0.6600\n",
      "Epoch 286/700\n",
      "100/100 - 0s - loss: 0.5780 - acc: 0.6600\n",
      "Epoch 287/700\n",
      "100/100 - 0s - loss: 0.5776 - acc: 0.6600\n",
      "Epoch 288/700\n",
      "100/100 - 0s - loss: 0.5772 - acc: 0.6600\n",
      "Epoch 289/700\n",
      "100/100 - 0s - loss: 0.5767 - acc: 0.6600\n",
      "Epoch 290/700\n",
      "100/100 - 0s - loss: 0.5763 - acc: 0.6600\n",
      "Epoch 291/700\n",
      "100/100 - 0s - loss: 0.5759 - acc: 0.6600\n",
      "Epoch 292/700\n",
      "100/100 - 0s - loss: 0.5755 - acc: 0.6600\n",
      "Epoch 293/700\n",
      "100/100 - 0s - loss: 0.5750 - acc: 0.6600\n",
      "Epoch 294/700\n",
      "100/100 - 0s - loss: 0.5747 - acc: 0.6600\n",
      "Epoch 295/700\n",
      "100/100 - 0s - loss: 0.5743 - acc: 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/700\n",
      "100/100 - 0s - loss: 0.5738 - acc: 0.6600\n",
      "Epoch 297/700\n",
      "100/100 - 0s - loss: 0.5735 - acc: 0.6600\n",
      "Epoch 298/700\n",
      "100/100 - 0s - loss: 0.5730 - acc: 0.6600\n",
      "Epoch 299/700\n",
      "100/100 - 0s - loss: 0.5726 - acc: 0.6600\n",
      "Epoch 300/700\n",
      "100/100 - 0s - loss: 0.5722 - acc: 0.6600\n",
      "Epoch 301/700\n",
      "100/100 - 0s - loss: 0.5718 - acc: 0.6600\n",
      "Epoch 302/700\n",
      "100/100 - 0s - loss: 0.5714 - acc: 0.6600\n",
      "Epoch 303/700\n",
      "100/100 - 0s - loss: 0.5710 - acc: 0.6600\n",
      "Epoch 304/700\n",
      "100/100 - 0s - loss: 0.5706 - acc: 0.6600\n",
      "Epoch 305/700\n",
      "100/100 - 0s - loss: 0.5702 - acc: 0.6600\n",
      "Epoch 306/700\n",
      "100/100 - 0s - loss: 0.5698 - acc: 0.6600\n",
      "Epoch 307/700\n",
      "100/100 - 0s - loss: 0.5695 - acc: 0.6600\n",
      "Epoch 308/700\n",
      "100/100 - 0s - loss: 0.5691 - acc: 0.6600\n",
      "Epoch 309/700\n",
      "100/100 - 0s - loss: 0.5687 - acc: 0.6600\n",
      "Epoch 310/700\n",
      "100/100 - 0s - loss: 0.5683 - acc: 0.6600\n",
      "Epoch 311/700\n",
      "100/100 - 0s - loss: 0.5679 - acc: 0.6600\n",
      "Epoch 312/700\n",
      "100/100 - 0s - loss: 0.5676 - acc: 0.6600\n",
      "Epoch 313/700\n",
      "100/100 - 0s - loss: 0.5672 - acc: 0.6600\n",
      "Epoch 314/700\n",
      "100/100 - 0s - loss: 0.5669 - acc: 0.6600\n",
      "Epoch 315/700\n",
      "100/100 - 0s - loss: 0.5665 - acc: 0.6600\n",
      "Epoch 316/700\n",
      "100/100 - 0s - loss: 0.5662 - acc: 0.6600\n",
      "Epoch 317/700\n",
      "100/100 - 0s - loss: 0.5658 - acc: 0.6600\n",
      "Epoch 318/700\n",
      "100/100 - 0s - loss: 0.5655 - acc: 0.6600\n",
      "Epoch 319/700\n",
      "100/100 - 0s - loss: 0.5651 - acc: 0.6600\n",
      "Epoch 320/700\n",
      "100/100 - 0s - loss: 0.5647 - acc: 0.6600\n",
      "Epoch 321/700\n",
      "100/100 - 0s - loss: 0.5643 - acc: 0.6600\n",
      "Epoch 322/700\n",
      "100/100 - 0s - loss: 0.5640 - acc: 0.6600\n",
      "Epoch 323/700\n",
      "100/100 - 0s - loss: 0.5637 - acc: 0.6600\n",
      "Epoch 324/700\n",
      "100/100 - 0s - loss: 0.5633 - acc: 0.6600\n",
      "Epoch 325/700\n",
      "100/100 - 0s - loss: 0.5629 - acc: 0.6600\n",
      "Epoch 326/700\n",
      "100/100 - 0s - loss: 0.5626 - acc: 0.6600\n",
      "Epoch 327/700\n",
      "100/100 - 0s - loss: 0.5623 - acc: 0.6600\n",
      "Epoch 328/700\n",
      "100/100 - 0s - loss: 0.5619 - acc: 0.6600\n",
      "Epoch 329/700\n",
      "100/100 - 0s - loss: 0.5616 - acc: 0.6600\n",
      "Epoch 330/700\n",
      "100/100 - 0s - loss: 0.5612 - acc: 0.6600\n",
      "Epoch 331/700\n",
      "100/100 - 0s - loss: 0.5609 - acc: 0.6600\n",
      "Epoch 332/700\n",
      "100/100 - 0s - loss: 0.5606 - acc: 0.6600\n",
      "Epoch 333/700\n",
      "100/100 - 0s - loss: 0.5602 - acc: 0.6600\n",
      "Epoch 334/700\n",
      "100/100 - 0s - loss: 0.5599 - acc: 0.6600\n",
      "Epoch 335/700\n",
      "100/100 - 0s - loss: 0.5596 - acc: 0.6600\n",
      "Epoch 336/700\n",
      "100/100 - 0s - loss: 0.5592 - acc: 0.6600\n",
      "Epoch 337/700\n",
      "100/100 - 0s - loss: 0.5590 - acc: 0.7100\n",
      "Epoch 338/700\n",
      "100/100 - 0s - loss: 0.5586 - acc: 0.7100\n",
      "Epoch 339/700\n",
      "100/100 - 0s - loss: 0.5583 - acc: 0.7100\n",
      "Epoch 340/700\n",
      "100/100 - 0s - loss: 0.5580 - acc: 0.7100\n",
      "Epoch 341/700\n",
      "100/100 - 0s - loss: 0.5577 - acc: 0.7100\n",
      "Epoch 342/700\n",
      "100/100 - 0s - loss: 0.5574 - acc: 0.7100\n",
      "Epoch 343/700\n",
      "100/100 - 0s - loss: 0.5571 - acc: 0.7100\n",
      "Epoch 344/700\n",
      "100/100 - 0s - loss: 0.5568 - acc: 0.7100\n",
      "Epoch 345/700\n",
      "100/100 - 0s - loss: 0.5566 - acc: 0.7100\n",
      "Epoch 346/700\n",
      "100/100 - 0s - loss: 0.5563 - acc: 0.7100\n",
      "Epoch 347/700\n",
      "100/100 - 0s - loss: 0.5560 - acc: 0.7100\n",
      "Epoch 348/700\n",
      "100/100 - 0s - loss: 0.5557 - acc: 0.7100\n",
      "Epoch 349/700\n",
      "100/100 - 0s - loss: 0.5553 - acc: 0.7100\n",
      "Epoch 350/700\n",
      "100/100 - 0s - loss: 0.5551 - acc: 0.7100\n",
      "Epoch 351/700\n",
      "100/100 - 0s - loss: 0.5547 - acc: 0.7100\n",
      "Epoch 352/700\n",
      "100/100 - 0s - loss: 0.5545 - acc: 0.7100\n",
      "Epoch 353/700\n",
      "100/100 - 0s - loss: 0.5542 - acc: 0.7100\n",
      "Epoch 354/700\n",
      "100/100 - 0s - loss: 0.5539 - acc: 0.7100\n",
      "Epoch 355/700\n",
      "100/100 - 0s - loss: 0.5535 - acc: 0.7100\n",
      "Epoch 356/700\n",
      "100/100 - 0s - loss: 0.5532 - acc: 0.7100\n",
      "Epoch 357/700\n",
      "100/100 - 0s - loss: 0.5529 - acc: 0.7200\n",
      "Epoch 358/700\n",
      "100/100 - 0s - loss: 0.5525 - acc: 0.7300\n",
      "Epoch 359/700\n",
      "100/100 - 0s - loss: 0.5523 - acc: 0.7400\n",
      "Epoch 360/700\n",
      "100/100 - 0s - loss: 0.5520 - acc: 0.7400\n",
      "Epoch 361/700\n",
      "100/100 - 0s - loss: 0.5517 - acc: 0.7500\n",
      "Epoch 362/700\n",
      "100/100 - 0s - loss: 0.5513 - acc: 0.7500\n",
      "Epoch 363/700\n",
      "100/100 - 0s - loss: 0.5510 - acc: 0.7500\n",
      "Epoch 364/700\n",
      "100/100 - 0s - loss: 0.5507 - acc: 0.7500\n",
      "Epoch 365/700\n",
      "100/100 - 0s - loss: 0.5503 - acc: 0.7600\n",
      "Epoch 366/700\n",
      "100/100 - 0s - loss: 0.5500 - acc: 0.7600\n",
      "Epoch 367/700\n",
      "100/100 - 0s - loss: 0.5496 - acc: 0.7600\n",
      "Epoch 368/700\n",
      "100/100 - 0s - loss: 0.5492 - acc: 0.7600\n",
      "Epoch 369/700\n",
      "100/100 - 0s - loss: 0.5489 - acc: 0.7600\n",
      "Epoch 370/700\n",
      "100/100 - 0s - loss: 0.5485 - acc: 0.7600\n",
      "Epoch 371/700\n",
      "100/100 - 0s - loss: 0.5481 - acc: 0.7700\n",
      "Epoch 372/700\n",
      "100/100 - 0s - loss: 0.5477 - acc: 0.7700\n",
      "Epoch 373/700\n",
      "100/100 - 0s - loss: 0.5473 - acc: 0.7800\n",
      "Epoch 374/700\n",
      "100/100 - 0s - loss: 0.5468 - acc: 0.7800\n",
      "Epoch 375/700\n",
      "100/100 - 0s - loss: 0.5464 - acc: 0.7900\n",
      "Epoch 376/700\n",
      "100/100 - 0s - loss: 0.5460 - acc: 0.7900\n",
      "Epoch 377/700\n",
      "100/100 - 0s - loss: 0.5455 - acc: 0.7900\n",
      "Epoch 378/700\n",
      "100/100 - 0s - loss: 0.5451 - acc: 0.7900\n",
      "Epoch 379/700\n",
      "100/100 - 0s - loss: 0.5446 - acc: 0.7800\n",
      "Epoch 380/700\n",
      "100/100 - 0s - loss: 0.5442 - acc: 0.7800\n",
      "Epoch 381/700\n",
      "100/100 - 0s - loss: 0.5437 - acc: 0.7900\n",
      "Epoch 382/700\n",
      "100/100 - 0s - loss: 0.5430 - acc: 0.7900\n",
      "Epoch 383/700\n",
      "100/100 - 0s - loss: 0.5424 - acc: 0.8200\n",
      "Epoch 384/700\n",
      "100/100 - 0s - loss: 0.5415 - acc: 0.8300\n",
      "Epoch 385/700\n",
      "100/100 - 0s - loss: 0.5405 - acc: 0.8400\n",
      "Epoch 386/700\n",
      "100/100 - 0s - loss: 0.5396 - acc: 0.8700\n",
      "Epoch 387/700\n",
      "100/100 - 0s - loss: 0.5386 - acc: 0.8800\n",
      "Epoch 388/700\n",
      "100/100 - 0s - loss: 0.5377 - acc: 0.8800\n",
      "Epoch 389/700\n",
      "100/100 - 0s - loss: 0.5366 - acc: 0.8800\n",
      "Epoch 390/700\n",
      "100/100 - 0s - loss: 0.5357 - acc: 0.8800\n",
      "Epoch 391/700\n",
      "100/100 - 0s - loss: 0.5345 - acc: 0.8900\n",
      "Epoch 392/700\n",
      "100/100 - 0s - loss: 0.5334 - acc: 0.8900\n",
      "Epoch 393/700\n",
      "100/100 - 0s - loss: 0.5323 - acc: 0.8900\n",
      "Epoch 394/700\n",
      "100/100 - 0s - loss: 0.5308 - acc: 0.8900\n",
      "Epoch 395/700\n",
      "100/100 - 0s - loss: 0.5294 - acc: 0.9000\n",
      "Epoch 396/700\n",
      "100/100 - 0s - loss: 0.5281 - acc: 0.9000\n",
      "Epoch 397/700\n",
      "100/100 - 0s - loss: 0.5265 - acc: 0.9000\n",
      "Epoch 398/700\n",
      "100/100 - 0s - loss: 0.5250 - acc: 0.9100\n",
      "Epoch 399/700\n",
      "100/100 - 0s - loss: 0.5235 - acc: 0.9200\n",
      "Epoch 400/700\n",
      "100/100 - 0s - loss: 0.5219 - acc: 0.9200\n",
      "Epoch 401/700\n",
      "100/100 - 0s - loss: 0.5203 - acc: 0.9200\n",
      "Epoch 402/700\n",
      "100/100 - 0s - loss: 0.5185 - acc: 0.9200\n",
      "Epoch 403/700\n",
      "100/100 - 0s - loss: 0.5167 - acc: 0.9100\n",
      "Epoch 404/700\n",
      "100/100 - 0s - loss: 0.5149 - acc: 0.9200\n",
      "Epoch 405/700\n",
      "100/100 - 0s - loss: 0.5128 - acc: 0.9200\n",
      "Epoch 406/700\n",
      "100/100 - 0s - loss: 0.5111 - acc: 0.9200\n",
      "Epoch 407/700\n",
      "100/100 - 0s - loss: 0.5097 - acc: 0.9100\n",
      "Epoch 408/700\n",
      "100/100 - 0s - loss: 0.5072 - acc: 0.9300\n",
      "Epoch 409/700\n",
      "100/100 - 0s - loss: 0.5045 - acc: 0.9300\n",
      "Epoch 410/700\n",
      "100/100 - 0s - loss: 0.5026 - acc: 0.9300\n",
      "Epoch 411/700\n",
      "100/100 - 0s - loss: 0.5002 - acc: 0.9300\n",
      "Epoch 412/700\n",
      "100/100 - 0s - loss: 0.4976 - acc: 0.9300\n",
      "Epoch 413/700\n",
      "100/100 - 0s - loss: 0.4959 - acc: 0.9200\n",
      "Epoch 414/700\n",
      "100/100 - 0s - loss: 0.4955 - acc: 0.9100\n",
      "Epoch 415/700\n",
      "100/100 - 0s - loss: 0.4931 - acc: 0.9200\n",
      "Epoch 416/700\n",
      "100/100 - 0s - loss: 0.4898 - acc: 0.9300\n",
      "Epoch 417/700\n",
      "100/100 - 0s - loss: 0.4868 - acc: 0.9200\n",
      "Epoch 418/700\n",
      "100/100 - 0s - loss: 0.4843 - acc: 0.9200\n",
      "Epoch 419/700\n",
      "100/100 - 0s - loss: 0.4824 - acc: 0.9200\n",
      "Epoch 420/700\n",
      "100/100 - 0s - loss: 0.4799 - acc: 0.9200\n",
      "Epoch 421/700\n",
      "100/100 - 0s - loss: 0.4776 - acc: 0.9300\n",
      "Epoch 422/700\n",
      "100/100 - 0s - loss: 0.4755 - acc: 0.9300\n",
      "Epoch 423/700\n",
      "100/100 - 0s - loss: 0.4733 - acc: 0.9300\n",
      "Epoch 424/700\n",
      "100/100 - 0s - loss: 0.4719 - acc: 0.9200\n",
      "Epoch 425/700\n",
      "100/100 - 0s - loss: 0.4711 - acc: 0.9300\n",
      "Epoch 426/700\n",
      "100/100 - 0s - loss: 0.4686 - acc: 0.9200\n",
      "Epoch 427/700\n",
      "100/100 - 0s - loss: 0.4649 - acc: 0.9300\n",
      "Epoch 428/700\n",
      "100/100 - 0s - loss: 0.4641 - acc: 0.9200\n",
      "Epoch 429/700\n",
      "100/100 - 0s - loss: 0.4621 - acc: 0.9200\n",
      "Epoch 430/700\n",
      "100/100 - 0s - loss: 0.4596 - acc: 0.9200\n",
      "Epoch 431/700\n",
      "100/100 - 0s - loss: 0.4577 - acc: 0.9300\n",
      "Epoch 432/700\n",
      "100/100 - 0s - loss: 0.4562 - acc: 0.9300\n",
      "Epoch 433/700\n",
      "100/100 - 0s - loss: 0.4544 - acc: 0.9300\n",
      "Epoch 434/700\n",
      "100/100 - 0s - loss: 0.4523 - acc: 0.9300\n",
      "Epoch 435/700\n",
      "100/100 - 0s - loss: 0.4500 - acc: 0.9300\n",
      "Epoch 436/700\n",
      "100/100 - 0s - loss: 0.4479 - acc: 0.9200\n",
      "Epoch 437/700\n",
      "100/100 - 0s - loss: 0.4486 - acc: 0.9200\n",
      "Epoch 438/700\n",
      "100/100 - 0s - loss: 0.4470 - acc: 0.9200\n",
      "Epoch 439/700\n",
      "100/100 - 0s - loss: 0.4431 - acc: 0.9200\n",
      "Epoch 440/700\n",
      "100/100 - 0s - loss: 0.4405 - acc: 0.9300\n",
      "Epoch 441/700\n",
      "100/100 - 0s - loss: 0.4409 - acc: 0.9300\n",
      "Epoch 442/700\n",
      "100/100 - 0s - loss: 0.4402 - acc: 0.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/700\n",
      "100/100 - 0s - loss: 0.4392 - acc: 0.9300\n",
      "Epoch 444/700\n",
      "100/100 - 0s - loss: 0.4382 - acc: 0.9300\n",
      "Epoch 445/700\n",
      "100/100 - 0s - loss: 0.4358 - acc: 0.9300\n",
      "Epoch 446/700\n",
      "100/100 - 0s - loss: 0.4322 - acc: 0.9400\n",
      "Epoch 447/700\n",
      "100/100 - 0s - loss: 0.4286 - acc: 0.9300\n",
      "Epoch 448/700\n",
      "100/100 - 0s - loss: 0.4274 - acc: 0.9300\n",
      "Epoch 449/700\n",
      "100/100 - 0s - loss: 0.4276 - acc: 0.9300\n",
      "Epoch 450/700\n",
      "100/100 - 0s - loss: 0.4250 - acc: 0.9300\n",
      "Epoch 451/700\n",
      "100/100 - 0s - loss: 0.4215 - acc: 0.9300\n",
      "Epoch 452/700\n",
      "100/100 - 0s - loss: 0.4188 - acc: 0.9200\n",
      "Epoch 453/700\n",
      "100/100 - 0s - loss: 0.4171 - acc: 0.9300\n",
      "Epoch 454/700\n",
      "100/100 - 0s - loss: 0.4158 - acc: 0.9400\n",
      "Epoch 455/700\n",
      "100/100 - 0s - loss: 0.4138 - acc: 0.9400\n",
      "Epoch 456/700\n",
      "100/100 - 0s - loss: 0.4122 - acc: 0.9300\n",
      "Epoch 457/700\n",
      "100/100 - 0s - loss: 0.4100 - acc: 0.9400\n",
      "Epoch 458/700\n",
      "100/100 - 0s - loss: 0.4076 - acc: 0.9400\n",
      "Epoch 459/700\n",
      "100/100 - 0s - loss: 0.4075 - acc: 0.9400\n",
      "Epoch 460/700\n",
      "100/100 - 0s - loss: 0.4054 - acc: 0.9300\n",
      "Epoch 461/700\n",
      "100/100 - 0s - loss: 0.4031 - acc: 0.9200\n",
      "Epoch 462/700\n",
      "100/100 - 0s - loss: 0.4004 - acc: 0.9400\n",
      "Epoch 463/700\n",
      "100/100 - 0s - loss: 0.3983 - acc: 0.9400\n",
      "Epoch 464/700\n",
      "100/100 - 0s - loss: 0.3967 - acc: 0.9400\n",
      "Epoch 465/700\n",
      "100/100 - 0s - loss: 0.3947 - acc: 0.9400\n",
      "Epoch 466/700\n",
      "100/100 - 0s - loss: 0.3935 - acc: 0.9400\n",
      "Epoch 467/700\n",
      "100/100 - 0s - loss: 0.3906 - acc: 0.9400\n",
      "Epoch 468/700\n",
      "100/100 - 0s - loss: 0.3896 - acc: 0.9300\n",
      "Epoch 469/700\n",
      "100/100 - 0s - loss: 0.3871 - acc: 0.9400\n",
      "Epoch 470/700\n",
      "100/100 - 0s - loss: 0.3865 - acc: 0.9300\n",
      "Epoch 471/700\n",
      "100/100 - 0s - loss: 0.3845 - acc: 0.9400\n",
      "Epoch 472/700\n",
      "100/100 - 0s - loss: 0.3817 - acc: 0.9400\n",
      "Epoch 473/700\n",
      "100/100 - 0s - loss: 0.3802 - acc: 0.9500\n",
      "Epoch 474/700\n",
      "100/100 - 0s - loss: 0.3795 - acc: 0.9500\n",
      "Epoch 475/700\n",
      "100/100 - 0s - loss: 0.3793 - acc: 0.9400\n",
      "Epoch 476/700\n",
      "100/100 - 0s - loss: 0.3774 - acc: 0.9400\n",
      "Epoch 477/700\n",
      "100/100 - 0s - loss: 0.3747 - acc: 0.9500\n",
      "Epoch 478/700\n",
      "100/100 - 0s - loss: 0.3725 - acc: 0.9400\n",
      "Epoch 479/700\n",
      "100/100 - 0s - loss: 0.3706 - acc: 0.9400\n",
      "Epoch 480/700\n",
      "100/100 - 0s - loss: 0.3692 - acc: 0.9400\n",
      "Epoch 481/700\n",
      "100/100 - 0s - loss: 0.3669 - acc: 0.9400\n",
      "Epoch 482/700\n",
      "100/100 - 0s - loss: 0.3655 - acc: 0.9500\n",
      "Epoch 483/700\n",
      "100/100 - 0s - loss: 0.3637 - acc: 0.9500\n",
      "Epoch 484/700\n",
      "100/100 - 0s - loss: 0.3622 - acc: 0.9500\n",
      "Epoch 485/700\n",
      "100/100 - 0s - loss: 0.3617 - acc: 0.9400\n",
      "Epoch 486/700\n",
      "100/100 - 0s - loss: 0.3610 - acc: 0.9400\n",
      "Epoch 487/700\n",
      "100/100 - 0s - loss: 0.3583 - acc: 0.9400\n",
      "Epoch 488/700\n",
      "100/100 - 0s - loss: 0.3560 - acc: 0.9500\n",
      "Epoch 489/700\n",
      "100/100 - 0s - loss: 0.3551 - acc: 0.9500\n",
      "Epoch 490/700\n",
      "100/100 - 0s - loss: 0.3531 - acc: 0.9500\n",
      "Epoch 491/700\n",
      "100/100 - 0s - loss: 0.3511 - acc: 0.9500\n",
      "Epoch 492/700\n",
      "100/100 - 0s - loss: 0.3504 - acc: 0.9500\n",
      "Epoch 493/700\n",
      "100/100 - 0s - loss: 0.3490 - acc: 0.9400\n",
      "Epoch 494/700\n",
      "100/100 - 0s - loss: 0.3472 - acc: 0.9500\n",
      "Epoch 495/700\n",
      "100/100 - 0s - loss: 0.3453 - acc: 0.9600\n",
      "Epoch 496/700\n",
      "100/100 - 0s - loss: 0.3431 - acc: 0.9500\n",
      "Epoch 497/700\n",
      "100/100 - 0s - loss: 0.3418 - acc: 0.9400\n",
      "Epoch 498/700\n",
      "100/100 - 0s - loss: 0.3400 - acc: 0.9500\n",
      "Epoch 499/700\n",
      "100/100 - 0s - loss: 0.3386 - acc: 0.9500\n",
      "Epoch 500/700\n",
      "100/100 - 0s - loss: 0.3368 - acc: 0.9500\n",
      "Epoch 501/700\n",
      "100/100 - 0s - loss: 0.3352 - acc: 0.9500\n",
      "Epoch 502/700\n",
      "100/100 - 0s - loss: 0.3335 - acc: 0.9600\n",
      "Epoch 503/700\n",
      "100/100 - 0s - loss: 0.3322 - acc: 0.9500\n",
      "Epoch 504/700\n",
      "100/100 - 0s - loss: 0.3323 - acc: 0.9500\n",
      "Epoch 505/700\n",
      "100/100 - 0s - loss: 0.3302 - acc: 0.9500\n",
      "Epoch 506/700\n",
      "100/100 - 0s - loss: 0.3278 - acc: 0.9500\n",
      "Epoch 507/700\n",
      "100/100 - 0s - loss: 0.3262 - acc: 0.9500\n",
      "Epoch 508/700\n",
      "100/100 - 0s - loss: 0.3248 - acc: 0.9500\n",
      "Epoch 509/700\n",
      "100/100 - 0s - loss: 0.3238 - acc: 0.9500\n",
      "Epoch 510/700\n",
      "100/100 - 0s - loss: 0.3219 - acc: 0.9500\n",
      "Epoch 511/700\n",
      "100/100 - 0s - loss: 0.3203 - acc: 0.9500\n",
      "Epoch 512/700\n",
      "100/100 - 0s - loss: 0.3192 - acc: 0.9500\n",
      "Epoch 513/700\n",
      "100/100 - 0s - loss: 0.3176 - acc: 0.9500\n",
      "Epoch 514/700\n",
      "100/100 - 0s - loss: 0.3156 - acc: 0.9500\n",
      "Epoch 515/700\n",
      "100/100 - 0s - loss: 0.3148 - acc: 0.9500\n",
      "Epoch 516/700\n",
      "100/100 - 0s - loss: 0.3120 - acc: 0.9600\n",
      "Epoch 517/700\n",
      "100/100 - 0s - loss: 0.3115 - acc: 0.9500\n",
      "Epoch 518/700\n",
      "100/100 - 0s - loss: 0.3101 - acc: 0.9500\n",
      "Epoch 519/700\n",
      "100/100 - 0s - loss: 0.3085 - acc: 0.9500\n",
      "Epoch 520/700\n",
      "100/100 - 0s - loss: 0.3071 - acc: 0.9500\n",
      "Epoch 521/700\n",
      "100/100 - 0s - loss: 0.3061 - acc: 0.9500\n",
      "Epoch 522/700\n",
      "100/100 - 0s - loss: 0.3046 - acc: 0.9500\n",
      "Epoch 523/700\n",
      "100/100 - 0s - loss: 0.3034 - acc: 0.9500\n",
      "Epoch 524/700\n",
      "100/100 - 0s - loss: 0.3019 - acc: 0.9500\n",
      "Epoch 525/700\n",
      "100/100 - 0s - loss: 0.2998 - acc: 0.9500\n",
      "Epoch 526/700\n",
      "100/100 - 0s - loss: 0.3012 - acc: 0.9500\n",
      "Epoch 527/700\n",
      "100/100 - 0s - loss: 0.3014 - acc: 0.9500\n",
      "Epoch 528/700\n",
      "100/100 - 0s - loss: 0.2983 - acc: 0.9500\n",
      "Epoch 529/700\n",
      "100/100 - 0s - loss: 0.2956 - acc: 0.9500\n",
      "Epoch 530/700\n",
      "100/100 - 0s - loss: 0.2955 - acc: 0.9500\n",
      "Epoch 531/700\n",
      "100/100 - 0s - loss: 0.2953 - acc: 0.9500\n",
      "Epoch 532/700\n",
      "100/100 - 0s - loss: 0.2931 - acc: 0.9400\n",
      "Epoch 533/700\n",
      "100/100 - 0s - loss: 0.2909 - acc: 0.9500\n",
      "Epoch 534/700\n",
      "100/100 - 0s - loss: 0.2920 - acc: 0.9500\n",
      "Epoch 535/700\n",
      "100/100 - 0s - loss: 0.2908 - acc: 0.9500\n",
      "Epoch 536/700\n",
      "100/100 - 0s - loss: 0.2866 - acc: 0.9500\n",
      "Epoch 537/700\n",
      "100/100 - 0s - loss: 0.2839 - acc: 0.9500\n",
      "Epoch 538/700\n",
      "100/100 - 0s - loss: 0.2858 - acc: 0.9500\n",
      "Epoch 539/700\n",
      "100/100 - 0s - loss: 0.2838 - acc: 0.9500\n",
      "Epoch 540/700\n",
      "100/100 - 0s - loss: 0.2818 - acc: 0.9500\n",
      "Epoch 541/700\n",
      "100/100 - 0s - loss: 0.2797 - acc: 0.9500\n",
      "Epoch 542/700\n",
      "100/100 - 0s - loss: 0.2805 - acc: 0.9500\n",
      "Epoch 543/700\n",
      "100/100 - 0s - loss: 0.2793 - acc: 0.9500\n",
      "Epoch 544/700\n",
      "100/100 - 0s - loss: 0.2768 - acc: 0.9500\n",
      "Epoch 545/700\n",
      "100/100 - 0s - loss: 0.2746 - acc: 0.9500\n",
      "Epoch 546/700\n",
      "100/100 - 0s - loss: 0.2755 - acc: 0.9600\n",
      "Epoch 547/700\n",
      "100/100 - 0s - loss: 0.2782 - acc: 0.9500\n",
      "Epoch 548/700\n",
      "100/100 - 0s - loss: 0.2774 - acc: 0.9500\n",
      "Epoch 549/700\n",
      "100/100 - 0s - loss: 0.2746 - acc: 0.9500\n",
      "Epoch 550/700\n",
      "100/100 - 0s - loss: 0.2741 - acc: 0.9500\n",
      "Epoch 551/700\n",
      "100/100 - 0s - loss: 0.2725 - acc: 0.9500\n",
      "Epoch 552/700\n",
      "100/100 - 0s - loss: 0.2697 - acc: 0.9500\n",
      "Epoch 553/700\n",
      "100/100 - 0s - loss: 0.2678 - acc: 0.9600\n",
      "Epoch 554/700\n",
      "100/100 - 0s - loss: 0.2653 - acc: 0.9500\n",
      "Epoch 555/700\n",
      "100/100 - 0s - loss: 0.2633 - acc: 0.9500\n",
      "Epoch 556/700\n",
      "100/100 - 0s - loss: 0.2640 - acc: 0.9500\n",
      "Epoch 557/700\n",
      "100/100 - 0s - loss: 0.2683 - acc: 0.9500\n",
      "Epoch 558/700\n",
      "100/100 - 0s - loss: 0.2651 - acc: 0.9600\n",
      "Epoch 559/700\n",
      "100/100 - 0s - loss: 0.2597 - acc: 0.9500\n",
      "Epoch 560/700\n",
      "100/100 - 0s - loss: 0.2604 - acc: 0.9600\n",
      "Epoch 561/700\n",
      "100/100 - 0s - loss: 0.2619 - acc: 0.9500\n",
      "Epoch 562/700\n",
      "100/100 - 0s - loss: 0.2603 - acc: 0.9500\n",
      "Epoch 563/700\n",
      "100/100 - 0s - loss: 0.2572 - acc: 0.9500\n",
      "Epoch 564/700\n",
      "100/100 - 0s - loss: 0.2562 - acc: 0.9500\n",
      "Epoch 565/700\n",
      "100/100 - 0s - loss: 0.2565 - acc: 0.9500\n",
      "Epoch 566/700\n",
      "100/100 - 0s - loss: 0.2553 - acc: 0.9500\n",
      "Epoch 567/700\n",
      "100/100 - 0s - loss: 0.2528 - acc: 0.9500\n",
      "Epoch 568/700\n",
      "100/100 - 0s - loss: 0.2514 - acc: 0.9500\n",
      "Epoch 569/700\n",
      "100/100 - 0s - loss: 0.2501 - acc: 0.9500\n",
      "Epoch 570/700\n",
      "100/100 - 0s - loss: 0.2488 - acc: 0.9500\n",
      "Epoch 571/700\n",
      "100/100 - 0s - loss: 0.2483 - acc: 0.9500\n",
      "Epoch 572/700\n",
      "100/100 - 0s - loss: 0.2471 - acc: 0.9500\n",
      "Epoch 573/700\n",
      "100/100 - 0s - loss: 0.2461 - acc: 0.9500\n",
      "Epoch 574/700\n",
      "100/100 - 0s - loss: 0.2450 - acc: 0.9500\n",
      "Epoch 575/700\n",
      "100/100 - 0s - loss: 0.2441 - acc: 0.9500\n",
      "Epoch 576/700\n",
      "100/100 - 0s - loss: 0.2429 - acc: 0.9500\n",
      "Epoch 577/700\n",
      "100/100 - 0s - loss: 0.2423 - acc: 0.9500\n",
      "Epoch 578/700\n",
      "100/100 - 0s - loss: 0.2443 - acc: 0.9500\n",
      "Epoch 579/700\n",
      "100/100 - 0s - loss: 0.2435 - acc: 0.9500\n",
      "Epoch 580/700\n",
      "100/100 - 0s - loss: 0.2400 - acc: 0.9500\n",
      "Epoch 581/700\n",
      "100/100 - 0s - loss: 0.2393 - acc: 0.9500\n",
      "Epoch 582/700\n",
      "100/100 - 0s - loss: 0.2378 - acc: 0.9500\n",
      "Epoch 583/700\n",
      "100/100 - 0s - loss: 0.2365 - acc: 0.9500\n",
      "Epoch 584/700\n",
      "100/100 - 0s - loss: 0.2366 - acc: 0.9500\n",
      "Epoch 585/700\n",
      "100/100 - 0s - loss: 0.2365 - acc: 0.9500\n",
      "Epoch 586/700\n",
      "100/100 - 0s - loss: 0.2358 - acc: 0.9500\n",
      "Epoch 587/700\n",
      "100/100 - 0s - loss: 0.2342 - acc: 0.9500\n",
      "Epoch 588/700\n",
      "100/100 - 0s - loss: 0.2320 - acc: 0.9500\n",
      "Epoch 589/700\n",
      "100/100 - 0s - loss: 0.2332 - acc: 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/700\n",
      "100/100 - 0s - loss: 0.2327 - acc: 0.9700\n",
      "Epoch 591/700\n",
      "100/100 - 0s - loss: 0.2317 - acc: 0.9700\n",
      "Epoch 592/700\n",
      "100/100 - 0s - loss: 0.2296 - acc: 0.9500\n",
      "Epoch 593/700\n",
      "100/100 - 0s - loss: 0.2282 - acc: 0.9500\n",
      "Epoch 594/700\n",
      "100/100 - 0s - loss: 0.2295 - acc: 0.9600\n",
      "Epoch 595/700\n",
      "100/100 - 0s - loss: 0.2291 - acc: 0.9600\n",
      "Epoch 596/700\n",
      "100/100 - 0s - loss: 0.2263 - acc: 0.9500\n",
      "Epoch 597/700\n",
      "100/100 - 0s - loss: 0.2258 - acc: 0.9500\n",
      "Epoch 598/700\n",
      "100/100 - 0s - loss: 0.2254 - acc: 0.9700\n",
      "Epoch 599/700\n",
      "100/100 - 0s - loss: 0.2231 - acc: 0.9500\n",
      "Epoch 600/700\n",
      "100/100 - 0s - loss: 0.2238 - acc: 0.9500\n",
      "Epoch 601/700\n",
      "100/100 - 0s - loss: 0.2223 - acc: 0.9500\n",
      "Epoch 602/700\n",
      "100/100 - 0s - loss: 0.2210 - acc: 0.9500\n",
      "Epoch 603/700\n",
      "100/100 - 0s - loss: 0.2203 - acc: 0.9500\n",
      "Epoch 604/700\n",
      "100/100 - 0s - loss: 0.2199 - acc: 0.9500\n",
      "Epoch 605/700\n",
      "100/100 - 0s - loss: 0.2187 - acc: 0.9500\n",
      "Epoch 606/700\n",
      "100/100 - 0s - loss: 0.2184 - acc: 0.9500\n",
      "Epoch 607/700\n",
      "100/100 - 0s - loss: 0.2172 - acc: 0.9500\n",
      "Epoch 608/700\n",
      "100/100 - 0s - loss: 0.2163 - acc: 0.9600\n",
      "Epoch 609/700\n",
      "100/100 - 0s - loss: 0.2167 - acc: 0.9700\n",
      "Epoch 610/700\n",
      "100/100 - 0s - loss: 0.2165 - acc: 0.9700\n",
      "Epoch 611/700\n",
      "100/100 - 0s - loss: 0.2171 - acc: 0.9700\n",
      "Epoch 612/700\n",
      "100/100 - 0s - loss: 0.2165 - acc: 0.9700\n",
      "Epoch 613/700\n",
      "100/100 - 0s - loss: 0.2144 - acc: 0.9700\n",
      "Epoch 614/700\n",
      "100/100 - 0s - loss: 0.2113 - acc: 0.9700\n",
      "Epoch 615/700\n",
      "100/100 - 0s - loss: 0.2123 - acc: 0.9600\n",
      "Epoch 616/700\n",
      "100/100 - 0s - loss: 0.2166 - acc: 0.9600\n",
      "Epoch 617/700\n",
      "100/100 - 0s - loss: 0.2134 - acc: 0.9600\n",
      "Epoch 618/700\n",
      "100/100 - 0s - loss: 0.2094 - acc: 0.9500\n",
      "Epoch 619/700\n",
      "100/100 - 0s - loss: 0.2081 - acc: 0.9600\n",
      "Epoch 620/700\n",
      "100/100 - 0s - loss: 0.2078 - acc: 0.9700\n",
      "Epoch 621/700\n",
      "100/100 - 0s - loss: 0.2073 - acc: 0.9700\n",
      "Epoch 622/700\n",
      "100/100 - 0s - loss: 0.2072 - acc: 0.9600\n",
      "Epoch 623/700\n",
      "100/100 - 0s - loss: 0.2054 - acc: 0.9600\n",
      "Epoch 624/700\n",
      "100/100 - 0s - loss: 0.2055 - acc: 0.9700\n",
      "Epoch 625/700\n",
      "100/100 - 0s - loss: 0.2052 - acc: 0.9700\n",
      "Epoch 626/700\n",
      "100/100 - 0s - loss: 0.2033 - acc: 0.9600\n",
      "Epoch 627/700\n",
      "100/100 - 0s - loss: 0.2027 - acc: 0.9600\n",
      "Epoch 628/700\n",
      "100/100 - 0s - loss: 0.2020 - acc: 0.9600\n",
      "Epoch 629/700\n",
      "100/100 - 0s - loss: 0.2013 - acc: 0.9600\n",
      "Epoch 630/700\n",
      "100/100 - 0s - loss: 0.2010 - acc: 0.9600\n",
      "Epoch 631/700\n",
      "100/100 - 0s - loss: 0.2002 - acc: 0.9600\n",
      "Epoch 632/700\n",
      "100/100 - 0s - loss: 0.1995 - acc: 0.9500\n",
      "Epoch 633/700\n",
      "100/100 - 0s - loss: 0.1994 - acc: 0.9500\n",
      "Epoch 634/700\n",
      "100/100 - 0s - loss: 0.1996 - acc: 0.9500\n",
      "Epoch 635/700\n",
      "100/100 - 0s - loss: 0.1977 - acc: 0.9500\n",
      "Epoch 636/700\n",
      "100/100 - 0s - loss: 0.1969 - acc: 0.9600\n",
      "Epoch 637/700\n",
      "100/100 - 0s - loss: 0.1968 - acc: 0.9700\n",
      "Epoch 638/700\n",
      "100/100 - 0s - loss: 0.1964 - acc: 0.9700\n",
      "Epoch 639/700\n",
      "100/100 - 0s - loss: 0.1954 - acc: 0.9700\n",
      "Epoch 640/700\n",
      "100/100 - 0s - loss: 0.1941 - acc: 0.9600\n",
      "Epoch 641/700\n",
      "100/100 - 0s - loss: 0.1935 - acc: 0.9600\n",
      "Epoch 642/700\n",
      "100/100 - 0s - loss: 0.1935 - acc: 0.9600\n",
      "Epoch 643/700\n",
      "100/100 - 0s - loss: 0.1929 - acc: 0.9500\n",
      "Epoch 644/700\n",
      "100/100 - 0s - loss: 0.1937 - acc: 0.9500\n",
      "Epoch 645/700\n",
      "100/100 - 0s - loss: 0.1932 - acc: 0.9500\n",
      "Epoch 646/700\n",
      "100/100 - 0s - loss: 0.1927 - acc: 0.9500\n",
      "Epoch 647/700\n",
      "100/100 - 0s - loss: 0.1902 - acc: 0.9600\n",
      "Epoch 648/700\n",
      "100/100 - 0s - loss: 0.1898 - acc: 0.9700\n",
      "Epoch 649/700\n",
      "100/100 - 0s - loss: 0.1890 - acc: 0.9700\n",
      "Epoch 650/700\n",
      "100/100 - 0s - loss: 0.1891 - acc: 0.9700\n",
      "Epoch 651/700\n",
      "100/100 - 0s - loss: 0.1883 - acc: 0.9700\n",
      "Epoch 652/700\n",
      "100/100 - 0s - loss: 0.1887 - acc: 0.9700\n",
      "Epoch 653/700\n",
      "100/100 - 0s - loss: 0.1886 - acc: 0.9700\n",
      "Epoch 654/700\n",
      "100/100 - 0s - loss: 0.1868 - acc: 0.9700\n",
      "Epoch 655/700\n",
      "100/100 - 0s - loss: 0.1863 - acc: 0.9700\n",
      "Epoch 656/700\n",
      "100/100 - 0s - loss: 0.1849 - acc: 0.9700\n",
      "Epoch 657/700\n",
      "100/100 - 0s - loss: 0.1845 - acc: 0.9700\n",
      "Epoch 658/700\n",
      "100/100 - 0s - loss: 0.1839 - acc: 0.9700\n",
      "Epoch 659/700\n",
      "100/100 - 0s - loss: 0.1828 - acc: 0.9700\n",
      "Epoch 660/700\n",
      "100/100 - 0s - loss: 0.1833 - acc: 0.9600\n",
      "Epoch 661/700\n",
      "100/100 - 0s - loss: 0.1834 - acc: 0.9500\n",
      "Epoch 662/700\n",
      "100/100 - 0s - loss: 0.1846 - acc: 0.9500\n",
      "Epoch 663/700\n",
      "100/100 - 0s - loss: 0.1831 - acc: 0.9500\n",
      "Epoch 664/700\n",
      "100/100 - 0s - loss: 0.1798 - acc: 0.9700\n",
      "Epoch 665/700\n",
      "100/100 - 0s - loss: 0.1800 - acc: 0.9700\n",
      "Epoch 666/700\n",
      "100/100 - 0s - loss: 0.1803 - acc: 0.9700\n",
      "Epoch 667/700\n",
      "100/100 - 0s - loss: 0.1809 - acc: 0.9700\n",
      "Epoch 668/700\n",
      "100/100 - 0s - loss: 0.1803 - acc: 0.9700\n",
      "Epoch 669/700\n",
      "100/100 - 0s - loss: 0.1805 - acc: 0.9700\n",
      "Epoch 670/700\n",
      "100/100 - 0s - loss: 0.1786 - acc: 0.9700\n",
      "Epoch 671/700\n",
      "100/100 - 0s - loss: 0.1796 - acc: 0.9700\n",
      "Epoch 672/700\n",
      "100/100 - 0s - loss: 0.1799 - acc: 0.9700\n",
      "Epoch 673/700\n",
      "100/100 - 0s - loss: 0.1788 - acc: 0.9700\n",
      "Epoch 674/700\n",
      "100/100 - 0s - loss: 0.1771 - acc: 0.9700\n",
      "Epoch 675/700\n",
      "100/100 - 0s - loss: 0.1757 - acc: 0.9700\n",
      "Epoch 676/700\n",
      "100/100 - 0s - loss: 0.1746 - acc: 0.9700\n",
      "Epoch 677/700\n",
      "100/100 - 0s - loss: 0.1726 - acc: 0.9700\n",
      "Epoch 678/700\n",
      "100/100 - 0s - loss: 0.1722 - acc: 0.9700\n",
      "Epoch 679/700\n",
      "100/100 - 0s - loss: 0.1721 - acc: 0.9700\n",
      "Epoch 680/700\n",
      "100/100 - 0s - loss: 0.1717 - acc: 0.9700\n",
      "Epoch 681/700\n",
      "100/100 - 0s - loss: 0.1722 - acc: 0.9700\n",
      "Epoch 682/700\n",
      "100/100 - 0s - loss: 0.1711 - acc: 0.9700\n",
      "Epoch 683/700\n",
      "100/100 - 0s - loss: 0.1700 - acc: 0.9700\n",
      "Epoch 684/700\n",
      "100/100 - 0s - loss: 0.1699 - acc: 0.9600\n",
      "Epoch 685/700\n",
      "100/100 - 0s - loss: 0.1701 - acc: 0.9600\n",
      "Epoch 686/700\n",
      "100/100 - 0s - loss: 0.1701 - acc: 0.9600\n",
      "Epoch 687/700\n",
      "100/100 - 0s - loss: 0.1694 - acc: 0.9600\n",
      "Epoch 688/700\n",
      "100/100 - 0s - loss: 0.1686 - acc: 0.9600\n",
      "Epoch 689/700\n",
      "100/100 - 0s - loss: 0.1676 - acc: 0.9600\n",
      "Epoch 690/700\n",
      "100/100 - 0s - loss: 0.1669 - acc: 0.9700\n",
      "Epoch 691/700\n",
      "100/100 - 0s - loss: 0.1660 - acc: 0.9700\n",
      "Epoch 692/700\n",
      "100/100 - 0s - loss: 0.1654 - acc: 0.9700\n",
      "Epoch 693/700\n",
      "100/100 - 0s - loss: 0.1663 - acc: 0.9700\n",
      "Epoch 694/700\n",
      "100/100 - 0s - loss: 0.1665 - acc: 0.9700\n",
      "Epoch 695/700\n",
      "100/100 - 0s - loss: 0.1660 - acc: 0.9700\n",
      "Epoch 696/700\n",
      "100/100 - 0s - loss: 0.1649 - acc: 0.9700\n",
      "Epoch 697/700\n",
      "100/100 - 0s - loss: 0.1636 - acc: 0.9700\n",
      "Epoch 698/700\n",
      "100/100 - 0s - loss: 0.1625 - acc: 0.9700\n",
      "Epoch 699/700\n",
      "100/100 - 0s - loss: 0.1620 - acc: 0.9700\n",
      "Epoch 700/700\n",
      "100/100 - 0s - loss: 0.1617 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf58636c10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs is the number of times the network \"runs\" through the entire training dataset\n",
    "# verbose is the amount of info to output\n",
    "model.fit(scaled_X_train,y_train,epochs=700,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)\n",
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred0</th>\n",
       "      <th>Pred1</th>\n",
       "      <th>Pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pred0  Pred1  Pred2\n",
       "True0     19      0      0\n",
       "True1      0     14      1\n",
       "True2      0      2     14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.88      0.93      0.90        15\n",
      "           2       0.93      0.88      0.90        16\n",
      "\n",
      "    accuracy                           0.94        50\n",
      "   macro avg       0.94      0.94      0.94        50\n",
      "weighted avg       0.94      0.94      0.94        50\n",
      "\n",
      "Accuracy Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(confusion_matrix(y_test.argmax(axis=1),predictions), \n",
    "                            index=['True0','True1', 'True2'], \n",
    "                            columns=['Pred0','Pred1', 'Pred2']))\n",
    "print(classification_report(y_test.argmax(axis=1),predictions))\n",
    "print('Accuracy Score:', accuracy_score(y_test.argmax(axis=1),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the model to be re-used\n",
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "> Specifically designed to work with sequence data\n",
    "* Time series\n",
    "* Sentences (NLP)\n",
    "* Audio\n",
    "* Car Trajectories\n",
    "\n",
    "<img src='feed_forward_neuron.png'>\n",
    "<img src='recur_neuron.png'>\n",
    "<img src='recur_neuron_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Cells:** Nodes that are a function of inputs from previous time steps\n",
    "\n",
    "<img src='recur_layer.png'>\n",
    "<img src='recur_layer_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM & GRU\n",
    "* Long-short term memory\n",
    "* \n",
    "\n",
    "> Help to reduce the \"forgetfullness\" of RNNs as what was input at $t_{0}$ may lose its influence at $t_{i+j}$\n",
    "\n",
    "<img src='lstm_cell.png'>\n",
    "\n",
    "**Input:**\n",
    "1. $C_{t-1}$: Cell state at $t-1$\n",
    "2. $h_{t-1}$: outcome of previous cell\n",
    "3. $x_{t}$: factor of relevance\n",
    "\n",
    "### First Step: Forget Gate Layer\n",
    "* We decide what information we are going to \"forget\", or \"throw away\", from the cell state\n",
    "<img src='forget_gate_layer.png'>\n",
    "\n",
    "### Second Step:\n",
    "* We decide what to store\n",
    "<img src='lstm_stage_2.png'>\n",
    "\n",
    "### Third Step:\n",
    "* We update the cell state (to send to the next cell state)\n",
    "<img src='lstm_stage_3.png'>\n",
    "\n",
    "### Fourth Step:\n",
    "* We output $h_{t}$\n",
    "<img src='lstm_stage_4.png'>\n",
    "\n",
    "**Different kind of LSTMs:**\n",
    "1. Peephole\n",
    "2. Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Peephole**\n",
    "<img src='lstm_peep.png'>\n",
    "**GRU**\n",
    "<img src='lstm_GRU.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTMs with Keras and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "        \n",
    "        return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the load by specifiying what to disable\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    import string\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "#     return [token.text.lower() for token in nlp(doc_text) if token.text not in [string.punctuation, '\\n\\n \\n\\n\\n!\"']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 words --> network predict #26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on\n",
      "\n",
      "me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore\n",
      "\n",
      "ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i\n",
      "\n",
      "some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought\n",
      "\n",
      "years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each item in the list is composed of one word to the right of the previous, and less the first\n",
    "# - shift the text one word right\n",
    "for i in range(5):\n",
    "    print(' '.join(text_sequences[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956,\n",
       " 14,\n",
       " 263,\n",
       " 51,\n",
       " 261,\n",
       " 408,\n",
       " 87,\n",
       " 219,\n",
       " 129,\n",
       " 111,\n",
       " 954,\n",
       " 260,\n",
       " 50,\n",
       " 43,\n",
       " 38,\n",
       " 315,\n",
       " 7,\n",
       " 23,\n",
       " 546,\n",
       " 3,\n",
       " 150,\n",
       " 259,\n",
       " 6,\n",
       " 2712,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956: call\n",
      "14: me\n",
      "263: ishmael\n",
      "51: some\n",
      "261: years\n",
      "408: ago\n",
      "87: never\n",
      "219: mind\n",
      "129: how\n",
      "111: long\n",
      "954: precisely\n",
      "260: having\n",
      "50: little\n",
      "43: or\n",
      "38: no\n",
      "315: money\n",
      "7: in\n",
      "23: my\n",
      "546: purse\n",
      "3: and\n",
      "150: nothing\n",
      "259: particular\n",
      "6: to\n",
      "2712: interest\n",
      "14: me\n",
      "24: on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i}: {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('call', 27),\n",
       "             ('me', 2471),\n",
       "             ('ishmael', 133),\n",
       "             ('some', 758),\n",
       "             ('years', 135),\n",
       "             ('ago', 84),\n",
       "             ('never', 449),\n",
       "             ('mind', 164),\n",
       "             ('how', 321),\n",
       "             ('long', 374),\n",
       "             ('precisely', 37),\n",
       "             ('having', 142),\n",
       "             ('little', 767),\n",
       "             ('or', 950),\n",
       "             ('no', 1003),\n",
       "             ('money', 120),\n",
       "             ('in', 5647),\n",
       "             ('my', 1786),\n",
       "             ('purse', 71),\n",
       "             ('and', 9646),\n",
       "             ('nothing', 281),\n",
       "             ('particular', 152),\n",
       "             ('to', 6497),\n",
       "             ('interest', 24),\n",
       "             ('on', 1716),\n",
       "             ('shore', 26),\n",
       "             ('i', 7150),\n",
       "             ('thought', 676),\n",
       "             ('would', 702),\n",
       "             ('sail', 104),\n",
       "             ('about', 1014),\n",
       "             ('a', 10377),\n",
       "             ('see', 416),\n",
       "             ('the', 15540),\n",
       "             ('watery', 26),\n",
       "             ('part', 234),\n",
       "             ('of', 8287),\n",
       "             ('world', 234),\n",
       "             ('it', 4238),\n",
       "             ('is', 1950),\n",
       "             ('way', 390),\n",
       "             ('have', 806),\n",
       "             ('driving', 26),\n",
       "             ('off', 416),\n",
       "             ('spleen', 26),\n",
       "             ('regulating', 26),\n",
       "             ('circulation', 26),\n",
       "             ('whenever', 130),\n",
       "             ('find', 78),\n",
       "             ('myself', 416),\n",
       "             ('growing', 26),\n",
       "             ('grim', 26),\n",
       "             ('mouth', 130),\n",
       "             ('damp', 78),\n",
       "             ('drizzly', 26),\n",
       "             ('november', 26),\n",
       "             ('soul', 78),\n",
       "             ('involuntarily', 52),\n",
       "             ('pausing', 52),\n",
       "             ('before', 364),\n",
       "             ('coffin', 104),\n",
       "             ('warehouses', 52),\n",
       "             ('bringing', 26),\n",
       "             ('up', 1237),\n",
       "             ('rear', 26),\n",
       "             ('every', 182),\n",
       "             ('funeral', 26),\n",
       "             ('meet', 26),\n",
       "             ('especially', 104),\n",
       "             ('hypos', 26),\n",
       "             ('get', 364),\n",
       "             ('such', 572),\n",
       "             ('an', 806),\n",
       "             ('upper', 26),\n",
       "             ('hand', 312),\n",
       "             ('that', 3770),\n",
       "             ('requires', 52),\n",
       "             ('strong', 78),\n",
       "             ('moral', 26),\n",
       "             ('principle', 26),\n",
       "             ('prevent', 26),\n",
       "             ('from', 1508),\n",
       "             ('deliberately', 26),\n",
       "             ('stepping', 26),\n",
       "             ('into', 988),\n",
       "             ('street', 104),\n",
       "             ('methodically', 26),\n",
       "             ('knocking', 26),\n",
       "             ('people', 52),\n",
       "             (\"'s\", 1691),\n",
       "             ('hats', 26),\n",
       "             ('then', 832),\n",
       "             ('account', 78),\n",
       "             ('high', 130),\n",
       "             ('time', 520),\n",
       "             ('sea', 546),\n",
       "             ('as', 2366),\n",
       "             ('soon', 234),\n",
       "             ('can', 338),\n",
       "             ('this', 2158),\n",
       "             ('substitute', 26),\n",
       "             ('for', 1820),\n",
       "             ('pistol', 26),\n",
       "             ('ball', 26),\n",
       "             ('with', 2392),\n",
       "             ('philosophical', 26),\n",
       "             ('flourish', 26),\n",
       "             ('cato', 26),\n",
       "             ('throws', 26),\n",
       "             ('himself', 338),\n",
       "             ('upon', 780),\n",
       "             ('his', 3139),\n",
       "             ('sword', 78),\n",
       "             ('quietly', 78),\n",
       "             ('take', 260),\n",
       "             ('ship', 182),\n",
       "             ('there', 1456),\n",
       "             ('surprising', 26),\n",
       "             ('if', 728),\n",
       "             ('they', 728),\n",
       "             ('but', 2652),\n",
       "             ('knew', 130),\n",
       "             ('almost', 286),\n",
       "             ('all', 1872),\n",
       "             ('men', 130),\n",
       "             ('their', 390),\n",
       "             ('degree', 78),\n",
       "             ('other', 494),\n",
       "             ('cherish', 26),\n",
       "             ('very', 494),\n",
       "             ('nearly', 52),\n",
       "             ('same', 312),\n",
       "             ('feelings', 26),\n",
       "             ('towards', 260),\n",
       "             ('ocean', 52),\n",
       "             ('now', 1040),\n",
       "             ('your', 442),\n",
       "             ('insular', 26),\n",
       "             ('city', 104),\n",
       "             ('manhattoes', 26),\n",
       "             ('belted', 26),\n",
       "             ('round', 364),\n",
       "             ('by', 962),\n",
       "             ('wharves', 26),\n",
       "             ('indian', 52),\n",
       "             ('isles', 26),\n",
       "             ('coral', 26),\n",
       "             ('reefs', 26),\n",
       "             ('commerce', 26),\n",
       "             ('surrounds', 26),\n",
       "             ('her', 156),\n",
       "             ('surf', 26),\n",
       "             ('right', 156),\n",
       "             ('left', 78),\n",
       "             ('streets', 208),\n",
       "             ('you', 2158),\n",
       "             ('waterward', 52),\n",
       "             ('its', 156),\n",
       "             ('extreme', 26),\n",
       "             ('downtown', 26),\n",
       "             ('battery', 52),\n",
       "             ('where', 364),\n",
       "             ('noble', 52),\n",
       "             ('mole', 26),\n",
       "             ('washed', 52),\n",
       "             ('waves', 26),\n",
       "             ('cooled', 26),\n",
       "             ('breezes', 26),\n",
       "             ('which', 572),\n",
       "             ('few', 104),\n",
       "             ('hours', 130),\n",
       "             ('previous', 104),\n",
       "             ('were', 962),\n",
       "             ('out', 956),\n",
       "             ('sight', 104),\n",
       "             ('land', 208),\n",
       "             ('look', 156),\n",
       "             ('at', 2184),\n",
       "             ('crowds', 52),\n",
       "             ('water', 234),\n",
       "             ('gazers', 26),\n",
       "             ('circumambulate', 26),\n",
       "             ('dreamy', 26),\n",
       "             ('sabbath', 52),\n",
       "             ('afternoon', 52),\n",
       "             ('go', 494),\n",
       "             ('corlears', 26),\n",
       "             ('hook', 26),\n",
       "             ('coenties', 26),\n",
       "             ('slip', 26),\n",
       "             ('thence', 52),\n",
       "             ('whitehall', 26),\n",
       "             ('northward', 26),\n",
       "             ('what', 1118),\n",
       "             ('do', 702),\n",
       "             ('see?--posted', 26),\n",
       "             ('like', 732),\n",
       "             ('silent', 52),\n",
       "             ('sentinels', 26),\n",
       "             ('around', 78),\n",
       "             ('town', 156),\n",
       "             ('stand', 182),\n",
       "             ('thousands', 52),\n",
       "             ('mortal', 26),\n",
       "             ('fixed', 78),\n",
       "             ('reveries', 52),\n",
       "             ('leaning', 52),\n",
       "             ('against', 234),\n",
       "             ('spiles', 26),\n",
       "             ('seated', 52),\n",
       "             ('pier', 26),\n",
       "             ('heads', 338),\n",
       "             ('looking', 312),\n",
       "             ('over', 702),\n",
       "             ('bulwarks', 52),\n",
       "             ('ships', 78),\n",
       "             ('china', 26),\n",
       "             ('aloft', 52),\n",
       "             ('rigging', 26),\n",
       "             ('striving', 26),\n",
       "             ('still', 364),\n",
       "             ('better', 208),\n",
       "             ('seaward', 26),\n",
       "             ('peep', 26),\n",
       "             ('these', 494),\n",
       "             ('are', 416),\n",
       "             ('landsmen', 26),\n",
       "             ('week', 52),\n",
       "             ('days', 104),\n",
       "             ('pent', 26),\n",
       "             ('lath', 26),\n",
       "             ('plaster', 52),\n",
       "             ('tied', 26),\n",
       "             ('counters', 26),\n",
       "             ('nailed', 26),\n",
       "             ('benches', 26),\n",
       "             ('clinched', 26),\n",
       "             ('desks', 26),\n",
       "             ('green', 130),\n",
       "             ('fields', 26),\n",
       "             ('gone', 52),\n",
       "             ('here', 598),\n",
       "             ('come', 338),\n",
       "             ('more', 494),\n",
       "             ('pacing', 26),\n",
       "             ('straight', 104),\n",
       "             ('seemingly', 26),\n",
       "             ('bound', 52),\n",
       "             ('dive', 26),\n",
       "             ('strange', 182),\n",
       "             ('will', 260),\n",
       "             ('content', 52),\n",
       "             ('them', 442),\n",
       "             ('extremest', 26),\n",
       "             ('limit', 26),\n",
       "             ('loitering', 26),\n",
       "             ('under', 260),\n",
       "             ('shady', 26),\n",
       "             ('lee', 26),\n",
       "             ('yonder', 52),\n",
       "             ('not', 1534),\n",
       "             ('suffice', 26),\n",
       "             ('must', 442),\n",
       "             ('just', 390),\n",
       "             ('nigh', 104),\n",
       "             ('possibly', 26),\n",
       "             ('without', 182),\n",
       "             ('falling', 52),\n",
       "             ('miles', 78),\n",
       "             ('leagues', 26),\n",
       "             ('inlanders', 26),\n",
       "             ('lanes', 26),\n",
       "             ('alleys', 26),\n",
       "             ('avenues', 26),\n",
       "             ('north', 52),\n",
       "             ('east', 26),\n",
       "             ('south', 156),\n",
       "             ('west', 26),\n",
       "             ('yet', 416),\n",
       "             ('unite', 26),\n",
       "             ('tell', 442),\n",
       "             ('does', 156),\n",
       "             ('magnetic', 26),\n",
       "             ('virtue', 26),\n",
       "             ('needles', 26),\n",
       "             ('compasses', 26),\n",
       "             ('those', 234),\n",
       "             ('attract', 26),\n",
       "             ('thither', 26),\n",
       "             ('once', 208),\n",
       "             ('say', 286),\n",
       "             ('country', 78),\n",
       "             ('lakes', 26),\n",
       "             ('any', 364),\n",
       "             ('path', 26),\n",
       "             ('please', 52),\n",
       "             ('ten', 52),\n",
       "             ('one', 1300),\n",
       "             ('carries', 26),\n",
       "             ('down', 468),\n",
       "             ('dale', 26),\n",
       "             ('leaves', 52),\n",
       "             ('pool', 26),\n",
       "             ('stream', 78),\n",
       "             ('magic', 52),\n",
       "             ('let', 156),\n",
       "             ('most', 468),\n",
       "             ('absent', 26),\n",
       "             ('minded', 26),\n",
       "             ('be', 1716),\n",
       "             ('plunged', 52),\n",
       "             ('deepest', 26),\n",
       "             ('man', 572),\n",
       "             ('legs', 104),\n",
       "             ('set', 156),\n",
       "             ('feet', 182),\n",
       "             ('going', 260),\n",
       "             ('he', 3247),\n",
       "             ('infallibly', 26),\n",
       "             ('lead', 78),\n",
       "             ('region', 26),\n",
       "             ('should', 286),\n",
       "             ('ever', 338),\n",
       "             ('athirst', 26),\n",
       "             ('great', 376),\n",
       "             ('american', 78),\n",
       "             ('desert', 26),\n",
       "             ('try', 104),\n",
       "             ('experiment', 26),\n",
       "             ('caravan', 26),\n",
       "             ('happen', 26),\n",
       "             ('supplied', 26),\n",
       "             ('metaphysical', 52),\n",
       "             ('professor', 26),\n",
       "             ('yes', 104),\n",
       "             ('knows', 26),\n",
       "             ('meditation', 26),\n",
       "             ('wedded', 26),\n",
       "             ('artist', 78),\n",
       "             ('desires', 26),\n",
       "             ('paint', 26),\n",
       "             ('dreamiest', 26),\n",
       "             ('shadiest', 26),\n",
       "             ('quietest', 26),\n",
       "             ('enchanting', 26),\n",
       "             ('bit', 130),\n",
       "             ('romantic', 26),\n",
       "             ('landscape', 26),\n",
       "             ('valley', 26),\n",
       "             ('saco', 26),\n",
       "             ('chief', 52),\n",
       "             ('element', 26),\n",
       "             ('employs', 26),\n",
       "             ('trees', 26),\n",
       "             ('each', 78),\n",
       "             ('hollow', 26),\n",
       "             ('trunk', 52),\n",
       "             ('hermit', 26),\n",
       "             ('crucifix', 26),\n",
       "             ('within', 130),\n",
       "             ('sleeps', 26),\n",
       "             ('meadow', 52),\n",
       "             ('sleep', 416),\n",
       "             ('cattle', 26),\n",
       "             ('cottage', 26),\n",
       "             ('goes', 78),\n",
       "             ('sleepy', 26),\n",
       "             ('smoke', 52),\n",
       "             ('deep', 78),\n",
       "             ('distant', 78),\n",
       "             ('woodlands', 26),\n",
       "             ('winds', 78),\n",
       "             ('mazy', 26),\n",
       "             ('reaching', 52),\n",
       "             ('overlapping', 26),\n",
       "             ('spurs', 26),\n",
       "             ('mountains', 26),\n",
       "             ('bathed', 26),\n",
       "             ('hill', 52),\n",
       "             ('side', 286),\n",
       "             ('blue', 78),\n",
       "             ('though', 494),\n",
       "             ('picture', 130),\n",
       "             ('lies', 26),\n",
       "             ('thus', 52),\n",
       "             ('tranced', 26),\n",
       "             ('pine', 52),\n",
       "             ('tree', 26),\n",
       "             ('shakes', 26),\n",
       "             ('sighs', 52),\n",
       "             ('shepherd', 52),\n",
       "             ('head', 598),\n",
       "             ('vain', 26),\n",
       "             ('unless', 104),\n",
       "             ('eye', 26),\n",
       "             ('him', 1092),\n",
       "             ('visit', 26),\n",
       "             ('prairies', 26),\n",
       "             ('june', 52),\n",
       "             ('when', 650),\n",
       "             ('scores', 52),\n",
       "             ('wade', 26),\n",
       "             ('knee', 26),\n",
       "             ('among', 78),\n",
       "             ('tiger', 26),\n",
       "             ('lilies', 26),\n",
       "             ('charm', 26),\n",
       "             ('wanting?--water', 26),\n",
       "             ('drop', 26),\n",
       "             ('niagara', 26),\n",
       "             ('cataract', 26),\n",
       "             ('sand', 26),\n",
       "             ('travel', 26),\n",
       "             ('thousand', 52),\n",
       "             ('why', 286),\n",
       "             ('did', 572),\n",
       "             ('poor', 104),\n",
       "             ('poet', 26),\n",
       "             ('tennessee', 26),\n",
       "             ('suddenly', 78),\n",
       "             ('receiving', 26),\n",
       "             ('two', 338),\n",
       "             ('handfuls', 26),\n",
       "             ('silver', 26),\n",
       "             ('deliberate', 26),\n",
       "             ('whether', 182),\n",
       "             ('buy', 26),\n",
       "             ('coat', 104),\n",
       "             ('sadly', 52),\n",
       "             ('needed', 26),\n",
       "             ('invest', 26),\n",
       "             ('pedestrian', 26),\n",
       "             ('trip', 26),\n",
       "             ('rockaway', 26),\n",
       "             ('beach', 26),\n",
       "             ('robust', 52),\n",
       "             ('healthy', 52),\n",
       "             ('boy', 52),\n",
       "             ('crazy', 52),\n",
       "             ('first', 494),\n",
       "             ('voyage', 208),\n",
       "             ('passenger', 104),\n",
       "             ('yourself', 156),\n",
       "             ('feel', 78),\n",
       "             ('mystical', 26),\n",
       "             ('vibration', 26),\n",
       "             ('told', 130),\n",
       "             ('old', 754),\n",
       "             ('persians', 26),\n",
       "             ('hold', 52),\n",
       "             ('holy', 52),\n",
       "             ('greeks', 26),\n",
       "             ('give', 208),\n",
       "             ('separate', 26),\n",
       "             ('deity', 26),\n",
       "             ('own', 286),\n",
       "             ('brother', 52),\n",
       "             ('jove', 26),\n",
       "             ('surely', 26),\n",
       "             ('meaning', 78),\n",
       "             ('deeper', 26),\n",
       "             ('story', 130),\n",
       "             ('narcissus', 26),\n",
       "             ('who', 416),\n",
       "             ('because', 182),\n",
       "             ('could', 650),\n",
       "             ('grasp', 26),\n",
       "             ('tormenting', 26),\n",
       "             ('mild', 26),\n",
       "             ('image', 156),\n",
       "             ('saw', 156),\n",
       "             ('fountain', 26),\n",
       "             ('was', 2886),\n",
       "             ('drowned', 26),\n",
       "             ('we', 286),\n",
       "             ('ourselves', 52),\n",
       "             ('rivers', 26),\n",
       "             ('oceans', 26),\n",
       "             ('ungraspable', 26),\n",
       "             ('phantom', 78),\n",
       "             ('life', 78),\n",
       "             ('key', 26),\n",
       "             ('am', 156),\n",
       "             ('habit', 26),\n",
       "             ('begin', 52),\n",
       "             ('grow', 52),\n",
       "             ('hazy', 26),\n",
       "             ('eyes', 182),\n",
       "             ('conscious', 26),\n",
       "             ('lungs', 26),\n",
       "             ('mean', 130),\n",
       "             ('inferred', 52),\n",
       "             ('needs', 52),\n",
       "             ('rag', 26),\n",
       "             ('something', 312),\n",
       "             ('besides', 156),\n",
       "             ('passengers', 78),\n",
       "             ('sick', 26),\n",
       "             ('quarrelsome', 26),\n",
       "             (\"don't\", 52),\n",
       "             ('nights', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('themselves', 52),\n",
       "             ('much', 442),\n",
       "             ('general', 26),\n",
       "             ('thing;--no', 26),\n",
       "             ('nor', 78),\n",
       "             ('salt', 26),\n",
       "             ('commodore', 52),\n",
       "             ('captain', 52),\n",
       "             ('cook', 52),\n",
       "             ('abandon', 26),\n",
       "             ('glory', 52),\n",
       "             ('distinction', 26),\n",
       "             ('offices', 26),\n",
       "             ('abominate', 26),\n",
       "             ('honourable', 26),\n",
       "             ('respectable', 26),\n",
       "             ('toils', 26),\n",
       "             ('trials', 26),\n",
       "             ('tribulations', 26),\n",
       "             ('kind', 78),\n",
       "             ('whatsoever', 52),\n",
       "             ('quite', 78),\n",
       "             ('care', 78),\n",
       "             ('taking', 104),\n",
       "             ('barques', 26),\n",
       "             ('brigs', 26),\n",
       "             ('schooners', 26),\n",
       "             ('cook,--though', 26),\n",
       "             ('confess', 52),\n",
       "             ('considerable', 26),\n",
       "             ('being', 390),\n",
       "             ('sort', 494),\n",
       "             ('officer', 52),\n",
       "             ('board', 78),\n",
       "             ('somehow', 78),\n",
       "             ('fancied', 26),\n",
       "             ('broiling', 26),\n",
       "             ('fowls;--though', 26),\n",
       "             ('broiled', 78),\n",
       "             ('judiciously', 26),\n",
       "             ('buttered', 26),\n",
       "             ('judgmatically', 26),\n",
       "             ('salted', 26),\n",
       "             ('peppered', 26),\n",
       "             ('speak', 130),\n",
       "             ('respectfully', 52),\n",
       "             ('reverentially', 26),\n",
       "             ('fowl', 26),\n",
       "             ('than', 390),\n",
       "             ('idolatrous', 26),\n",
       "             ('dotings', 26),\n",
       "             ('egyptians', 26),\n",
       "             ('ibis', 26),\n",
       "             ('roasted', 26),\n",
       "             ('river', 26),\n",
       "             ('horse', 52),\n",
       "             ('mummies', 26),\n",
       "             ('creatures', 26),\n",
       "             ('huge', 52),\n",
       "             ('bake', 26),\n",
       "             ('houses', 52),\n",
       "             ('pyramids', 26),\n",
       "             ('simple', 26),\n",
       "             ('sailor', 156),\n",
       "             ('mast', 78),\n",
       "             ('plumb', 26),\n",
       "             ('forecastle', 52),\n",
       "             ('royal', 26),\n",
       "             ('true', 104),\n",
       "             ('rather', 208),\n",
       "             ('order', 130),\n",
       "             ('make', 260),\n",
       "             ('jump', 52),\n",
       "             ('spar', 52),\n",
       "             ('grasshopper', 26),\n",
       "             ('may', 312),\n",
       "             ('thing', 104),\n",
       "             ('unpleasant', 26),\n",
       "             ('enough', 338),\n",
       "             ('touches', 26),\n",
       "             ('sense', 78),\n",
       "             ('honour', 26),\n",
       "             ('particularly', 52),\n",
       "             ('established', 26),\n",
       "             ('family', 26),\n",
       "             ('van', 26),\n",
       "             ('rensselaers', 26),\n",
       "             ('randolphs', 26),\n",
       "             ('hardicanutes', 26),\n",
       "             ('putting', 52),\n",
       "             ('tar', 52),\n",
       "             ('pot', 26),\n",
       "             ('been', 468),\n",
       "             ('lording', 26),\n",
       "             ('schoolmaster', 52),\n",
       "             ('making', 130),\n",
       "             ('tallest', 26),\n",
       "             ('boys', 52),\n",
       "             ('awe', 26),\n",
       "             ('transition', 52),\n",
       "             ('keen', 26),\n",
       "             ('assure', 26),\n",
       "             ('decoction', 26),\n",
       "             ('seneca', 26),\n",
       "             ('stoics', 26),\n",
       "             ('enable', 26),\n",
       "             ('grin', 52),\n",
       "             ('bear', 52),\n",
       "             ('even', 130),\n",
       "             ('wears', 26),\n",
       "             ('hunks', 52),\n",
       "             ('orders', 26),\n",
       "             ('broom', 26),\n",
       "             ('sweep', 52),\n",
       "             ('decks', 26),\n",
       "             ('indignity', 26),\n",
       "             ('amount', 26),\n",
       "             ('weighed', 52),\n",
       "             ('scales', 26),\n",
       "             ('new', 286),\n",
       "             ('testament', 26),\n",
       "             ('think', 182),\n",
       "             ('archangel', 26),\n",
       "             ('gabriel', 26),\n",
       "             ('thinks', 182),\n",
       "             ('anything', 52),\n",
       "             ('less', 52),\n",
       "             ('promptly', 26),\n",
       "             ('obey', 26),\n",
       "             ('instance', 26),\n",
       "             ('ai', 104),\n",
       "             (\"n't\", 624),\n",
       "             ('slave', 26),\n",
       "             ('well', 208),\n",
       "             ('however', 208),\n",
       "             ('captains', 26),\n",
       "             ('thump', 52),\n",
       "             ('punch', 26),\n",
       "             ('satisfaction', 26),\n",
       "             ('knowing', 78),\n",
       "             ('everybody', 26),\n",
       "             ('else', 208),\n",
       "             ('served', 26),\n",
       "             ('either', 78),\n",
       "             ('physical', 26),\n",
       "             ('point', 52),\n",
       "             ('view', 52),\n",
       "             ('so', 1066),\n",
       "             ('universal', 26),\n",
       "             ('passed', 78),\n",
       "             ('hands', 78),\n",
       "             ('rub', 26),\n",
       "             ('shoulder', 26),\n",
       "             ('blades', 26),\n",
       "             ('again', 286),\n",
       "             ('always', 104),\n",
       "             ('paying', 78),\n",
       "             ('trouble', 26),\n",
       "             ('whereas', 26),\n",
       "             ('pay', 78),\n",
       "             ('single', 52),\n",
       "             ('penny', 78),\n",
       "             ('heard', 208),\n",
       "             ('contrary', 26),\n",
       "             ('difference', 52),\n",
       "             ('between', 234),\n",
       "             ('paid', 26),\n",
       "             ('act', 52),\n",
       "             ('perhaps', 130),\n",
       "             ('uncomfortable', 52),\n",
       "             ('infliction', 26),\n",
       "             ('orchard', 26),\n",
       "             ('thieves', 26),\n",
       "             ('entailed', 26),\n",
       "             ('us', 104),\n",
       "             ('paid,--what', 26),\n",
       "             ('compare', 52),\n",
       "             ('urbane', 26),\n",
       "             ('activity', 26),\n",
       "             ('receives', 26),\n",
       "             ('really', 104),\n",
       "             ('marvellous', 104),\n",
       "             ('considering', 26),\n",
       "             ('earnestly', 26),\n",
       "             ('believe', 26),\n",
       "             ('root', 26),\n",
       "             ('earthly', 52),\n",
       "             ('ills', 26),\n",
       "             ('monied', 26),\n",
       "             ('enter', 52),\n",
       "             ('heaven', 104),\n",
       "             ('ah', 26),\n",
       "             ('cheerfully', 26),\n",
       "             ('consign', 26),\n",
       "             ('perdition', 26),\n",
       "             ('finally', 26),\n",
       "             ('wholesome', 26),\n",
       "             ('exercise', 26),\n",
       "             ('pure', 26),\n",
       "             ('air', 104),\n",
       "             ('fore', 26),\n",
       "             ('castle', 26),\n",
       "             ('deck', 52),\n",
       "             ('far', 104),\n",
       "             ('prevalent', 26),\n",
       "             ('astern', 26),\n",
       "             ('violate', 26),\n",
       "             ('pythagorean', 26),\n",
       "             ('maxim', 26),\n",
       "             ('quarter', 52),\n",
       "             ('gets', 26),\n",
       "             ('atmosphere', 26),\n",
       "             ('second', 104),\n",
       "             ('sailors', 78),\n",
       "             ('breathes', 26),\n",
       "             ('commonalty', 26),\n",
       "             ('leaders', 52),\n",
       "             ('many', 104),\n",
       "             ('things', 130),\n",
       "             ('suspect', 26),\n",
       "             ('wherefore', 26),\n",
       "             ('after', 234),\n",
       "             ('repeatedly', 26),\n",
       "             ('smelt', 52),\n",
       "             ('merchant', 26),\n",
       "             ('whaling', 234),\n",
       "             ('invisible', 26),\n",
       "             ('police', 26),\n",
       "             ('fates', 52),\n",
       "             ('has', 104),\n",
       "             ('constant', 26),\n",
       "             ('surveillance', 26),\n",
       "             ('secretly', 26),\n",
       "             ('dogs', 26),\n",
       "             ('influences', 26),\n",
       "             ('unaccountable', 104),\n",
       "             ('answer', 130),\n",
       "             ('doubtless', 52),\n",
       "             ('formed', 52),\n",
       "             ('grand', 104),\n",
       "             ('programme', 26),\n",
       "             ('providence', 26),\n",
       "             ('drawn', 26),\n",
       "             ('came', 286),\n",
       "             ('brief', 26),\n",
       "             ('interlude', 26),\n",
       "             ('solo', 26),\n",
       "             ('extensive', 26),\n",
       "             ('performances', 26),\n",
       "             ('bill', 26),\n",
       "             ('run', 52),\n",
       "             ('contested', 26),\n",
       "             ('election', 26),\n",
       "             ('presidency', 26),\n",
       "             ('united', 26),\n",
       "             ('states', 26),\n",
       "             ('bloody', 26),\n",
       "             ('battle', 26),\n",
       "             ('affghanistan', 26),\n",
       "             ('exactly', 78),\n",
       "             ('stage', 52),\n",
       "             ('managers', 26),\n",
       "             ('put', 208),\n",
       "             ('shabby', 52),\n",
       "             ('others', 52),\n",
       "             ('magnificent', 26),\n",
       "             ('parts', 130),\n",
       "             ('tragedies', 26),\n",
       "             ('short', 78),\n",
       "             ('easy', 78),\n",
       "             ('genteel', 26),\n",
       "             ('comedies', 26),\n",
       "             ('jolly', 104),\n",
       "             ('farces', 26),\n",
       "             ('recall', 26),\n",
       "             ('circumstances', 52),\n",
       "             ('springs', 26),\n",
       "             ('motives', 52),\n",
       "             ('cunningly', 26),\n",
       "             ('presented', 26),\n",
       "             ('various', 52),\n",
       "             ('disguises', 26),\n",
       "             ('induced', 26),\n",
       "             ('performing', 26),\n",
       "             ('cajoling', 26),\n",
       "             ('delusion', 26),\n",
       "             ('choice', 26),\n",
       "             ('resulting', 26),\n",
       "             ('unbiased', 26),\n",
       "             ('freewill', 26),\n",
       "             ('discriminating', 26),\n",
       "             ('judgment', 26),\n",
       "             ('overwhelming', 26),\n",
       "             ('idea', 182),\n",
       "             ('whale', 260),\n",
       "             ('portentous', 78),\n",
       "             ('mysterious', 52),\n",
       "             ('monster', 26),\n",
       "             ('roused', 26),\n",
       "             ('curiosity', 52),\n",
       "             ('wild', 130),\n",
       "             ('seas', 156),\n",
       "             ('rolled', 156),\n",
       "             ('island', 78),\n",
       "             ('bulk', 26),\n",
       "             ('undeliverable', 26),\n",
       "             ('nameless', 78),\n",
       "             ('perils', 26),\n",
       "             ('attending', 26),\n",
       "             ('marvels', 26),\n",
       "             ('patagonian', 26),\n",
       "             ('sights', 26),\n",
       "             ('sounds', 78),\n",
       "             ('helped', 26),\n",
       "             ('sway', 26),\n",
       "             ('wish', 26),\n",
       "             ('inducements', 26),\n",
       "             ('tormented', 52),\n",
       "             ('everlasting', 52),\n",
       "             ('itch', 26),\n",
       "             ('remote', 26),\n",
       "             ('love', 26),\n",
       "             ('forbidden', 26),\n",
       "             ('barbarous', 26),\n",
       "             ('coasts', 26),\n",
       "             ('ignoring', 26),\n",
       "             ('good', 442),\n",
       "             ('quick', 26),\n",
       "             ('perceive', 26),\n",
       "             ('horror', 26),\n",
       "             ('social', 26),\n",
       "             ('since', 78),\n",
       "             ('friendly', 26),\n",
       "             ('terms', 26),\n",
       "             ('inmates', 26),\n",
       "             ('place', 390),\n",
       "             ('lodges', 26),\n",
       "             ('reason', 130),\n",
       "             ('welcome', 26),\n",
       "             ('flood', 26),\n",
       "             ('gates', 26),\n",
       "             ('wonder', 52),\n",
       "             ('swung', 26),\n",
       "             ('open', 104),\n",
       "             ('conceits', 26),\n",
       "             ('swayed', 26),\n",
       "             ('purpose', 52),\n",
       "             ('floated', 52),\n",
       "             ('inmost', 26),\n",
       "             ('endless', 26),\n",
       "             ('processions', 26),\n",
       "             ('mid', 26),\n",
       "             ('hooded', 26),\n",
       "             ('snow', 78),\n",
       "             ('stuffed', 52),\n",
       "             ('shirt', 104),\n",
       "             ('carpet', 26),\n",
       "             ('bag', 182),\n",
       "             ('tucked', 26),\n",
       "             ('arm', 286),\n",
       "             ('started', 26),\n",
       "             ('cape', 104),\n",
       "             ('horn', 52),\n",
       "             ('pacific', 26),\n",
       "             ('quitting', 26),\n",
       "             ('manhatto', 26),\n",
       "             ('duly', 26),\n",
       "             ('arrived', 52),\n",
       "             ('bedford', 104),\n",
       "             ('saturday', 78),\n",
       "             ('night', 624),\n",
       "             ('december', 26),\n",
       "             ('disappointed', 26),\n",
       "             ('learning', 26),\n",
       "             ('packet', 26),\n",
       "             ('nantucket', 182),\n",
       "             ('had', 858),\n",
       "             ('already', 26),\n",
       "             ('sailed', 26),\n",
       "             ('offer', 52),\n",
       "             ('till', 156),\n",
       "             ('following', 52),\n",
       "             ('monday', 26),\n",
       "             ('young', 130),\n",
       "             ('candidates', 26),\n",
       "             ('pains', 26),\n",
       "             ('penalties', 26),\n",
       "             ('stop', 208),\n",
       "             ('embark', 52),\n",
       "             ('related', 26),\n",
       "             ('doing', 26),\n",
       "             ('made', 338),\n",
       "             ('craft', 130),\n",
       "             ('fine', 104),\n",
       "             ('boisterous', 26),\n",
       "             ('everything', 26),\n",
       "             ('connected', 26),\n",
       "             ('famous', 26),\n",
       "             ('amazingly', 26),\n",
       "             ('pleased', 52),\n",
       "             ('late', 182),\n",
       "             ('gradually', 26),\n",
       "             ('monopolising', 26),\n",
       "             ('business', 130),\n",
       "             ('matter', 78),\n",
       "             ('behind', 26),\n",
       "             ('original', 52),\n",
       "             ('tyre', 26),\n",
       "             ('carthage;--the', 26),\n",
       "             ('dead', 130),\n",
       "             ('stranded', 52),\n",
       "             ('aboriginal', 26),\n",
       "             ('whalemen', 26),\n",
       "             ('red', 78),\n",
       "             ('sally', 26),\n",
       "             ('canoes', 26),\n",
       "             ('chase', 26),\n",
       "             ('leviathan', 52),\n",
       "             ('too', 364),\n",
       "             ('adventurous', 26),\n",
       "             ('sloop', 26),\n",
       "             ('forth', 26),\n",
       "             ('partly', 78),\n",
       "             ('laden', 26),\n",
       "             ('imported', 26),\n",
       "             ('cobblestones', 26),\n",
       "             ('throw', 26),\n",
       "             ('whales', 52),\n",
       "             ('discover', 26),\n",
       "             ('risk', 26),\n",
       "             ('harpoon', 135),\n",
       "             ('bowsprit', 26),\n",
       "             ('day', 156),\n",
       "             ('another', 130),\n",
       "             ('ere', 78),\n",
       "             ('destined', 26),\n",
       "             ('port', 26),\n",
       "             ('became', 78),\n",
       "             ('concernment', 26),\n",
       "             ('eat', 26),\n",
       "             ('meanwhile', 78),\n",
       "             ('dubious', 26),\n",
       "             ('nay', 52),\n",
       "             ('dark', 208),\n",
       "             ('dismal', 52),\n",
       "             ('bitingly', 26),\n",
       "             ('cold', 182),\n",
       "             ('cheerless', 26),\n",
       "             ('anxious', 26),\n",
       "             ('grapnels', 26),\n",
       "             ('sounded', 26),\n",
       "             ('pocket', 78),\n",
       "             ('only', 364),\n",
       "             ('brought', 26),\n",
       "             ('pieces', 26),\n",
       "             ('silver,--so', 26),\n",
       "             ('wherever', 52),\n",
       "             ('said', 494),\n",
       "             ('stood', 260),\n",
       "             ('middle', 130),\n",
       "             ('dreary', 52),\n",
       "             ('shouldering', 26),\n",
       "             ('comparing', 26),\n",
       "             ('gloom', 26),\n",
       "             ('darkness', 78),\n",
       "             ('wisdom', 26),\n",
       "             ('conclude', 26),\n",
       "             ('lodge', 26),\n",
       "             ('dear', 26),\n",
       "             ('sure', 130),\n",
       "             ('inquire', 26),\n",
       "             ('price', 26),\n",
       "             ('halting', 26),\n",
       "             ('steps', 26),\n",
       "             ('paced', 26),\n",
       "             ('sign', 156),\n",
       "             ('crossed', 52),\n",
       "             ('harpoons\"--but', 26),\n",
       "             ('looked', 156),\n",
       "             ('expensive', 52),\n",
       "             ('further', 104),\n",
       "             ('bright', 52),\n",
       "             ('windows', 52),\n",
       "             ('fish', 78),\n",
       "             ('inn', 78),\n",
       "             ('fervent', 26),\n",
       "             ('rays', 26),\n",
       "             ('seemed', 416),\n",
       "             ('melted', 26),\n",
       "             ('packed', 52),\n",
       "             ('ice', 104),\n",
       "             ('house', 286),\n",
       "             ('everywhere', 26),\n",
       "             ('congealed', 26),\n",
       "             ('frost', 104),\n",
       "             ('lay', 234),\n",
       "             ('inches', 52),\n",
       "             ('thick', 52),\n",
       "             ...])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want this to be a matrix\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2712,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2711, ...,   53,    2, 2717],\n",
       "       [ 166, 2711,    3, ...,    2, 2717,   26]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all columns but the last\n",
    "X = sequences[:,:-1]\n",
    "# Grab only the last column\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note the syntax for factor selection\n",
    "w = np.array([[1,2,3],[4,5,6]])\n",
    "display(w)\n",
    "display(w[:,:-1])\n",
    "display(w[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 25)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size,seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # turns positive integers into dense vectors of a fixed size\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=seq_len, input_length=seq_len))\n",
    "    # Common to make this some multiple of your sequence length: seq_len*10 = units = #neurons\n",
    "    model.add(LSTM(units=seq_len*10, return_sequences=True))\n",
    "    model.add(LSTM(units=seq_len*10))\n",
    "    model.add(Dense(units=seq_len*10, activation='relu'))\n",
    "    \n",
    "    # output of network - a word\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            67950     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 25, 250)           276000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 250)               501000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2718)              682218    \n",
      "=================================================================\n",
      "Total params: 1,589,918\n",
      "Trainable params: 1,589,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/daiglechris/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      "11312/11312 [==============================] - 41s 4ms/sample - loss: 6.8054 - acc: 0.0454\n",
      "Epoch 2/2\n",
      "11312/11312 [==============================] - 38s 3ms/sample - loss: 6.3756 - acc: 0.0492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf0ebbc550>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text based off of a seeded input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate new text\n",
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    \"\"\"\n",
    "    Function that takes in some text and returns some predicted words to complete the sentence\n",
    "    \n",
    "    model: trained DL model\n",
    "    tokenizer: association of vocabulary and what ID number goes with what word\n",
    "    seq_len: length of the sequence of expected texted\n",
    "    seed_text: input text (preferably the same length as the sequence)\n",
    "    num_gen_words: number of words to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len,truncating='pre')\n",
    "        \n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        \n",
    "        input_text += ' '+pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thought',\n",
       " 'i',\n",
       " 'to',\n",
       " 'myself',\n",
       " 'the',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'human',\n",
       " 'being',\n",
       " 'just',\n",
       " 'as',\n",
       " 'i',\n",
       " 'am',\n",
       " 'he',\n",
       " 'has',\n",
       " 'just',\n",
       " 'as',\n",
       " 'much',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'fear',\n",
       " 'me',\n",
       " 'as',\n",
       " 'i',\n",
       " 'have']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be seen there was no bad olfactories my own letter was cheerily listening over his hearers who 's more can go have a wearing\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load in a much more deeply trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('epochBIG.h5')\n",
    "tokenizer = load(open('epochBIG','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board existence ha often massa parcel how a thing alone touching broken shy cried sir free on boys my hail here rings a special here'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=text_sequences[0],num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
