{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use one-hot-encoding to use in Keras\n",
    "# Class_0 --> [1,0,0]\n",
    "# Class_1 --> [0,1,0]\n",
    "# Class_2 --> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([5,10,15,20])/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only fit to the training features\n",
    "scaler_obj.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_obj.transform(X_train)\n",
    "scaled_X_test = scaler_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Units: neurons/nodes\n",
    "# Input_dim: number of features/factors/x-variables to be input\n",
    "# activation: function for node\n",
    "model.add(Dense(units=8, input_dim=4, activation='sigmoid'))\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mbair/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/700\n",
      " - 1s - loss: 1.1010 - acc: 0.3500\n",
      "Epoch 2/700\n",
      " - 0s - loss: 1.0969 - acc: 0.3500\n",
      "Epoch 3/700\n",
      " - 0s - loss: 1.0930 - acc: 0.3500\n",
      "Epoch 4/700\n",
      " - 0s - loss: 1.0915 - acc: 0.3500\n",
      "Epoch 5/700\n",
      " - 0s - loss: 1.0892 - acc: 0.3500\n",
      "Epoch 6/700\n",
      " - 0s - loss: 1.0875 - acc: 0.3500\n",
      "Epoch 7/700\n",
      " - 0s - loss: 1.0859 - acc: 0.3500\n",
      "Epoch 8/700\n",
      " - 0s - loss: 1.0843 - acc: 0.3500\n",
      "Epoch 9/700\n",
      " - 0s - loss: 1.0832 - acc: 0.3500\n",
      "Epoch 10/700\n",
      " - 0s - loss: 1.0817 - acc: 0.3500\n",
      "Epoch 11/700\n",
      " - 0s - loss: 1.0793 - acc: 0.3500\n",
      "Epoch 12/700\n",
      " - 0s - loss: 1.0770 - acc: 0.3500\n",
      "Epoch 13/700\n",
      " - 0s - loss: 1.0746 - acc: 0.3500\n",
      "Epoch 14/700\n",
      " - 0s - loss: 1.0728 - acc: 0.3600\n",
      "Epoch 15/700\n",
      " - 0s - loss: 1.0706 - acc: 0.3900\n",
      "Epoch 16/700\n",
      " - 0s - loss: 1.0691 - acc: 0.4200\n",
      "Epoch 17/700\n",
      " - 0s - loss: 1.0668 - acc: 0.4400\n",
      "Epoch 18/700\n",
      " - 0s - loss: 1.0654 - acc: 0.4700\n",
      "Epoch 19/700\n",
      " - 0s - loss: 1.0635 - acc: 0.5100\n",
      "Epoch 20/700\n",
      " - 0s - loss: 1.0618 - acc: 0.5300\n",
      "Epoch 21/700\n",
      " - 0s - loss: 1.0595 - acc: 0.5200\n",
      "Epoch 22/700\n",
      " - 0s - loss: 1.0577 - acc: 0.5600\n",
      "Epoch 23/700\n",
      " - 0s - loss: 1.0556 - acc: 0.5600\n",
      "Epoch 24/700\n",
      " - 0s - loss: 1.0533 - acc: 0.5600\n",
      "Epoch 25/700\n",
      " - 0s - loss: 1.0511 - acc: 0.5600\n",
      "Epoch 26/700\n",
      " - 0s - loss: 1.0490 - acc: 0.5600\n",
      "Epoch 27/700\n",
      " - 0s - loss: 1.0469 - acc: 0.5400\n",
      "Epoch 28/700\n",
      " - 0s - loss: 1.0453 - acc: 0.5100\n",
      "Epoch 29/700\n",
      " - 0s - loss: 1.0433 - acc: 0.4900\n",
      "Epoch 30/700\n",
      " - 0s - loss: 1.0409 - acc: 0.5100\n",
      "Epoch 31/700\n",
      " - 0s - loss: 1.0384 - acc: 0.5100\n",
      "Epoch 32/700\n",
      " - 0s - loss: 1.0359 - acc: 0.5200\n",
      "Epoch 33/700\n",
      " - 0s - loss: 1.0334 - acc: 0.5500\n",
      "Epoch 34/700\n",
      " - 0s - loss: 1.0307 - acc: 0.5600\n",
      "Epoch 35/700\n",
      " - 0s - loss: 1.0274 - acc: 0.5600\n",
      "Epoch 36/700\n",
      " - 0s - loss: 1.0245 - acc: 0.5500\n",
      "Epoch 37/700\n",
      " - 0s - loss: 1.0217 - acc: 0.5700\n",
      "Epoch 38/700\n",
      " - 0s - loss: 1.0182 - acc: 0.5800\n",
      "Epoch 39/700\n",
      " - 0s - loss: 1.0150 - acc: 0.5800\n",
      "Epoch 40/700\n",
      " - 0s - loss: 1.0119 - acc: 0.5700\n",
      "Epoch 41/700\n",
      " - 0s - loss: 1.0087 - acc: 0.5900\n",
      "Epoch 42/700\n",
      " - 0s - loss: 1.0052 - acc: 0.5900\n",
      "Epoch 43/700\n",
      " - 0s - loss: 1.0014 - acc: 0.5700\n",
      "Epoch 44/700\n",
      " - 0s - loss: 0.9982 - acc: 0.5600\n",
      "Epoch 45/700\n",
      " - 0s - loss: 0.9945 - acc: 0.5600\n",
      "Epoch 46/700\n",
      " - 0s - loss: 0.9905 - acc: 0.5600\n",
      "Epoch 47/700\n",
      " - 0s - loss: 0.9867 - acc: 0.5600\n",
      "Epoch 48/700\n",
      " - 0s - loss: 0.9834 - acc: 0.5300\n",
      "Epoch 49/700\n",
      " - 0s - loss: 0.9795 - acc: 0.5100\n",
      "Epoch 50/700\n",
      " - 0s - loss: 0.9749 - acc: 0.4700\n",
      "Epoch 51/700\n",
      " - 0s - loss: 0.9709 - acc: 0.4200\n",
      "Epoch 52/700\n",
      " - 0s - loss: 0.9672 - acc: 0.4500\n",
      "Epoch 53/700\n",
      " - 0s - loss: 0.9626 - acc: 0.4800\n",
      "Epoch 54/700\n",
      " - 0s - loss: 0.9571 - acc: 0.6400\n",
      "Epoch 55/700\n",
      " - 0s - loss: 0.9525 - acc: 0.7400\n",
      "Epoch 56/700\n",
      " - 0s - loss: 0.9468 - acc: 0.8200\n",
      "Epoch 57/700\n",
      " - 0s - loss: 0.9420 - acc: 0.8500\n",
      "Epoch 58/700\n",
      " - 0s - loss: 0.9365 - acc: 0.8700\n",
      "Epoch 59/700\n",
      " - 0s - loss: 0.9314 - acc: 0.8900\n",
      "Epoch 60/700\n",
      " - 0s - loss: 0.9262 - acc: 0.9000\n",
      "Epoch 61/700\n",
      " - 0s - loss: 0.9206 - acc: 0.9100\n",
      "Epoch 62/700\n",
      " - 0s - loss: 0.9146 - acc: 0.9100\n",
      "Epoch 63/700\n",
      " - 0s - loss: 0.9096 - acc: 0.9000\n",
      "Epoch 64/700\n",
      " - 0s - loss: 0.9032 - acc: 0.9100\n",
      "Epoch 65/700\n",
      " - 0s - loss: 0.8970 - acc: 0.9100\n",
      "Epoch 66/700\n",
      " - 0s - loss: 0.8909 - acc: 0.9000\n",
      "Epoch 67/700\n",
      " - 0s - loss: 0.8844 - acc: 0.9000\n",
      "Epoch 68/700\n",
      " - 0s - loss: 0.8765 - acc: 0.9200\n",
      "Epoch 69/700\n",
      " - 0s - loss: 0.8689 - acc: 0.9100\n",
      "Epoch 70/700\n",
      " - 0s - loss: 0.8624 - acc: 0.8700\n",
      "Epoch 71/700\n",
      " - 0s - loss: 0.8565 - acc: 0.8400\n",
      "Epoch 72/700\n",
      " - 0s - loss: 0.8490 - acc: 0.8300\n",
      "Epoch 73/700\n",
      " - 0s - loss: 0.8419 - acc: 0.8100\n",
      "Epoch 74/700\n",
      " - 0s - loss: 0.8324 - acc: 0.8100\n",
      "Epoch 75/700\n",
      " - 0s - loss: 0.8236 - acc: 0.8100\n",
      "Epoch 76/700\n",
      " - 0s - loss: 0.8153 - acc: 0.8300\n",
      "Epoch 77/700\n",
      " - 0s - loss: 0.8070 - acc: 0.8600\n",
      "Epoch 78/700\n",
      " - 0s - loss: 0.7988 - acc: 0.8500\n",
      "Epoch 79/700\n",
      " - 0s - loss: 0.7902 - acc: 0.8400\n",
      "Epoch 80/700\n",
      " - 0s - loss: 0.7815 - acc: 0.8400\n",
      "Epoch 81/700\n",
      " - 0s - loss: 0.7728 - acc: 0.8400\n",
      "Epoch 82/700\n",
      " - 0s - loss: 0.7642 - acc: 0.8400\n",
      "Epoch 83/700\n",
      " - 0s - loss: 0.7551 - acc: 0.8400\n",
      "Epoch 84/700\n",
      " - 0s - loss: 0.7463 - acc: 0.8300\n",
      "Epoch 85/700\n",
      " - 0s - loss: 0.7380 - acc: 0.8200\n",
      "Epoch 86/700\n",
      " - 0s - loss: 0.7294 - acc: 0.8200\n",
      "Epoch 87/700\n",
      " - 0s - loss: 0.7207 - acc: 0.8300\n",
      "Epoch 88/700\n",
      " - 0s - loss: 0.7118 - acc: 0.8300\n",
      "Epoch 89/700\n",
      " - 0s - loss: 0.7031 - acc: 0.8300\n",
      "Epoch 90/700\n",
      " - 0s - loss: 0.6943 - acc: 0.8400\n",
      "Epoch 91/700\n",
      " - 0s - loss: 0.6862 - acc: 0.8300\n",
      "Epoch 92/700\n",
      " - 0s - loss: 0.6767 - acc: 0.8500\n",
      "Epoch 93/700\n",
      " - 0s - loss: 0.6682 - acc: 0.8600\n",
      "Epoch 94/700\n",
      " - 0s - loss: 0.6607 - acc: 0.8700\n",
      "Epoch 95/700\n",
      " - 0s - loss: 0.6526 - acc: 0.8800\n",
      "Epoch 96/700\n",
      " - 0s - loss: 0.6443 - acc: 0.8800\n",
      "Epoch 97/700\n",
      " - 0s - loss: 0.6331 - acc: 0.8700\n",
      "Epoch 98/700\n",
      " - 0s - loss: 0.6211 - acc: 0.8700\n",
      "Epoch 99/700\n",
      " - 0s - loss: 0.6094 - acc: 0.8700\n",
      "Epoch 100/700\n",
      " - 0s - loss: 0.5982 - acc: 0.8800\n",
      "Epoch 101/700\n",
      " - 0s - loss: 0.5882 - acc: 0.8800\n",
      "Epoch 102/700\n",
      " - 0s - loss: 0.5794 - acc: 0.8800\n",
      "Epoch 103/700\n",
      " - 0s - loss: 0.5705 - acc: 0.8800\n",
      "Epoch 104/700\n",
      " - 0s - loss: 0.5620 - acc: 0.8900\n",
      "Epoch 105/700\n",
      " - 0s - loss: 0.5557 - acc: 0.8800\n",
      "Epoch 106/700\n",
      " - 0s - loss: 0.5483 - acc: 0.8800\n",
      "Epoch 107/700\n",
      " - 0s - loss: 0.5387 - acc: 0.8900\n",
      "Epoch 108/700\n",
      " - 0s - loss: 0.5324 - acc: 0.8700\n",
      "Epoch 109/700\n",
      " - 0s - loss: 0.5245 - acc: 0.8600\n",
      "Epoch 110/700\n",
      " - 0s - loss: 0.5171 - acc: 0.8700\n",
      "Epoch 111/700\n",
      " - 0s - loss: 0.5100 - acc: 0.8700\n",
      "Epoch 112/700\n",
      " - 0s - loss: 0.5046 - acc: 0.8700\n",
      "Epoch 113/700\n",
      " - 0s - loss: 0.4968 - acc: 0.8800\n",
      "Epoch 114/700\n",
      " - 0s - loss: 0.4896 - acc: 0.8900\n",
      "Epoch 115/700\n",
      " - 0s - loss: 0.4837 - acc: 0.9000\n",
      "Epoch 116/700\n",
      " - 0s - loss: 0.4802 - acc: 0.9200\n",
      "Epoch 117/700\n",
      " - 0s - loss: 0.4739 - acc: 0.9300\n",
      "Epoch 118/700\n",
      " - 0s - loss: 0.4667 - acc: 0.9100\n",
      "Epoch 119/700\n",
      " - 0s - loss: 0.4600 - acc: 0.9000\n",
      "Epoch 120/700\n",
      " - 0s - loss: 0.4545 - acc: 0.9000\n",
      "Epoch 121/700\n",
      " - 0s - loss: 0.4494 - acc: 0.9000\n",
      "Epoch 122/700\n",
      " - 0s - loss: 0.4436 - acc: 0.9000\n",
      "Epoch 123/700\n",
      " - 0s - loss: 0.4387 - acc: 0.9200\n",
      "Epoch 124/700\n",
      " - 0s - loss: 0.4335 - acc: 0.9300\n",
      "Epoch 125/700\n",
      " - 0s - loss: 0.4296 - acc: 0.9200\n",
      "Epoch 126/700\n",
      " - 0s - loss: 0.4242 - acc: 0.9200\n",
      "Epoch 127/700\n",
      " - 0s - loss: 0.4179 - acc: 0.9300\n",
      "Epoch 128/700\n",
      " - 0s - loss: 0.4130 - acc: 0.9200\n",
      "Epoch 129/700\n",
      " - 0s - loss: 0.4076 - acc: 0.9200\n",
      "Epoch 130/700\n",
      " - 0s - loss: 0.4044 - acc: 0.9200\n",
      "Epoch 131/700\n",
      " - 0s - loss: 0.3989 - acc: 0.9300\n",
      "Epoch 132/700\n",
      " - 0s - loss: 0.3943 - acc: 0.9300\n",
      "Epoch 133/700\n",
      " - 0s - loss: 0.3901 - acc: 0.9300\n",
      "Epoch 134/700\n",
      " - 0s - loss: 0.3869 - acc: 0.9100\n",
      "Epoch 135/700\n",
      " - 0s - loss: 0.3840 - acc: 0.9000\n",
      "Epoch 136/700\n",
      " - 0s - loss: 0.3795 - acc: 0.9000\n",
      "Epoch 137/700\n",
      " - 0s - loss: 0.3761 - acc: 0.9200\n",
      "Epoch 138/700\n",
      " - 0s - loss: 0.3697 - acc: 0.9300\n",
      "Epoch 139/700\n",
      " - 0s - loss: 0.3659 - acc: 0.9200\n",
      "Epoch 140/700\n",
      " - 0s - loss: 0.3630 - acc: 0.9200\n",
      "Epoch 141/700\n",
      " - 0s - loss: 0.3592 - acc: 0.9200\n",
      "Epoch 142/700\n",
      " - 0s - loss: 0.3553 - acc: 0.9200\n",
      "Epoch 143/700\n",
      " - 0s - loss: 0.3510 - acc: 0.9200\n",
      "Epoch 144/700\n",
      " - 0s - loss: 0.3470 - acc: 0.9300\n",
      "Epoch 145/700\n",
      " - 0s - loss: 0.3436 - acc: 0.9300\n",
      "Epoch 146/700\n",
      " - 0s - loss: 0.3397 - acc: 0.9400\n",
      "Epoch 147/700\n",
      " - 0s - loss: 0.3389 - acc: 0.9400\n",
      "Epoch 148/700\n",
      " - 0s - loss: 0.3345 - acc: 0.9400\n",
      "Epoch 149/700\n",
      " - 0s - loss: 0.3301 - acc: 0.9300\n",
      "Epoch 150/700\n",
      " - 0s - loss: 0.3275 - acc: 0.9300\n",
      "Epoch 151/700\n",
      " - 0s - loss: 0.3302 - acc: 0.9200\n",
      "Epoch 152/700\n",
      " - 0s - loss: 0.3268 - acc: 0.9000\n",
      "Epoch 153/700\n",
      " - 0s - loss: 0.3199 - acc: 0.9200\n",
      "Epoch 154/700\n",
      " - 0s - loss: 0.3139 - acc: 0.9400\n",
      "Epoch 155/700\n",
      " - 0s - loss: 0.3131 - acc: 0.9400\n",
      "Epoch 156/700\n",
      " - 0s - loss: 0.3099 - acc: 0.9400\n",
      "Epoch 157/700\n",
      " - 0s - loss: 0.3062 - acc: 0.9400\n",
      "Epoch 158/700\n",
      " - 0s - loss: 0.3033 - acc: 0.9300\n",
      "Epoch 159/700\n",
      " - 0s - loss: 0.3008 - acc: 0.9400\n",
      "Epoch 160/700\n",
      " - 0s - loss: 0.2990 - acc: 0.9300\n",
      "Epoch 161/700\n",
      " - 0s - loss: 0.2959 - acc: 0.9400\n",
      "Epoch 162/700\n",
      " - 0s - loss: 0.2929 - acc: 0.9400\n",
      "Epoch 163/700\n",
      " - 0s - loss: 0.2904 - acc: 0.9300\n",
      "Epoch 164/700\n",
      " - 0s - loss: 0.2871 - acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/700\n",
      " - 0s - loss: 0.2846 - acc: 0.9400\n",
      "Epoch 166/700\n",
      " - 0s - loss: 0.2843 - acc: 0.9200\n",
      "Epoch 167/700\n",
      " - 0s - loss: 0.2868 - acc: 0.9300\n",
      "Epoch 168/700\n",
      " - 0s - loss: 0.2846 - acc: 0.9300\n",
      "Epoch 169/700\n",
      " - 0s - loss: 0.2777 - acc: 0.9400\n",
      "Epoch 170/700\n",
      " - 0s - loss: 0.2729 - acc: 0.9300\n",
      "Epoch 171/700\n",
      " - 0s - loss: 0.2695 - acc: 0.9400\n",
      "Epoch 172/700\n",
      " - 0s - loss: 0.2680 - acc: 0.9400\n",
      "Epoch 173/700\n",
      " - 0s - loss: 0.2654 - acc: 0.9400\n",
      "Epoch 174/700\n",
      " - 0s - loss: 0.2625 - acc: 0.9400\n",
      "Epoch 175/700\n",
      " - 0s - loss: 0.2599 - acc: 0.9400\n",
      "Epoch 176/700\n",
      " - 0s - loss: 0.2603 - acc: 0.9300\n",
      "Epoch 177/700\n",
      " - 0s - loss: 0.2595 - acc: 0.9400\n",
      "Epoch 178/700\n",
      " - 0s - loss: 0.2567 - acc: 0.9400\n",
      "Epoch 179/700\n",
      " - 0s - loss: 0.2525 - acc: 0.9300\n",
      "Epoch 180/700\n",
      " - 0s - loss: 0.2496 - acc: 0.9400\n",
      "Epoch 181/700\n",
      " - 0s - loss: 0.2471 - acc: 0.9400\n",
      "Epoch 182/700\n",
      " - 0s - loss: 0.2450 - acc: 0.9400\n",
      "Epoch 183/700\n",
      " - 0s - loss: 0.2431 - acc: 0.9400\n",
      "Epoch 184/700\n",
      " - 0s - loss: 0.2412 - acc: 0.9400\n",
      "Epoch 185/700\n",
      " - 0s - loss: 0.2400 - acc: 0.9400\n",
      "Epoch 186/700\n",
      " - 0s - loss: 0.2366 - acc: 0.9400\n",
      "Epoch 187/700\n",
      " - 0s - loss: 0.2353 - acc: 0.9400\n",
      "Epoch 188/700\n",
      " - 0s - loss: 0.2345 - acc: 0.9400\n",
      "Epoch 189/700\n",
      " - 0s - loss: 0.2308 - acc: 0.9400\n",
      "Epoch 190/700\n",
      " - 0s - loss: 0.2308 - acc: 0.9500\n",
      "Epoch 191/700\n",
      " - 0s - loss: 0.2305 - acc: 0.9500\n",
      "Epoch 192/700\n",
      " - 0s - loss: 0.2274 - acc: 0.9500\n",
      "Epoch 193/700\n",
      " - 0s - loss: 0.2261 - acc: 0.9500\n",
      "Epoch 194/700\n",
      " - 0s - loss: 0.2252 - acc: 0.9400\n",
      "Epoch 195/700\n",
      " - 0s - loss: 0.2232 - acc: 0.9500\n",
      "Epoch 196/700\n",
      " - 0s - loss: 0.2210 - acc: 0.9400\n",
      "Epoch 197/700\n",
      " - 0s - loss: 0.2187 - acc: 0.9500\n",
      "Epoch 198/700\n",
      " - 0s - loss: 0.2200 - acc: 0.9400\n",
      "Epoch 199/700\n",
      " - 0s - loss: 0.2222 - acc: 0.9500\n",
      "Epoch 200/700\n",
      " - 0s - loss: 0.2212 - acc: 0.9300\n",
      "Epoch 201/700\n",
      " - 0s - loss: 0.2175 - acc: 0.9300\n",
      "Epoch 202/700\n",
      " - 0s - loss: 0.2130 - acc: 0.9400\n",
      "Epoch 203/700\n",
      " - 0s - loss: 0.2109 - acc: 0.9400\n",
      "Epoch 204/700\n",
      " - 0s - loss: 0.2086 - acc: 0.9500\n",
      "Epoch 205/700\n",
      " - 0s - loss: 0.2087 - acc: 0.9500\n",
      "Epoch 206/700\n",
      " - 0s - loss: 0.2051 - acc: 0.9400\n",
      "Epoch 207/700\n",
      " - 0s - loss: 0.2021 - acc: 0.9500\n",
      "Epoch 208/700\n",
      " - 0s - loss: 0.2044 - acc: 0.9400\n",
      "Epoch 209/700\n",
      " - 0s - loss: 0.2048 - acc: 0.9400\n",
      "Epoch 210/700\n",
      " - 0s - loss: 0.2041 - acc: 0.9500\n",
      "Epoch 211/700\n",
      " - 0s - loss: 0.2018 - acc: 0.9400\n",
      "Epoch 212/700\n",
      " - 0s - loss: 0.1987 - acc: 0.9400\n",
      "Epoch 213/700\n",
      " - 0s - loss: 0.1953 - acc: 0.9400\n",
      "Epoch 214/700\n",
      " - 0s - loss: 0.2003 - acc: 0.9400\n",
      "Epoch 215/700\n",
      " - 0s - loss: 0.2032 - acc: 0.9300\n",
      "Epoch 216/700\n",
      " - 0s - loss: 0.1980 - acc: 0.9400\n",
      "Epoch 217/700\n",
      " - 0s - loss: 0.1937 - acc: 0.9500\n",
      "Epoch 218/700\n",
      " - 0s - loss: 0.1909 - acc: 0.9400\n",
      "Epoch 219/700\n",
      " - 0s - loss: 0.1888 - acc: 0.9500\n",
      "Epoch 220/700\n",
      " - 0s - loss: 0.1873 - acc: 0.9500\n",
      "Epoch 221/700\n",
      " - 0s - loss: 0.1866 - acc: 0.9400\n",
      "Epoch 222/700\n",
      " - 0s - loss: 0.1862 - acc: 0.9400\n",
      "Epoch 223/700\n",
      " - 0s - loss: 0.1847 - acc: 0.9500\n",
      "Epoch 224/700\n",
      " - 0s - loss: 0.1836 - acc: 0.9500\n",
      "Epoch 225/700\n",
      " - 0s - loss: 0.1825 - acc: 0.9500\n",
      "Epoch 226/700\n",
      " - 0s - loss: 0.1814 - acc: 0.9500\n",
      "Epoch 227/700\n",
      " - 0s - loss: 0.1801 - acc: 0.9500\n",
      "Epoch 228/700\n",
      " - 0s - loss: 0.1804 - acc: 0.9400\n",
      "Epoch 229/700\n",
      " - 0s - loss: 0.1814 - acc: 0.9400\n",
      "Epoch 230/700\n",
      " - 0s - loss: 0.1818 - acc: 0.9400\n",
      "Epoch 231/700\n",
      " - 0s - loss: 0.1789 - acc: 0.9400\n",
      "Epoch 232/700\n",
      " - 0s - loss: 0.1776 - acc: 0.9400\n",
      "Epoch 233/700\n",
      " - 0s - loss: 0.1745 - acc: 0.9400\n",
      "Epoch 234/700\n",
      " - 0s - loss: 0.1735 - acc: 0.9500\n",
      "Epoch 235/700\n",
      " - 0s - loss: 0.1740 - acc: 0.9500\n",
      "Epoch 236/700\n",
      " - 0s - loss: 0.1730 - acc: 0.9500\n",
      "Epoch 237/700\n",
      " - 0s - loss: 0.1710 - acc: 0.9500\n",
      "Epoch 238/700\n",
      " - 0s - loss: 0.1698 - acc: 0.9400\n",
      "Epoch 239/700\n",
      " - 0s - loss: 0.1683 - acc: 0.9500\n",
      "Epoch 240/700\n",
      " - 0s - loss: 0.1673 - acc: 0.9500\n",
      "Epoch 241/700\n",
      " - 0s - loss: 0.1664 - acc: 0.9500\n",
      "Epoch 242/700\n",
      " - 0s - loss: 0.1658 - acc: 0.9500\n",
      "Epoch 243/700\n",
      " - 0s - loss: 0.1672 - acc: 0.9400\n",
      "Epoch 244/700\n",
      " - 0s - loss: 0.1759 - acc: 0.9500\n",
      "Epoch 245/700\n",
      " - 0s - loss: 0.1767 - acc: 0.9500\n",
      "Epoch 246/700\n",
      " - 0s - loss: 0.1703 - acc: 0.9400\n",
      "Epoch 247/700\n",
      " - 0s - loss: 0.1654 - acc: 0.9400\n",
      "Epoch 248/700\n",
      " - 0s - loss: 0.1619 - acc: 0.9500\n",
      "Epoch 249/700\n",
      " - 0s - loss: 0.1596 - acc: 0.9500\n",
      "Epoch 250/700\n",
      " - 0s - loss: 0.1599 - acc: 0.9500\n",
      "Epoch 251/700\n",
      " - 0s - loss: 0.1585 - acc: 0.9500\n",
      "Epoch 252/700\n",
      " - 0s - loss: 0.1574 - acc: 0.9500\n",
      "Epoch 253/700\n",
      " - 0s - loss: 0.1571 - acc: 0.9500\n",
      "Epoch 254/700\n",
      " - 0s - loss: 0.1580 - acc: 0.9400\n",
      "Epoch 255/700\n",
      " - 0s - loss: 0.1570 - acc: 0.9500\n",
      "Epoch 256/700\n",
      " - 0s - loss: 0.1552 - acc: 0.9500\n",
      "Epoch 257/700\n",
      " - 0s - loss: 0.1544 - acc: 0.9500\n",
      "Epoch 258/700\n",
      " - 0s - loss: 0.1531 - acc: 0.9500\n",
      "Epoch 259/700\n",
      " - 0s - loss: 0.1533 - acc: 0.9500\n",
      "Epoch 260/700\n",
      " - 0s - loss: 0.1570 - acc: 0.9500\n",
      "Epoch 261/700\n",
      " - 0s - loss: 0.1625 - acc: 0.9400\n",
      "Epoch 262/700\n",
      " - 0s - loss: 0.1619 - acc: 0.9300\n",
      "Epoch 263/700\n",
      " - 0s - loss: 0.1542 - acc: 0.9500\n",
      "Epoch 264/700\n",
      " - 0s - loss: 0.1503 - acc: 0.9500\n",
      "Epoch 265/700\n",
      " - 0s - loss: 0.1484 - acc: 0.9500\n",
      "Epoch 266/700\n",
      " - 0s - loss: 0.1522 - acc: 0.9400\n",
      "Epoch 267/700\n",
      " - 0s - loss: 0.1573 - acc: 0.9500\n",
      "Epoch 268/700\n",
      " - 0s - loss: 0.1587 - acc: 0.9500\n",
      "Epoch 269/700\n",
      " - 0s - loss: 0.1528 - acc: 0.9400\n",
      "Epoch 270/700\n",
      " - 0s - loss: 0.1502 - acc: 0.9500\n",
      "Epoch 271/700\n",
      " - 0s - loss: 0.1458 - acc: 0.9600\n",
      "Epoch 272/700\n",
      " - 0s - loss: 0.1458 - acc: 0.9500\n",
      "Epoch 273/700\n",
      " - 0s - loss: 0.1434 - acc: 0.9500\n",
      "Epoch 274/700\n",
      " - 0s - loss: 0.1467 - acc: 0.9500\n",
      "Epoch 275/700\n",
      " - 0s - loss: 0.1543 - acc: 0.9400\n",
      "Epoch 276/700\n",
      " - 0s - loss: 0.1606 - acc: 0.9300\n",
      "Epoch 277/700\n",
      " - 0s - loss: 0.1518 - acc: 0.9400\n",
      "Epoch 278/700\n",
      " - 0s - loss: 0.1413 - acc: 0.9500\n",
      "Epoch 279/700\n",
      " - 0s - loss: 0.1394 - acc: 0.9500\n",
      "Epoch 280/700\n",
      " - 0s - loss: 0.1434 - acc: 0.9400\n",
      "Epoch 281/700\n",
      " - 0s - loss: 0.1454 - acc: 0.9500\n",
      "Epoch 282/700\n",
      " - 0s - loss: 0.1492 - acc: 0.9500\n",
      "Epoch 283/700\n",
      " - 0s - loss: 0.1485 - acc: 0.9400\n",
      "Epoch 284/700\n",
      " - 0s - loss: 0.1432 - acc: 0.9400\n",
      "Epoch 285/700\n",
      " - 0s - loss: 0.1379 - acc: 0.9500\n",
      "Epoch 286/700\n",
      " - 0s - loss: 0.1371 - acc: 0.9500\n",
      "Epoch 287/700\n",
      " - 0s - loss: 0.1368 - acc: 0.9600\n",
      "Epoch 288/700\n",
      " - 0s - loss: 0.1391 - acc: 0.9600\n",
      "Epoch 289/700\n",
      " - 0s - loss: 0.1368 - acc: 0.9600\n",
      "Epoch 290/700\n",
      " - 0s - loss: 0.1348 - acc: 0.9500\n",
      "Epoch 291/700\n",
      " - 0s - loss: 0.1343 - acc: 0.9500\n",
      "Epoch 292/700\n",
      " - 0s - loss: 0.1342 - acc: 0.9500\n",
      "Epoch 293/700\n",
      " - 0s - loss: 0.1330 - acc: 0.9500\n",
      "Epoch 294/700\n",
      " - 0s - loss: 0.1336 - acc: 0.9500\n",
      "Epoch 295/700\n",
      " - 0s - loss: 0.1325 - acc: 0.9500\n",
      "Epoch 296/700\n",
      " - 0s - loss: 0.1321 - acc: 0.9500\n",
      "Epoch 297/700\n",
      " - 0s - loss: 0.1323 - acc: 0.9500\n",
      "Epoch 298/700\n",
      " - 0s - loss: 0.1323 - acc: 0.9500\n",
      "Epoch 299/700\n",
      " - 0s - loss: 0.1314 - acc: 0.9500\n",
      "Epoch 300/700\n",
      " - 0s - loss: 0.1317 - acc: 0.9500\n",
      "Epoch 301/700\n",
      " - 0s - loss: 0.1305 - acc: 0.9500\n",
      "Epoch 302/700\n",
      " - 0s - loss: 0.1337 - acc: 0.9600\n",
      "Epoch 303/700\n",
      " - 0s - loss: 0.1343 - acc: 0.9600\n",
      "Epoch 304/700\n",
      " - 0s - loss: 0.1320 - acc: 0.9600\n",
      "Epoch 305/700\n",
      " - 0s - loss: 0.1307 - acc: 0.9600\n",
      "Epoch 306/700\n",
      " - 0s - loss: 0.1289 - acc: 0.9600\n",
      "Epoch 307/700\n",
      " - 0s - loss: 0.1276 - acc: 0.9500\n",
      "Epoch 308/700\n",
      " - 0s - loss: 0.1267 - acc: 0.9500\n",
      "Epoch 309/700\n",
      " - 0s - loss: 0.1266 - acc: 0.9500\n",
      "Epoch 310/700\n",
      " - 0s - loss: 0.1271 - acc: 0.9500\n",
      "Epoch 311/700\n",
      " - 0s - loss: 0.1264 - acc: 0.9500\n",
      "Epoch 312/700\n",
      " - 0s - loss: 0.1251 - acc: 0.9500\n",
      "Epoch 313/700\n",
      " - 0s - loss: 0.1249 - acc: 0.9500\n",
      "Epoch 314/700\n",
      " - 0s - loss: 0.1249 - acc: 0.9500\n",
      "Epoch 315/700\n",
      " - 0s - loss: 0.1247 - acc: 0.9500\n",
      "Epoch 316/700\n",
      " - 0s - loss: 0.1239 - acc: 0.9500\n",
      "Epoch 317/700\n",
      " - 0s - loss: 0.1227 - acc: 0.9500\n",
      "Epoch 318/700\n",
      " - 0s - loss: 0.1234 - acc: 0.9500\n",
      "Epoch 319/700\n",
      " - 0s - loss: 0.1226 - acc: 0.9500\n",
      "Epoch 320/700\n",
      " - 0s - loss: 0.1225 - acc: 0.9500\n",
      "Epoch 321/700\n",
      " - 0s - loss: 0.1228 - acc: 0.9500\n",
      "Epoch 322/700\n",
      " - 0s - loss: 0.1231 - acc: 0.9600\n",
      "Epoch 323/700\n",
      " - 0s - loss: 0.1227 - acc: 0.9600\n",
      "Epoch 324/700\n",
      " - 0s - loss: 0.1217 - acc: 0.9600\n",
      "Epoch 325/700\n",
      " - 0s - loss: 0.1191 - acc: 0.9500\n",
      "Epoch 326/700\n",
      " - 0s - loss: 0.1191 - acc: 0.9500\n",
      "Epoch 327/700\n",
      " - 0s - loss: 0.1202 - acc: 0.9500\n",
      "Epoch 328/700\n",
      " - 0s - loss: 0.1192 - acc: 0.9500\n",
      "Epoch 329/700\n",
      " - 0s - loss: 0.1180 - acc: 0.9500\n",
      "Epoch 330/700\n",
      " - 0s - loss: 0.1182 - acc: 0.9500\n",
      "Epoch 331/700\n",
      " - 0s - loss: 0.1179 - acc: 0.9500\n",
      "Epoch 332/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1178 - acc: 0.9500\n",
      "Epoch 333/700\n",
      " - 0s - loss: 0.1172 - acc: 0.9500\n",
      "Epoch 334/700\n",
      " - 0s - loss: 0.1169 - acc: 0.9500\n",
      "Epoch 335/700\n",
      " - 0s - loss: 0.1237 - acc: 0.9500\n",
      "Epoch 336/700\n",
      " - 0s - loss: 0.1299 - acc: 0.9600\n",
      "Epoch 337/700\n",
      " - 0s - loss: 0.1265 - acc: 0.9600\n",
      "Epoch 338/700\n",
      " - 0s - loss: 0.1219 - acc: 0.9400\n",
      "Epoch 339/700\n",
      " - 0s - loss: 0.1155 - acc: 0.9500\n",
      "Epoch 340/700\n",
      " - 0s - loss: 0.1147 - acc: 0.9500\n",
      "Epoch 341/700\n",
      " - 0s - loss: 0.1144 - acc: 0.9500\n",
      "Epoch 342/700\n",
      " - 0s - loss: 0.1142 - acc: 0.9500\n",
      "Epoch 343/700\n",
      " - 0s - loss: 0.1140 - acc: 0.9500\n",
      "Epoch 344/700\n",
      " - 0s - loss: 0.1147 - acc: 0.9500\n",
      "Epoch 345/700\n",
      " - 0s - loss: 0.1135 - acc: 0.9500\n",
      "Epoch 346/700\n",
      " - 0s - loss: 0.1129 - acc: 0.9500\n",
      "Epoch 347/700\n",
      " - 0s - loss: 0.1129 - acc: 0.9500\n",
      "Epoch 348/700\n",
      " - 0s - loss: 0.1124 - acc: 0.9500\n",
      "Epoch 349/700\n",
      " - 0s - loss: 0.1140 - acc: 0.9500\n",
      "Epoch 350/700\n",
      " - 0s - loss: 0.1124 - acc: 0.9500\n",
      "Epoch 351/700\n",
      " - 0s - loss: 0.1116 - acc: 0.9500\n",
      "Epoch 352/700\n",
      " - 0s - loss: 0.1145 - acc: 0.9600\n",
      "Epoch 353/700\n",
      " - 0s - loss: 0.1178 - acc: 0.9600\n",
      "Epoch 354/700\n",
      " - 0s - loss: 0.1176 - acc: 0.9600\n",
      "Epoch 355/700\n",
      " - 0s - loss: 0.1139 - acc: 0.9600\n",
      "Epoch 356/700\n",
      " - 0s - loss: 0.1094 - acc: 0.9500\n",
      "Epoch 357/700\n",
      " - 0s - loss: 0.1101 - acc: 0.9500\n",
      "Epoch 358/700\n",
      " - 0s - loss: 0.1094 - acc: 0.9500\n",
      "Epoch 359/700\n",
      " - 0s - loss: 0.1094 - acc: 0.9500\n",
      "Epoch 360/700\n",
      " - 0s - loss: 0.1087 - acc: 0.9500\n",
      "Epoch 361/700\n",
      " - 0s - loss: 0.1086 - acc: 0.9500\n",
      "Epoch 362/700\n",
      " - 0s - loss: 0.1083 - acc: 0.9500\n",
      "Epoch 363/700\n",
      " - 0s - loss: 0.1087 - acc: 0.9500\n",
      "Epoch 364/700\n",
      " - 0s - loss: 0.1082 - acc: 0.9500\n",
      "Epoch 365/700\n",
      " - 0s - loss: 0.1077 - acc: 0.9500\n",
      "Epoch 366/700\n",
      " - 0s - loss: 0.1078 - acc: 0.9500\n",
      "Epoch 367/700\n",
      " - 0s - loss: 0.1079 - acc: 0.9500\n",
      "Epoch 368/700\n",
      " - 0s - loss: 0.1067 - acc: 0.9500\n",
      "Epoch 369/700\n",
      " - 0s - loss: 0.1067 - acc: 0.9500\n",
      "Epoch 370/700\n",
      " - 0s - loss: 0.1081 - acc: 0.9500\n",
      "Epoch 371/700\n",
      " - 0s - loss: 0.1084 - acc: 0.9600\n",
      "Epoch 372/700\n",
      " - 0s - loss: 0.1079 - acc: 0.9600\n",
      "Epoch 373/700\n",
      " - 0s - loss: 0.1067 - acc: 0.9600\n",
      "Epoch 374/700\n",
      " - 0s - loss: 0.1056 - acc: 0.9600\n",
      "Epoch 375/700\n",
      " - 0s - loss: 0.1051 - acc: 0.9500\n",
      "Epoch 376/700\n",
      " - 0s - loss: 0.1064 - acc: 0.9500\n",
      "Epoch 377/700\n",
      " - 0s - loss: 0.1066 - acc: 0.9500\n",
      "Epoch 378/700\n",
      " - 0s - loss: 0.1068 - acc: 0.9500\n",
      "Epoch 379/700\n",
      " - 0s - loss: 0.1052 - acc: 0.9500\n",
      "Epoch 380/700\n",
      " - 0s - loss: 0.1032 - acc: 0.9500\n",
      "Epoch 381/700\n",
      " - 0s - loss: 0.1038 - acc: 0.9600\n",
      "Epoch 382/700\n",
      " - 0s - loss: 0.1075 - acc: 0.9600\n",
      "Epoch 383/700\n",
      " - 0s - loss: 0.1110 - acc: 0.9600\n",
      "Epoch 384/700\n",
      " - 0s - loss: 0.1118 - acc: 0.9600\n",
      "Epoch 385/700\n",
      " - 0s - loss: 0.1073 - acc: 0.9600\n",
      "Epoch 386/700\n",
      " - 0s - loss: 0.1003 - acc: 0.9500\n",
      "Epoch 387/700\n",
      " - 0s - loss: 0.1058 - acc: 0.9500\n",
      "Epoch 388/700\n",
      " - 0s - loss: 0.1080 - acc: 0.9500\n",
      "Epoch 389/700\n",
      " - 0s - loss: 0.1067 - acc: 0.9500\n",
      "Epoch 390/700\n",
      " - 0s - loss: 0.1024 - acc: 0.9500\n",
      "Epoch 391/700\n",
      " - 0s - loss: 0.1009 - acc: 0.9500\n",
      "Epoch 392/700\n",
      " - 0s - loss: 0.1043 - acc: 0.9600\n",
      "Epoch 393/700\n",
      " - 0s - loss: 0.1056 - acc: 0.9600\n",
      "Epoch 394/700\n",
      " - 0s - loss: 0.1045 - acc: 0.9600\n",
      "Epoch 395/700\n",
      " - 0s - loss: 0.1031 - acc: 0.9500\n",
      "Epoch 396/700\n",
      " - 0s - loss: 0.0996 - acc: 0.9500\n",
      "Epoch 397/700\n",
      " - 0s - loss: 0.1021 - acc: 0.9600\n",
      "Epoch 398/700\n",
      " - 0s - loss: 0.1100 - acc: 0.9600\n",
      "Epoch 399/700\n",
      " - 0s - loss: 0.1075 - acc: 0.9500\n",
      "Epoch 400/700\n",
      " - 0s - loss: 0.1019 - acc: 0.9500\n",
      "Epoch 401/700\n",
      " - 0s - loss: 0.0974 - acc: 0.9500\n",
      "Epoch 402/700\n",
      " - 0s - loss: 0.0992 - acc: 0.9500\n",
      "Epoch 403/700\n",
      " - 0s - loss: 0.1003 - acc: 0.9600\n",
      "Epoch 404/700\n",
      " - 0s - loss: 0.1010 - acc: 0.9600\n",
      "Epoch 405/700\n",
      " - 0s - loss: 0.0974 - acc: 0.9600\n",
      "Epoch 406/700\n",
      " - 0s - loss: 0.0981 - acc: 0.9500\n",
      "Epoch 407/700\n",
      " - 0s - loss: 0.0986 - acc: 0.9500\n",
      "Epoch 408/700\n",
      " - 0s - loss: 0.0979 - acc: 0.9500\n",
      "Epoch 409/700\n",
      " - 0s - loss: 0.0970 - acc: 0.9500\n",
      "Epoch 410/700\n",
      " - 0s - loss: 0.0977 - acc: 0.9500\n",
      "Epoch 411/700\n",
      " - 0s - loss: 0.0967 - acc: 0.9500\n",
      "Epoch 412/700\n",
      " - 0s - loss: 0.0967 - acc: 0.9500\n",
      "Epoch 413/700\n",
      " - 0s - loss: 0.0971 - acc: 0.9500\n",
      "Epoch 414/700\n",
      " - 0s - loss: 0.0971 - acc: 0.9500\n",
      "Epoch 415/700\n",
      " - 0s - loss: 0.0964 - acc: 0.9500\n",
      "Epoch 416/700\n",
      " - 0s - loss: 0.0965 - acc: 0.9500\n",
      "Epoch 417/700\n",
      " - 0s - loss: 0.0963 - acc: 0.9500\n",
      "Epoch 418/700\n",
      " - 0s - loss: 0.0979 - acc: 0.9500\n",
      "Epoch 419/700\n",
      " - 0s - loss: 0.0979 - acc: 0.9500\n",
      "Epoch 420/700\n",
      " - 0s - loss: 0.0993 - acc: 0.9600\n",
      "Epoch 421/700\n",
      " - 0s - loss: 0.0994 - acc: 0.9600\n",
      "Epoch 422/700\n",
      " - 0s - loss: 0.0966 - acc: 0.9500\n",
      "Epoch 423/700\n",
      " - 0s - loss: 0.0937 - acc: 0.9500\n",
      "Epoch 424/700\n",
      " - 0s - loss: 0.0965 - acc: 0.9500\n",
      "Epoch 425/700\n",
      " - 0s - loss: 0.0955 - acc: 0.9600\n",
      "Epoch 426/700\n",
      " - 0s - loss: 0.0952 - acc: 0.9500\n",
      "Epoch 427/700\n",
      " - 0s - loss: 0.0964 - acc: 0.9500\n",
      "Epoch 428/700\n",
      " - 0s - loss: 0.0972 - acc: 0.9600\n",
      "Epoch 429/700\n",
      " - 0s - loss: 0.0979 - acc: 0.9600\n",
      "Epoch 430/700\n",
      " - 0s - loss: 0.0978 - acc: 0.9500\n",
      "Epoch 431/700\n",
      " - 0s - loss: 0.0942 - acc: 0.9500\n",
      "Epoch 432/700\n",
      " - 0s - loss: 0.0940 - acc: 0.9500\n",
      "Epoch 433/700\n",
      " - 0s - loss: 0.0931 - acc: 0.9500\n",
      "Epoch 434/700\n",
      " - 0s - loss: 0.0938 - acc: 0.9500\n",
      "Epoch 435/700\n",
      " - 0s - loss: 0.0923 - acc: 0.9500\n",
      "Epoch 436/700\n",
      " - 0s - loss: 0.0921 - acc: 0.9500\n",
      "Epoch 437/700\n",
      " - 0s - loss: 0.0919 - acc: 0.9500\n",
      "Epoch 438/700\n",
      " - 0s - loss: 0.0918 - acc: 0.9500\n",
      "Epoch 439/700\n",
      " - 0s - loss: 0.0914 - acc: 0.9500\n",
      "Epoch 440/700\n",
      " - 0s - loss: 0.0913 - acc: 0.9500\n",
      "Epoch 441/700\n",
      " - 0s - loss: 0.0909 - acc: 0.9500\n",
      "Epoch 442/700\n",
      " - 0s - loss: 0.0928 - acc: 0.9500\n",
      "Epoch 443/700\n",
      " - 0s - loss: 0.0936 - acc: 0.9500\n",
      "Epoch 444/700\n",
      " - 0s - loss: 0.0927 - acc: 0.9500\n",
      "Epoch 445/700\n",
      " - 0s - loss: 0.0914 - acc: 0.9500\n",
      "Epoch 446/700\n",
      " - 0s - loss: 0.0905 - acc: 0.9500\n",
      "Epoch 447/700\n",
      " - 0s - loss: 0.0902 - acc: 0.9500\n",
      "Epoch 448/700\n",
      " - 0s - loss: 0.0899 - acc: 0.9500\n",
      "Epoch 449/700\n",
      " - 0s - loss: 0.0899 - acc: 0.9500\n",
      "Epoch 450/700\n",
      " - 0s - loss: 0.0903 - acc: 0.9500\n",
      "Epoch 451/700\n",
      " - 0s - loss: 0.0895 - acc: 0.9500\n",
      "Epoch 452/700\n",
      " - 0s - loss: 0.0892 - acc: 0.9500\n",
      "Epoch 453/700\n",
      " - 0s - loss: 0.0878 - acc: 0.9500\n",
      "Epoch 454/700\n",
      " - 0s - loss: 0.0921 - acc: 0.9600\n",
      "Epoch 455/700\n",
      " - 0s - loss: 0.0956 - acc: 0.9700\n",
      "Epoch 456/700\n",
      " - 0s - loss: 0.0953 - acc: 0.9700\n",
      "Epoch 457/700\n",
      " - 0s - loss: 0.0919 - acc: 0.9600\n",
      "Epoch 458/700\n",
      " - 0s - loss: 0.0920 - acc: 0.9500\n",
      "Epoch 459/700\n",
      " - 0s - loss: 0.0885 - acc: 0.9500\n",
      "Epoch 460/700\n",
      " - 0s - loss: 0.0888 - acc: 0.9600\n",
      "Epoch 461/700\n",
      " - 0s - loss: 0.0887 - acc: 0.9600\n",
      "Epoch 462/700\n",
      " - 0s - loss: 0.0890 - acc: 0.9600\n",
      "Epoch 463/700\n",
      " - 0s - loss: 0.0886 - acc: 0.9600\n",
      "Epoch 464/700\n",
      " - 0s - loss: 0.0893 - acc: 0.9500\n",
      "Epoch 465/700\n",
      " - 0s - loss: 0.0868 - acc: 0.9500\n",
      "Epoch 466/700\n",
      " - 0s - loss: 0.0869 - acc: 0.9500\n",
      "Epoch 467/700\n",
      " - 0s - loss: 0.0871 - acc: 0.9500\n",
      "Epoch 468/700\n",
      " - 0s - loss: 0.0906 - acc: 0.9600\n",
      "Epoch 469/700\n",
      " - 0s - loss: 0.0899 - acc: 0.9600\n",
      "Epoch 470/700\n",
      " - 0s - loss: 0.0876 - acc: 0.9500\n",
      "Epoch 471/700\n",
      " - 0s - loss: 0.0853 - acc: 0.9500\n",
      "Epoch 472/700\n",
      " - 0s - loss: 0.0859 - acc: 0.9600\n",
      "Epoch 473/700\n",
      " - 0s - loss: 0.0867 - acc: 0.9600\n",
      "Epoch 474/700\n",
      " - 0s - loss: 0.0867 - acc: 0.9500\n",
      "Epoch 475/700\n",
      " - 0s - loss: 0.0855 - acc: 0.9500\n",
      "Epoch 476/700\n",
      " - 0s - loss: 0.0850 - acc: 0.9500\n",
      "Epoch 477/700\n",
      " - 0s - loss: 0.0849 - acc: 0.9500\n",
      "Epoch 478/700\n",
      " - 0s - loss: 0.0847 - acc: 0.9500\n",
      "Epoch 479/700\n",
      " - 0s - loss: 0.0847 - acc: 0.9500\n",
      "Epoch 480/700\n",
      " - 0s - loss: 0.0845 - acc: 0.9500\n",
      "Epoch 481/700\n",
      " - 0s - loss: 0.0837 - acc: 0.9500\n",
      "Epoch 482/700\n",
      " - 0s - loss: 0.0851 - acc: 0.9500\n",
      "Epoch 483/700\n",
      " - 0s - loss: 0.0853 - acc: 0.9500\n",
      "Epoch 484/700\n",
      " - 0s - loss: 0.0843 - acc: 0.9500\n",
      "Epoch 485/700\n",
      " - 0s - loss: 0.0850 - acc: 0.9500\n",
      "Epoch 486/700\n",
      " - 0s - loss: 0.0830 - acc: 0.9500\n",
      "Epoch 487/700\n",
      " - 0s - loss: 0.0858 - acc: 0.9600\n",
      "Epoch 488/700\n",
      " - 0s - loss: 0.0862 - acc: 0.9600\n",
      "Epoch 489/700\n",
      " - 0s - loss: 0.0850 - acc: 0.9600\n",
      "Epoch 490/700\n",
      " - 0s - loss: 0.0839 - acc: 0.9500\n",
      "Epoch 491/700\n",
      " - 0s - loss: 0.0828 - acc: 0.9500\n",
      "Epoch 492/700\n",
      " - 0s - loss: 0.0855 - acc: 0.9500\n",
      "Epoch 493/700\n",
      " - 0s - loss: 0.0842 - acc: 0.9600\n",
      "Epoch 494/700\n",
      " - 0s - loss: 0.0835 - acc: 0.9600\n",
      "Epoch 495/700\n",
      " - 0s - loss: 0.0824 - acc: 0.9500\n",
      "Epoch 496/700\n",
      " - 0s - loss: 0.0845 - acc: 0.9600\n",
      "Epoch 497/700\n",
      " - 0s - loss: 0.0865 - acc: 0.9600\n",
      "Epoch 498/700\n",
      " - 0s - loss: 0.0857 - acc: 0.9600\n",
      "Epoch 499/700\n",
      " - 0s - loss: 0.0842 - acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/700\n",
      " - 0s - loss: 0.0828 - acc: 0.9600\n",
      "Epoch 501/700\n",
      " - 0s - loss: 0.0834 - acc: 0.9500\n",
      "Epoch 502/700\n",
      " - 0s - loss: 0.0817 - acc: 0.9500\n",
      "Epoch 503/700\n",
      " - 0s - loss: 0.0826 - acc: 0.9500\n",
      "Epoch 504/700\n",
      " - 0s - loss: 0.0857 - acc: 0.9600\n",
      "Epoch 505/700\n",
      " - 0s - loss: 0.0870 - acc: 0.9600\n",
      "Epoch 506/700\n",
      " - 0s - loss: 0.0841 - acc: 0.9600\n",
      "Epoch 507/700\n",
      " - 0s - loss: 0.0842 - acc: 0.9600\n",
      "Epoch 508/700\n",
      " - 0s - loss: 0.0833 - acc: 0.9600\n",
      "Epoch 509/700\n",
      " - 0s - loss: 0.0797 - acc: 0.9500\n",
      "Epoch 510/700\n",
      " - 0s - loss: 0.0805 - acc: 0.9600\n",
      "Epoch 511/700\n",
      " - 0s - loss: 0.0857 - acc: 0.9700\n",
      "Epoch 512/700\n",
      " - 0s - loss: 0.0840 - acc: 0.9700\n",
      "Epoch 513/700\n",
      " - 0s - loss: 0.0834 - acc: 0.9600\n",
      "Epoch 514/700\n",
      " - 0s - loss: 0.0803 - acc: 0.9500\n",
      "Epoch 515/700\n",
      " - 0s - loss: 0.0798 - acc: 0.9500\n",
      "Epoch 516/700\n",
      " - 0s - loss: 0.0799 - acc: 0.9600\n",
      "Epoch 517/700\n",
      " - 0s - loss: 0.0817 - acc: 0.9600\n",
      "Epoch 518/700\n",
      " - 0s - loss: 0.0819 - acc: 0.9600\n",
      "Epoch 519/700\n",
      " - 0s - loss: 0.0826 - acc: 0.9600\n",
      "Epoch 520/700\n",
      " - 0s - loss: 0.0823 - acc: 0.9600\n",
      "Epoch 521/700\n",
      " - 0s - loss: 0.0807 - acc: 0.9600\n",
      "Epoch 522/700\n",
      " - 0s - loss: 0.0796 - acc: 0.9600\n",
      "Epoch 523/700\n",
      " - 0s - loss: 0.0789 - acc: 0.9500\n",
      "Epoch 524/700\n",
      " - 0s - loss: 0.0784 - acc: 0.9500\n",
      "Epoch 525/700\n",
      " - 0s - loss: 0.0780 - acc: 0.9500\n",
      "Epoch 526/700\n",
      " - 0s - loss: 0.0794 - acc: 0.9600\n",
      "Epoch 527/700\n",
      " - 0s - loss: 0.0796 - acc: 0.9600\n",
      "Epoch 528/700\n",
      " - 0s - loss: 0.0781 - acc: 0.9600\n",
      "Epoch 529/700\n",
      " - 0s - loss: 0.0790 - acc: 0.9500\n",
      "Epoch 530/700\n",
      " - 0s - loss: 0.0776 - acc: 0.9500\n",
      "Epoch 531/700\n",
      " - 0s - loss: 0.0781 - acc: 0.9600\n",
      "Epoch 532/700\n",
      " - 0s - loss: 0.0793 - acc: 0.9600\n",
      "Epoch 533/700\n",
      " - 0s - loss: 0.0777 - acc: 0.9500\n",
      "Epoch 534/700\n",
      " - 0s - loss: 0.0780 - acc: 0.9500\n",
      "Epoch 535/700\n",
      " - 0s - loss: 0.0789 - acc: 0.9600\n",
      "Epoch 536/700\n",
      " - 0s - loss: 0.0821 - acc: 0.9600\n",
      "Epoch 537/700\n",
      " - 0s - loss: 0.0794 - acc: 0.9600\n",
      "Epoch 538/700\n",
      " - 0s - loss: 0.0789 - acc: 0.9500\n",
      "Epoch 539/700\n",
      " - 0s - loss: 0.0763 - acc: 0.9500\n",
      "Epoch 540/700\n",
      " - 0s - loss: 0.0770 - acc: 0.9600\n",
      "Epoch 541/700\n",
      " - 0s - loss: 0.0772 - acc: 0.9500\n",
      "Epoch 542/700\n",
      " - 0s - loss: 0.0764 - acc: 0.9600\n",
      "Epoch 543/700\n",
      " - 0s - loss: 0.0775 - acc: 0.9700\n",
      "Epoch 544/700\n",
      " - 0s - loss: 0.0775 - acc: 0.9700\n",
      "Epoch 545/700\n",
      " - 0s - loss: 0.0774 - acc: 0.9500\n",
      "Epoch 546/700\n",
      " - 0s - loss: 0.0770 - acc: 0.9700\n",
      "Epoch 547/700\n",
      " - 0s - loss: 0.0786 - acc: 0.9700\n",
      "Epoch 548/700\n",
      " - 0s - loss: 0.0769 - acc: 0.9700\n",
      "Epoch 549/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9500\n",
      "Epoch 550/700\n",
      " - 0s - loss: 0.0763 - acc: 0.9600\n",
      "Epoch 551/700\n",
      " - 0s - loss: 0.0768 - acc: 0.9600\n",
      "Epoch 552/700\n",
      " - 0s - loss: 0.0755 - acc: 0.9500\n",
      "Epoch 553/700\n",
      " - 0s - loss: 0.0763 - acc: 0.9600\n",
      "Epoch 554/700\n",
      " - 0s - loss: 0.0777 - acc: 0.9700\n",
      "Epoch 555/700\n",
      " - 0s - loss: 0.0796 - acc: 0.9700\n",
      "Epoch 556/700\n",
      " - 0s - loss: 0.0799 - acc: 0.9700\n",
      "Epoch 557/700\n",
      " - 0s - loss: 0.0789 - acc: 0.9700\n",
      "Epoch 558/700\n",
      " - 0s - loss: 0.0776 - acc: 0.9700\n",
      "Epoch 559/700\n",
      " - 0s - loss: 0.0761 - acc: 0.9700\n",
      "Epoch 560/700\n",
      " - 0s - loss: 0.0757 - acc: 0.9700\n",
      "Epoch 561/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9700\n",
      "Epoch 562/700\n",
      " - 0s - loss: 0.0752 - acc: 0.9700\n",
      "Epoch 563/700\n",
      " - 0s - loss: 0.0740 - acc: 0.9700\n",
      "Epoch 564/700\n",
      " - 0s - loss: 0.0736 - acc: 0.9500\n",
      "Epoch 565/700\n",
      " - 0s - loss: 0.0748 - acc: 0.9600\n",
      "Epoch 566/700\n",
      " - 0s - loss: 0.0769 - acc: 0.9600\n",
      "Epoch 567/700\n",
      " - 0s - loss: 0.0776 - acc: 0.9700\n",
      "Epoch 568/700\n",
      " - 0s - loss: 0.0759 - acc: 0.9600\n",
      "Epoch 569/700\n",
      " - 0s - loss: 0.0726 - acc: 0.9600\n",
      "Epoch 570/700\n",
      " - 0s - loss: 0.0757 - acc: 0.9600\n",
      "Epoch 571/700\n",
      " - 0s - loss: 0.0755 - acc: 0.9700\n",
      "Epoch 572/700\n",
      " - 0s - loss: 0.0769 - acc: 0.9700\n",
      "Epoch 573/700\n",
      " - 0s - loss: 0.0773 - acc: 0.9700\n",
      "Epoch 574/700\n",
      " - 0s - loss: 0.0752 - acc: 0.9700\n",
      "Epoch 575/700\n",
      " - 0s - loss: 0.0733 - acc: 0.9700\n",
      "Epoch 576/700\n",
      " - 0s - loss: 0.0724 - acc: 0.9600\n",
      "Epoch 577/700\n",
      " - 0s - loss: 0.0741 - acc: 0.9600\n",
      "Epoch 578/700\n",
      " - 0s - loss: 0.0773 - acc: 0.9700\n",
      "Epoch 579/700\n",
      " - 0s - loss: 0.0761 - acc: 0.9600\n",
      "Epoch 580/700\n",
      " - 0s - loss: 0.0735 - acc: 0.9600\n",
      "Epoch 581/700\n",
      " - 0s - loss: 0.0718 - acc: 0.9700\n",
      "Epoch 582/700\n",
      " - 0s - loss: 0.0719 - acc: 0.9700\n",
      "Epoch 583/700\n",
      " - 0s - loss: 0.0717 - acc: 0.9600\n",
      "Epoch 584/700\n",
      " - 0s - loss: 0.0714 - acc: 0.9600\n",
      "Epoch 585/700\n",
      " - 0s - loss: 0.0729 - acc: 0.9600\n",
      "Epoch 586/700\n",
      " - 0s - loss: 0.0747 - acc: 0.9700\n",
      "Epoch 587/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9700\n",
      "Epoch 588/700\n",
      " - 0s - loss: 0.0757 - acc: 0.9600\n",
      "Epoch 589/700\n",
      " - 0s - loss: 0.0757 - acc: 0.9600\n",
      "Epoch 590/700\n",
      " - 0s - loss: 0.0763 - acc: 0.9700\n",
      "Epoch 591/700\n",
      " - 0s - loss: 0.0771 - acc: 0.9700\n",
      "Epoch 592/700\n",
      " - 0s - loss: 0.0721 - acc: 0.9700\n",
      "Epoch 593/700\n",
      " - 0s - loss: 0.0721 - acc: 0.9600\n",
      "Epoch 594/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9700\n",
      "Epoch 595/700\n",
      " - 0s - loss: 0.0785 - acc: 0.9700\n",
      "Epoch 596/700\n",
      " - 0s - loss: 0.0767 - acc: 0.9700\n",
      "Epoch 597/700\n",
      " - 0s - loss: 0.0733 - acc: 0.9700\n",
      "Epoch 598/700\n",
      " - 0s - loss: 0.0703 - acc: 0.9700\n",
      "Epoch 599/700\n",
      " - 0s - loss: 0.0702 - acc: 0.9600\n",
      "Epoch 600/700\n",
      " - 0s - loss: 0.0705 - acc: 0.9600\n",
      "Epoch 601/700\n",
      " - 0s - loss: 0.0711 - acc: 0.9600\n",
      "Epoch 602/700\n",
      " - 0s - loss: 0.0688 - acc: 0.9600\n",
      "Epoch 603/700\n",
      " - 0s - loss: 0.0694 - acc: 0.9700\n",
      "Epoch 604/700\n",
      " - 0s - loss: 0.0766 - acc: 0.9700\n",
      "Epoch 605/700\n",
      " - 0s - loss: 0.0826 - acc: 0.9700\n",
      "Epoch 606/700\n",
      " - 0s - loss: 0.0828 - acc: 0.9700\n",
      "Epoch 607/700\n",
      " - 0s - loss: 0.0786 - acc: 0.9700\n",
      "Epoch 608/700\n",
      " - 0s - loss: 0.0724 - acc: 0.9700\n",
      "Epoch 609/700\n",
      " - 0s - loss: 0.0692 - acc: 0.9700\n",
      "Epoch 610/700\n",
      " - 0s - loss: 0.0688 - acc: 0.9600\n",
      "Epoch 611/700\n",
      " - 0s - loss: 0.0688 - acc: 0.9700\n",
      "Epoch 612/700\n",
      " - 0s - loss: 0.0701 - acc: 0.9700\n",
      "Epoch 613/700\n",
      " - 0s - loss: 0.0738 - acc: 0.9600\n",
      "Epoch 614/700\n",
      " - 0s - loss: 0.0743 - acc: 0.9700\n",
      "Epoch 615/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9700\n",
      "Epoch 616/700\n",
      " - 0s - loss: 0.0710 - acc: 0.9700\n",
      "Epoch 617/700\n",
      " - 0s - loss: 0.0690 - acc: 0.9700\n",
      "Epoch 618/700\n",
      " - 0s - loss: 0.0685 - acc: 0.9600\n",
      "Epoch 619/700\n",
      " - 0s - loss: 0.0679 - acc: 0.9700\n",
      "Epoch 620/700\n",
      " - 0s - loss: 0.0682 - acc: 0.9600\n",
      "Epoch 621/700\n",
      " - 0s - loss: 0.0685 - acc: 0.9700\n",
      "Epoch 622/700\n",
      " - 0s - loss: 0.0683 - acc: 0.9700\n",
      "Epoch 623/700\n",
      " - 0s - loss: 0.0680 - acc: 0.9700\n",
      "Epoch 624/700\n",
      " - 0s - loss: 0.0678 - acc: 0.9700\n",
      "Epoch 625/700\n",
      " - 0s - loss: 0.0676 - acc: 0.9600\n",
      "Epoch 626/700\n",
      " - 0s - loss: 0.0676 - acc: 0.9600\n",
      "Epoch 627/700\n",
      " - 0s - loss: 0.0674 - acc: 0.9700\n",
      "Epoch 628/700\n",
      " - 0s - loss: 0.0675 - acc: 0.9600\n",
      "Epoch 629/700\n",
      " - 0s - loss: 0.0663 - acc: 0.9700\n",
      "Epoch 630/700\n",
      " - 0s - loss: 0.0737 - acc: 0.9600\n",
      "Epoch 631/700\n",
      " - 0s - loss: 0.0752 - acc: 0.9700\n",
      "Epoch 632/700\n",
      " - 0s - loss: 0.0758 - acc: 0.9700\n",
      "Epoch 633/700\n",
      " - 0s - loss: 0.0734 - acc: 0.9700\n",
      "Epoch 634/700\n",
      " - 0s - loss: 0.0691 - acc: 0.9700\n",
      "Epoch 635/700\n",
      " - 0s - loss: 0.0672 - acc: 0.9600\n",
      "Epoch 636/700\n",
      " - 0s - loss: 0.0680 - acc: 0.9600\n",
      "Epoch 637/700\n",
      " - 0s - loss: 0.0667 - acc: 0.9700\n",
      "Epoch 638/700\n",
      " - 0s - loss: 0.0675 - acc: 0.9700\n",
      "Epoch 639/700\n",
      " - 0s - loss: 0.0676 - acc: 0.9700\n",
      "Epoch 640/700\n",
      " - 0s - loss: 0.0672 - acc: 0.9700\n",
      "Epoch 641/700\n",
      " - 0s - loss: 0.0666 - acc: 0.9700\n",
      "Epoch 642/700\n",
      " - 0s - loss: 0.0673 - acc: 0.9700\n",
      "Epoch 643/700\n",
      " - 0s - loss: 0.0672 - acc: 0.9700\n",
      "Epoch 644/700\n",
      " - 0s - loss: 0.0663 - acc: 0.9700\n",
      "Epoch 645/700\n",
      " - 0s - loss: 0.0659 - acc: 0.9700\n",
      "Epoch 646/700\n",
      " - 0s - loss: 0.0675 - acc: 0.9700\n",
      "Epoch 647/700\n",
      " - 0s - loss: 0.0696 - acc: 0.9700\n",
      "Epoch 648/700\n",
      " - 0s - loss: 0.0704 - acc: 0.9700\n",
      "Epoch 649/700\n",
      " - 0s - loss: 0.0687 - acc: 0.9600\n",
      "Epoch 650/700\n",
      " - 0s - loss: 0.0655 - acc: 0.9700\n",
      "Epoch 651/700\n",
      " - 0s - loss: 0.0656 - acc: 0.9700\n",
      "Epoch 652/700\n",
      " - 0s - loss: 0.0657 - acc: 0.9800\n",
      "Epoch 653/700\n",
      " - 0s - loss: 0.0654 - acc: 0.9700\n",
      "Epoch 654/700\n",
      " - 0s - loss: 0.0651 - acc: 0.9600\n",
      "Epoch 655/700\n",
      " - 0s - loss: 0.0651 - acc: 0.9700\n",
      "Epoch 656/700\n",
      " - 0s - loss: 0.0652 - acc: 0.9700\n",
      "Epoch 657/700\n",
      " - 0s - loss: 0.0657 - acc: 0.9700\n",
      "Epoch 658/700\n",
      " - 0s - loss: 0.0677 - acc: 0.9700\n",
      "Epoch 659/700\n",
      " - 0s - loss: 0.0695 - acc: 0.9700\n",
      "Epoch 660/700\n",
      " - 0s - loss: 0.0679 - acc: 0.9700\n",
      "Epoch 661/700\n",
      " - 0s - loss: 0.0625 - acc: 0.9700\n",
      "Epoch 662/700\n",
      " - 0s - loss: 0.0683 - acc: 0.9600\n",
      "Epoch 663/700\n",
      " - 0s - loss: 0.0675 - acc: 0.9700\n",
      "Epoch 664/700\n",
      " - 0s - loss: 0.0668 - acc: 0.9700\n",
      "Epoch 665/700\n",
      " - 0s - loss: 0.0641 - acc: 0.9700\n",
      "Epoch 666/700\n",
      " - 0s - loss: 0.0679 - acc: 0.9700\n",
      "Epoch 667/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0681 - acc: 0.9700\n",
      "Epoch 668/700\n",
      " - 0s - loss: 0.0658 - acc: 0.9700\n",
      "Epoch 669/700\n",
      " - 0s - loss: 0.0646 - acc: 0.9700\n",
      "Epoch 670/700\n",
      " - 0s - loss: 0.0646 - acc: 0.9700\n",
      "Epoch 671/700\n",
      " - 0s - loss: 0.0648 - acc: 0.9700\n",
      "Epoch 672/700\n",
      " - 0s - loss: 0.0646 - acc: 0.9700\n",
      "Epoch 673/700\n",
      " - 0s - loss: 0.0645 - acc: 0.9600\n",
      "Epoch 674/700\n",
      " - 0s - loss: 0.0657 - acc: 0.9700\n",
      "Epoch 675/700\n",
      " - 0s - loss: 0.0670 - acc: 0.9700\n",
      "Epoch 676/700\n",
      " - 0s - loss: 0.0679 - acc: 0.9700\n",
      "Epoch 677/700\n",
      " - 0s - loss: 0.0693 - acc: 0.9700\n",
      "Epoch 678/700\n",
      " - 0s - loss: 0.0685 - acc: 0.9700\n",
      "Epoch 679/700\n",
      " - 0s - loss: 0.0662 - acc: 0.9700\n",
      "Epoch 680/700\n",
      " - 0s - loss: 0.0647 - acc: 0.9600\n",
      "Epoch 681/700\n",
      " - 0s - loss: 0.0665 - acc: 0.9600\n",
      "Epoch 682/700\n",
      " - 0s - loss: 0.0681 - acc: 0.9700\n",
      "Epoch 683/700\n",
      " - 0s - loss: 0.0680 - acc: 0.9700\n",
      "Epoch 684/700\n",
      " - 0s - loss: 0.0659 - acc: 0.9700\n",
      "Epoch 685/700\n",
      " - 0s - loss: 0.0638 - acc: 0.9700\n",
      "Epoch 686/700\n",
      " - 0s - loss: 0.0624 - acc: 0.9800\n",
      "Epoch 687/700\n",
      " - 0s - loss: 0.0645 - acc: 0.9700\n",
      "Epoch 688/700\n",
      " - 0s - loss: 0.0644 - acc: 0.9800\n",
      "Epoch 689/700\n",
      " - 0s - loss: 0.0620 - acc: 0.9700\n",
      "Epoch 690/700\n",
      " - 0s - loss: 0.0628 - acc: 0.9700\n",
      "Epoch 691/700\n",
      " - 0s - loss: 0.0632 - acc: 0.9700\n",
      "Epoch 692/700\n",
      " - 0s - loss: 0.0626 - acc: 0.9700\n",
      "Epoch 693/700\n",
      " - 0s - loss: 0.0622 - acc: 0.9700\n",
      "Epoch 694/700\n",
      " - 0s - loss: 0.0623 - acc: 0.9700\n",
      "Epoch 695/700\n",
      " - 0s - loss: 0.0629 - acc: 0.9700\n",
      "Epoch 696/700\n",
      " - 0s - loss: 0.0621 - acc: 0.9800\n",
      "Epoch 697/700\n",
      " - 0s - loss: 0.0621 - acc: 0.9700\n",
      "Epoch 698/700\n",
      " - 0s - loss: 0.0627 - acc: 0.9700\n",
      "Epoch 699/700\n",
      " - 0s - loss: 0.0633 - acc: 0.9800\n",
      "Epoch 700/700\n",
      " - 0s - loss: 0.0648 - acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3772c750>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs is the number of times the network \"runs\" through the entire training dataset\n",
    "# verbose is the amount of info to output\n",
    "model.fit(scaled_X_train,y_train,epochs=700,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)\n",
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred0</th>\n",
       "      <th>Pred1</th>\n",
       "      <th>Pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True1</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pred0  Pred1  Pred2\n",
       "True0     19      0      0\n",
       "True1      0     15      0\n",
       "True2      0      2     14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.88      1.00      0.94        15\n",
      "           2       1.00      0.88      0.93        16\n",
      "\n",
      "    accuracy                           0.96        50\n",
      "   macro avg       0.96      0.96      0.96        50\n",
      "weighted avg       0.96      0.96      0.96        50\n",
      "\n",
      "Accuracy Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(confusion_matrix(y_test.argmax(axis=1),predictions), \n",
    "                            index=['True0','True1', 'True2'], \n",
    "                            columns=['Pred0','Pred1', 'Pred2']))\n",
    "print(classification_report(y_test.argmax(axis=1),predictions))\n",
    "print('Accuracy Score:', accuracy_score(y_test.argmax(axis=1),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the model to be re-used\n",
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "> Specifically designed to work with sequence data\n",
    "* Time series\n",
    "* Sentences (NLP)\n",
    "* Audio\n",
    "* Car Trajectories\n",
    "\n",
    "<img src='feed_forward_neuron.png'>\n",
    "<img src='recur_neuron.png'>\n",
    "<img src='recur_neuron_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Cells:** Nodes that are a function of inputs from previous time steps\n",
    "\n",
    "<img src='recur_layer.png'>\n",
    "<img src='recur_layer_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM & GRU\n",
    "* Long-short term memory\n",
    "* \n",
    "\n",
    "> Help to reduce the \"forgetfullness\" of RNNs as what was input at $t_{0}$ may lose its influence at $t_{i+j}$\n",
    "\n",
    "<img src='lstm_cell.png'>\n",
    "\n",
    "**Input:**\n",
    "1. $C_{t-1}$: Cell state at $t-1$\n",
    "2. $h_{t-1}$: outcome of previous cell\n",
    "3. $x_{t}$: factor of relevance\n",
    "\n",
    "### First Step: Forget Gate Layer\n",
    "* We decide what information we are going to \"forget\", or \"throw away\", from the cell state\n",
    "<img src='forget_gate_layer.png'>\n",
    "\n",
    "### Second Step:\n",
    "* We decide what to store\n",
    "<img src='lstm_stage_2.png'>\n",
    "\n",
    "### Third Step:\n",
    "* We update the cell state (to send to the next cell state)\n",
    "<img src='lstm_stage_3.png'>\n",
    "\n",
    "### Fourth Step:\n",
    "* We output $h_{t}$\n",
    "<img src='lstm_stage_4.png'>\n",
    "\n",
    "**Different kind of LSTMs:**\n",
    "1. Peephole\n",
    "2. Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Peephole**\n",
    "<img src='lstm_peep.png'>\n",
    "**GRU**\n",
    "<img src='lstm_GRU.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTMs with Keras and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "        \n",
    "        return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the load by specifiying what to disable\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    import string\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "#     return [token.text.lower() for token in nlp(doc_text) if token.text not in [string.punctuation, '\\n\\n \\n\\n\\n!\"']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11394"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 words --> network predict #26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on\n",
      "\n",
      "me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore\n",
      "\n",
      "ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i\n",
      "\n",
      "some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought\n",
      "\n",
      "years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each item in the list is composed of one word to the right of the previous, and less the first\n",
    "# - shift the text one word right\n",
    "for i in range(5):\n",
    "    print(' '.join(text_sequences[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[964,\n",
       " 14,\n",
       " 265,\n",
       " 51,\n",
       " 263,\n",
       " 416,\n",
       " 87,\n",
       " 222,\n",
       " 129,\n",
       " 111,\n",
       " 962,\n",
       " 262,\n",
       " 50,\n",
       " 43,\n",
       " 37,\n",
       " 321,\n",
       " 7,\n",
       " 23,\n",
       " 555,\n",
       " 3,\n",
       " 150,\n",
       " 261,\n",
       " 6,\n",
       " 2704,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creature'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964: call\n",
      "14: me\n",
      "265: ishmael\n",
      "51: some\n",
      "263: years\n",
      "416: ago\n",
      "87: never\n",
      "222: mind\n",
      "129: how\n",
      "111: long\n",
      "962: precisely\n",
      "262: having\n",
      "50: little\n",
      "43: or\n",
      "37: no\n",
      "321: money\n",
      "7: in\n",
      "23: my\n",
      "555: purse\n",
      "3: and\n",
      "150: nothing\n",
      "261: particular\n",
      "6: to\n",
      "2704: interest\n",
      "14: me\n",
      "24: on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i}: {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('call', 27),\n",
       "             ('me', 2471),\n",
       "             ('ishmael', 133),\n",
       "             ('some', 758),\n",
       "             ('years', 135),\n",
       "             ('ago', 84),\n",
       "             ('never', 449),\n",
       "             ('mind', 164),\n",
       "             ('how', 321),\n",
       "             ('long', 374),\n",
       "             ('precisely', 37),\n",
       "             ('having', 142),\n",
       "             ('little', 767),\n",
       "             ('or', 950),\n",
       "             ('no', 1029),\n",
       "             ('money', 120),\n",
       "             ('in', 5647),\n",
       "             ('my', 1812),\n",
       "             ('purse', 71),\n",
       "             ('and', 9646),\n",
       "             ('nothing', 281),\n",
       "             ('particular', 152),\n",
       "             ('to', 6497),\n",
       "             ('interest', 24),\n",
       "             ('on', 1716),\n",
       "             ('shore', 26),\n",
       "             ('i', 7176),\n",
       "             ('thought', 676),\n",
       "             ('would', 702),\n",
       "             ('sail', 104),\n",
       "             ('about', 1014),\n",
       "             ('a', 10377),\n",
       "             ('see', 442),\n",
       "             ('the', 15566),\n",
       "             ('watery', 26),\n",
       "             ('part', 234),\n",
       "             ('of', 8287),\n",
       "             ('world', 234),\n",
       "             ('it', 4394),\n",
       "             ('is', 1950),\n",
       "             ('way', 390),\n",
       "             ('have', 806),\n",
       "             ('driving', 26),\n",
       "             ('off', 416),\n",
       "             ('spleen', 26),\n",
       "             ('regulating', 26),\n",
       "             ('circulation', 26),\n",
       "             ('whenever', 130),\n",
       "             ('find', 78),\n",
       "             ('myself', 416),\n",
       "             ('growing', 26),\n",
       "             ('grim', 26),\n",
       "             ('mouth', 130),\n",
       "             ('damp', 78),\n",
       "             ('drizzly', 26),\n",
       "             ('november', 26),\n",
       "             ('soul', 78),\n",
       "             ('involuntarily', 52),\n",
       "             ('pausing', 52),\n",
       "             ('before', 364),\n",
       "             ('coffin', 130),\n",
       "             ('warehouses', 52),\n",
       "             ('bringing', 26),\n",
       "             ('up', 1237),\n",
       "             ('rear', 26),\n",
       "             ('every', 182),\n",
       "             ('funeral', 26),\n",
       "             ('meet', 26),\n",
       "             ('especially', 104),\n",
       "             ('hypos', 26),\n",
       "             ('get', 364),\n",
       "             ('such', 572),\n",
       "             ('an', 806),\n",
       "             ('upper', 26),\n",
       "             ('hand', 312),\n",
       "             ('that', 3770),\n",
       "             ('requires', 52),\n",
       "             ('strong', 78),\n",
       "             ('moral', 26),\n",
       "             ('principle', 26),\n",
       "             ('prevent', 26),\n",
       "             ('from', 1508),\n",
       "             ('deliberately', 26),\n",
       "             ('stepping', 26),\n",
       "             ('into', 988),\n",
       "             ('street', 104),\n",
       "             ('methodically', 26),\n",
       "             ('knocking', 26),\n",
       "             ('people', 52),\n",
       "             (\"'s\", 1691),\n",
       "             ('hats', 26),\n",
       "             ('then', 832),\n",
       "             ('account', 78),\n",
       "             ('high', 130),\n",
       "             ('time', 520),\n",
       "             ('sea', 546),\n",
       "             ('as', 2366),\n",
       "             ('soon', 234),\n",
       "             ('can', 338),\n",
       "             ('this', 2184),\n",
       "             ('substitute', 26),\n",
       "             ('for', 1820),\n",
       "             ('pistol', 26),\n",
       "             ('ball', 26),\n",
       "             ('with', 2392),\n",
       "             ('philosophical', 26),\n",
       "             ('flourish', 26),\n",
       "             ('cato', 26),\n",
       "             ('throws', 26),\n",
       "             ('himself', 338),\n",
       "             ('upon', 780),\n",
       "             ('his', 3139),\n",
       "             ('sword', 78),\n",
       "             ('quietly', 78),\n",
       "             ('take', 260),\n",
       "             ('ship', 182),\n",
       "             ('there', 1456),\n",
       "             ('surprising', 26),\n",
       "             ('if', 728),\n",
       "             ('they', 728),\n",
       "             ('but', 2704),\n",
       "             ('knew', 130),\n",
       "             ('almost', 286),\n",
       "             ('all', 1872),\n",
       "             ('men', 130),\n",
       "             ('their', 390),\n",
       "             ('degree', 78),\n",
       "             ('other', 494),\n",
       "             ('cherish', 26),\n",
       "             ('very', 494),\n",
       "             ('nearly', 52),\n",
       "             ('same', 312),\n",
       "             ('feelings', 26),\n",
       "             ('towards', 260),\n",
       "             ('ocean', 52),\n",
       "             ('now', 1040),\n",
       "             ('your', 442),\n",
       "             ('insular', 26),\n",
       "             ('city', 104),\n",
       "             ('manhattoes', 26),\n",
       "             ('belted', 26),\n",
       "             ('round', 364),\n",
       "             ('by', 962),\n",
       "             ('wharves', 26),\n",
       "             ('indian', 52),\n",
       "             ('isles', 26),\n",
       "             ('coral', 26),\n",
       "             ('reefs', 26),\n",
       "             ('commerce', 26),\n",
       "             ('surrounds', 26),\n",
       "             ('her', 156),\n",
       "             ('surf', 26),\n",
       "             ('right', 156),\n",
       "             ('left', 78),\n",
       "             ('streets', 208),\n",
       "             ('you', 2210),\n",
       "             ('waterward', 52),\n",
       "             ('its', 156),\n",
       "             ('extreme', 26),\n",
       "             ('downtown', 26),\n",
       "             ('battery', 52),\n",
       "             ('where', 364),\n",
       "             ('noble', 52),\n",
       "             ('mole', 26),\n",
       "             ('washed', 52),\n",
       "             ('waves', 26),\n",
       "             ('cooled', 26),\n",
       "             ('breezes', 26),\n",
       "             ('which', 572),\n",
       "             ('few', 104),\n",
       "             ('hours', 130),\n",
       "             ('previous', 104),\n",
       "             ('were', 962),\n",
       "             ('out', 956),\n",
       "             ('sight', 104),\n",
       "             ('land', 208),\n",
       "             ('look', 156),\n",
       "             ('at', 2184),\n",
       "             ('crowds', 52),\n",
       "             ('water', 260),\n",
       "             ('gazers', 26),\n",
       "             ('circumambulate', 26),\n",
       "             ('dreamy', 26),\n",
       "             ('sabbath', 52),\n",
       "             ('afternoon', 52),\n",
       "             ('go', 494),\n",
       "             ('corlears', 26),\n",
       "             ('hook', 26),\n",
       "             ('coenties', 26),\n",
       "             ('slip', 26),\n",
       "             ('thence', 52),\n",
       "             ('whitehall', 26),\n",
       "             ('northward', 26),\n",
       "             ('what', 1170),\n",
       "             ('do', 702),\n",
       "             ('?--', 182),\n",
       "             ('posted', 26),\n",
       "             ('like', 732),\n",
       "             ('silent', 52),\n",
       "             ('sentinels', 26),\n",
       "             ('around', 78),\n",
       "             ('town', 182),\n",
       "             ('stand', 182),\n",
       "             ('thousands', 52),\n",
       "             ('mortal', 26),\n",
       "             ('fixed', 78),\n",
       "             ('reveries', 52),\n",
       "             ('leaning', 52),\n",
       "             ('against', 234),\n",
       "             ('spiles', 26),\n",
       "             ('seated', 52),\n",
       "             ('pier', 26),\n",
       "             ('heads', 338),\n",
       "             ('looking', 312),\n",
       "             ('over', 702),\n",
       "             ('bulwarks', 52),\n",
       "             ('ships', 78),\n",
       "             ('china', 26),\n",
       "             ('aloft', 52),\n",
       "             ('rigging', 26),\n",
       "             ('striving', 26),\n",
       "             ('still', 364),\n",
       "             ('better', 208),\n",
       "             ('seaward', 26),\n",
       "             ('peep', 26),\n",
       "             ('these', 494),\n",
       "             ('are', 416),\n",
       "             ('landsmen', 26),\n",
       "             ('week', 52),\n",
       "             ('days', 104),\n",
       "             ('pent', 26),\n",
       "             ('lath', 26),\n",
       "             ('plaster', 52),\n",
       "             ('tied', 26),\n",
       "             ('counters', 26),\n",
       "             ('nailed', 26),\n",
       "             ('benches', 26),\n",
       "             ('clinched', 26),\n",
       "             ('desks', 26),\n",
       "             ('green', 130),\n",
       "             ('fields', 26),\n",
       "             ('gone', 52),\n",
       "             ('here', 624),\n",
       "             ('come', 338),\n",
       "             ('more', 494),\n",
       "             ('pacing', 26),\n",
       "             ('straight', 104),\n",
       "             ('seemingly', 26),\n",
       "             ('bound', 52),\n",
       "             ('dive', 26),\n",
       "             ('strange', 182),\n",
       "             ('will', 260),\n",
       "             ('content', 52),\n",
       "             ('them', 442),\n",
       "             ('extremest', 26),\n",
       "             ('limit', 26),\n",
       "             ('loitering', 26),\n",
       "             ('under', 260),\n",
       "             ('shady', 26),\n",
       "             ('lee', 26),\n",
       "             ('yonder', 52),\n",
       "             ('not', 1534),\n",
       "             ('suffice', 26),\n",
       "             ('must', 442),\n",
       "             ('just', 390),\n",
       "             ('nigh', 104),\n",
       "             ('possibly', 26),\n",
       "             ('without', 182),\n",
       "             ('falling', 52),\n",
       "             ('miles', 78),\n",
       "             ('leagues', 26),\n",
       "             ('inlanders', 26),\n",
       "             ('lanes', 26),\n",
       "             ('alleys', 26),\n",
       "             ('avenues', 26),\n",
       "             ('north', 52),\n",
       "             ('east', 26),\n",
       "             ('south', 156),\n",
       "             ('west', 26),\n",
       "             ('yet', 416),\n",
       "             ('unite', 26),\n",
       "             ('tell', 442),\n",
       "             ('does', 156),\n",
       "             ('magnetic', 26),\n",
       "             ('virtue', 26),\n",
       "             ('needles', 26),\n",
       "             ('compasses', 26),\n",
       "             ('those', 234),\n",
       "             ('attract', 26),\n",
       "             ('thither', 26),\n",
       "             ('once', 208),\n",
       "             ('say', 286),\n",
       "             ('country', 78),\n",
       "             ('lakes', 26),\n",
       "             ('any', 364),\n",
       "             ('path', 26),\n",
       "             ('please', 52),\n",
       "             ('ten', 52),\n",
       "             ('one', 1300),\n",
       "             ('carries', 26),\n",
       "             ('down', 468),\n",
       "             ('dale', 26),\n",
       "             ('leaves', 52),\n",
       "             ('pool', 26),\n",
       "             ('stream', 78),\n",
       "             ('magic', 52),\n",
       "             ('let', 156),\n",
       "             ('most', 468),\n",
       "             ('absent', 26),\n",
       "             ('minded', 26),\n",
       "             ('be', 1716),\n",
       "             ('plunged', 52),\n",
       "             ('deepest', 26),\n",
       "             ('man', 572),\n",
       "             ('legs', 104),\n",
       "             ('set', 156),\n",
       "             ('feet', 182),\n",
       "             ('going', 260),\n",
       "             ('he', 3273),\n",
       "             ('infallibly', 26),\n",
       "             ('lead', 78),\n",
       "             ('region', 26),\n",
       "             ('should', 286),\n",
       "             ('ever', 338),\n",
       "             ('athirst', 26),\n",
       "             ('great', 376),\n",
       "             ('american', 78),\n",
       "             ('desert', 26),\n",
       "             ('try', 104),\n",
       "             ('experiment', 26),\n",
       "             ('caravan', 26),\n",
       "             ('happen', 26),\n",
       "             ('supplied', 26),\n",
       "             ('metaphysical', 52),\n",
       "             ('professor', 26),\n",
       "             ('yes', 104),\n",
       "             ('knows', 26),\n",
       "             ('meditation', 26),\n",
       "             ('wedded', 26),\n",
       "             ('artist', 78),\n",
       "             ('desires', 26),\n",
       "             ('paint', 26),\n",
       "             ('dreamiest', 26),\n",
       "             ('shadiest', 26),\n",
       "             ('quietest', 26),\n",
       "             ('enchanting', 26),\n",
       "             ('bit', 130),\n",
       "             ('romantic', 26),\n",
       "             ('landscape', 26),\n",
       "             ('valley', 26),\n",
       "             ('saco', 26),\n",
       "             ('chief', 52),\n",
       "             ('element', 26),\n",
       "             ('employs', 26),\n",
       "             ('trees', 26),\n",
       "             ('each', 78),\n",
       "             ('hollow', 26),\n",
       "             ('trunk', 52),\n",
       "             ('hermit', 26),\n",
       "             ('crucifix', 26),\n",
       "             ('within', 130),\n",
       "             ('sleeps', 26),\n",
       "             ('meadow', 52),\n",
       "             ('sleep', 416),\n",
       "             ('cattle', 26),\n",
       "             ('cottage', 26),\n",
       "             ('goes', 78),\n",
       "             ('sleepy', 26),\n",
       "             ('smoke', 52),\n",
       "             ('deep', 78),\n",
       "             ('distant', 78),\n",
       "             ('woodlands', 26),\n",
       "             ('winds', 78),\n",
       "             ('mazy', 26),\n",
       "             ('reaching', 52),\n",
       "             ('overlapping', 26),\n",
       "             ('spurs', 26),\n",
       "             ('mountains', 26),\n",
       "             ('bathed', 26),\n",
       "             ('hill', 52),\n",
       "             ('side', 286),\n",
       "             ('blue', 78),\n",
       "             ('though', 546),\n",
       "             ('picture', 130),\n",
       "             ('lies', 26),\n",
       "             ('thus', 52),\n",
       "             ('tranced', 26),\n",
       "             ('pine', 52),\n",
       "             ('tree', 26),\n",
       "             ('shakes', 26),\n",
       "             ('sighs', 52),\n",
       "             ('shepherd', 52),\n",
       "             ('head', 624),\n",
       "             ('vain', 26),\n",
       "             ('unless', 104),\n",
       "             ('eye', 26),\n",
       "             ('him', 1092),\n",
       "             ('visit', 26),\n",
       "             ('prairies', 26),\n",
       "             ('june', 52),\n",
       "             ('when', 650),\n",
       "             ('scores', 52),\n",
       "             ('wade', 26),\n",
       "             ('knee', 26),\n",
       "             ('among', 78),\n",
       "             ('tiger', 26),\n",
       "             ('lilies', 26),\n",
       "             ('charm', 26),\n",
       "             ('wanting', 26),\n",
       "             ('drop', 26),\n",
       "             ('niagara', 26),\n",
       "             ('cataract', 26),\n",
       "             ('sand', 26),\n",
       "             ('travel', 26),\n",
       "             ('thousand', 52),\n",
       "             ('why', 286),\n",
       "             ('did', 572),\n",
       "             ('poor', 104),\n",
       "             ('poet', 26),\n",
       "             ('tennessee', 26),\n",
       "             ('suddenly', 78),\n",
       "             ('receiving', 26),\n",
       "             ('two', 338),\n",
       "             ('handfuls', 26),\n",
       "             ('silver', 52),\n",
       "             ('deliberate', 26),\n",
       "             ('whether', 182),\n",
       "             ('buy', 26),\n",
       "             ('coat', 104),\n",
       "             ('sadly', 52),\n",
       "             ('needed', 26),\n",
       "             ('invest', 26),\n",
       "             ('pedestrian', 26),\n",
       "             ('trip', 26),\n",
       "             ('rockaway', 26),\n",
       "             ('beach', 26),\n",
       "             ('robust', 52),\n",
       "             ('healthy', 52),\n",
       "             ('boy', 52),\n",
       "             ('crazy', 52),\n",
       "             ('first', 494),\n",
       "             ('voyage', 208),\n",
       "             ('passenger', 104),\n",
       "             ('yourself', 156),\n",
       "             ('feel', 78),\n",
       "             ('mystical', 26),\n",
       "             ('vibration', 26),\n",
       "             ('told', 130),\n",
       "             ('old', 754),\n",
       "             ('persians', 26),\n",
       "             ('hold', 52),\n",
       "             ('holy', 52),\n",
       "             ('greeks', 26),\n",
       "             ('give', 208),\n",
       "             ('separate', 26),\n",
       "             ('deity', 26),\n",
       "             ('own', 286),\n",
       "             ('brother', 52),\n",
       "             ('jove', 26),\n",
       "             ('surely', 26),\n",
       "             ('meaning', 78),\n",
       "             ('deeper', 26),\n",
       "             ('story', 130),\n",
       "             ('narcissus', 26),\n",
       "             ('who', 416),\n",
       "             ('because', 182),\n",
       "             ('could', 650),\n",
       "             ('grasp', 26),\n",
       "             ('tormenting', 26),\n",
       "             ('mild', 26),\n",
       "             ('image', 156),\n",
       "             ('saw', 156),\n",
       "             ('fountain', 26),\n",
       "             ('was', 2886),\n",
       "             ('drowned', 26),\n",
       "             ('we', 286),\n",
       "             ('ourselves', 52),\n",
       "             ('rivers', 26),\n",
       "             ('oceans', 26),\n",
       "             ('ungraspable', 26),\n",
       "             ('phantom', 78),\n",
       "             ('life', 78),\n",
       "             ('key', 26),\n",
       "             ('am', 156),\n",
       "             ('habit', 26),\n",
       "             ('begin', 52),\n",
       "             ('grow', 52),\n",
       "             ('hazy', 26),\n",
       "             ('eyes', 182),\n",
       "             ('conscious', 26),\n",
       "             ('lungs', 26),\n",
       "             ('mean', 130),\n",
       "             ('inferred', 52),\n",
       "             ('needs', 52),\n",
       "             ('rag', 26),\n",
       "             ('something', 312),\n",
       "             ('besides', 156),\n",
       "             ('passengers', 78),\n",
       "             ('sick', 26),\n",
       "             ('quarrelsome', 26),\n",
       "             (\"don't\", 52),\n",
       "             ('nights', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('themselves', 52),\n",
       "             ('much', 442),\n",
       "             ('general', 26),\n",
       "             ('thing', 130),\n",
       "             (';--', 104),\n",
       "             ('nor', 78),\n",
       "             ('salt', 26),\n",
       "             ('commodore', 52),\n",
       "             ('captain', 52),\n",
       "             ('cook', 78),\n",
       "             ('abandon', 26),\n",
       "             ('glory', 52),\n",
       "             ('distinction', 26),\n",
       "             ('offices', 26),\n",
       "             ('abominate', 26),\n",
       "             ('honourable', 26),\n",
       "             ('respectable', 26),\n",
       "             ('toils', 26),\n",
       "             ('trials', 26),\n",
       "             ('tribulations', 26),\n",
       "             ('kind', 78),\n",
       "             ('whatsoever', 52),\n",
       "             ('quite', 78),\n",
       "             ('care', 78),\n",
       "             ('taking', 104),\n",
       "             ('barques', 26),\n",
       "             ('brigs', 26),\n",
       "             ('schooners', 26),\n",
       "             (',--', 130),\n",
       "             ('confess', 52),\n",
       "             ('considerable', 26),\n",
       "             ('being', 390),\n",
       "             ('sort', 494),\n",
       "             ('officer', 52),\n",
       "             ('board', 78),\n",
       "             ('somehow', 78),\n",
       "             ('fancied', 26),\n",
       "             ('broiling', 26),\n",
       "             ('fowls', 26),\n",
       "             ('broiled', 78),\n",
       "             ('judiciously', 26),\n",
       "             ('buttered', 26),\n",
       "             ('judgmatically', 26),\n",
       "             ('salted', 26),\n",
       "             ('peppered', 26),\n",
       "             ('speak', 130),\n",
       "             ('respectfully', 52),\n",
       "             ('reverentially', 26),\n",
       "             ('fowl', 26),\n",
       "             ('than', 390),\n",
       "             ('idolatrous', 26),\n",
       "             ('dotings', 26),\n",
       "             ('egyptians', 26),\n",
       "             ('ibis', 26),\n",
       "             ('roasted', 26),\n",
       "             ('river', 26),\n",
       "             ('horse', 52),\n",
       "             ('mummies', 26),\n",
       "             ('creatures', 26),\n",
       "             ('huge', 52),\n",
       "             ('bake', 26),\n",
       "             ('houses', 52),\n",
       "             ('pyramids', 26),\n",
       "             ('simple', 26),\n",
       "             ('sailor', 156),\n",
       "             ('mast', 78),\n",
       "             ('plumb', 26),\n",
       "             ('forecastle', 52),\n",
       "             ('royal', 26),\n",
       "             ('true', 104),\n",
       "             ('rather', 260),\n",
       "             ('order', 130),\n",
       "             ('make', 260),\n",
       "             ('jump', 52),\n",
       "             ('spar', 52),\n",
       "             ('grasshopper', 26),\n",
       "             ('may', 312),\n",
       "             ('unpleasant', 26),\n",
       "             ('enough', 338),\n",
       "             ('touches', 26),\n",
       "             ('sense', 78),\n",
       "             ('honour', 26),\n",
       "             ('particularly', 52),\n",
       "             ('established', 26),\n",
       "             ('family', 26),\n",
       "             ('van', 26),\n",
       "             ('rensselaers', 26),\n",
       "             ('randolphs', 26),\n",
       "             ('hardicanutes', 26),\n",
       "             ('putting', 52),\n",
       "             ('tar', 52),\n",
       "             ('pot', 26),\n",
       "             ('been', 468),\n",
       "             ('lording', 26),\n",
       "             ('schoolmaster', 52),\n",
       "             ('making', 130),\n",
       "             ('tallest', 26),\n",
       "             ('boys', 52),\n",
       "             ('awe', 26),\n",
       "             ('transition', 52),\n",
       "             ('keen', 26),\n",
       "             ('assure', 26),\n",
       "             ('decoction', 26),\n",
       "             ('seneca', 26),\n",
       "             ('stoics', 26),\n",
       "             ('enable', 26),\n",
       "             ('grin', 52),\n",
       "             ('bear', 52),\n",
       "             ('even', 130),\n",
       "             ('wears', 26),\n",
       "             ('hunks', 52),\n",
       "             ('orders', 26),\n",
       "             ('broom', 26),\n",
       "             ('sweep', 52),\n",
       "             ('decks', 26),\n",
       "             ('indignity', 26),\n",
       "             ('amount', 26),\n",
       "             ('weighed', 52),\n",
       "             ('scales', 26),\n",
       "             ('new', 286),\n",
       "             ('testament', 26),\n",
       "             ('think', 182),\n",
       "             ('archangel', 26),\n",
       "             ('gabriel', 26),\n",
       "             ('thinks', 182),\n",
       "             ('anything', 52),\n",
       "             ('less', 52),\n",
       "             ('promptly', 26),\n",
       "             ('obey', 26),\n",
       "             ('instance', 26),\n",
       "             ('ai', 104),\n",
       "             (\"n't\", 624),\n",
       "             ('slave', 26),\n",
       "             ('well', 208),\n",
       "             ('however', 208),\n",
       "             ('captains', 26),\n",
       "             ('thump', 52),\n",
       "             ('punch', 26),\n",
       "             ('satisfaction', 26),\n",
       "             ('knowing', 78),\n",
       "             ('everybody', 26),\n",
       "             ('else', 208),\n",
       "             ('served', 26),\n",
       "             ('either', 78),\n",
       "             ('physical', 26),\n",
       "             ('point', 52),\n",
       "             ('view', 52),\n",
       "             ('so', 1092),\n",
       "             ('universal', 26),\n",
       "             ('passed', 78),\n",
       "             ('hands', 78),\n",
       "             ('rub', 26),\n",
       "             ('shoulder', 26),\n",
       "             ('blades', 26),\n",
       "             ('again', 286),\n",
       "             ('always', 104),\n",
       "             ('paying', 78),\n",
       "             ('trouble', 26),\n",
       "             ('whereas', 26),\n",
       "             ('pay', 78),\n",
       "             ('single', 52),\n",
       "             ('penny', 78),\n",
       "             ('heard', 208),\n",
       "             ('contrary', 26),\n",
       "             ('difference', 52),\n",
       "             ('between', 234),\n",
       "             ('paid', 52),\n",
       "             ('act', 52),\n",
       "             ('perhaps', 130),\n",
       "             ('uncomfortable', 52),\n",
       "             ('infliction', 26),\n",
       "             ('orchard', 26),\n",
       "             ('thieves', 26),\n",
       "             ('entailed', 26),\n",
       "             ('us', 104),\n",
       "             ('compare', 52),\n",
       "             ('urbane', 26),\n",
       "             ('activity', 26),\n",
       "             ('receives', 26),\n",
       "             ('really', 104),\n",
       "             ('marvellous', 104),\n",
       "             ('considering', 26),\n",
       "             ('earnestly', 26),\n",
       "             ('believe', 26),\n",
       "             ('root', 26),\n",
       "             ('earthly', 52),\n",
       "             ('ills', 26),\n",
       "             ('monied', 26),\n",
       "             ('enter', 52),\n",
       "             ('heaven', 104),\n",
       "             ('ah', 26),\n",
       "             ('cheerfully', 26),\n",
       "             ('consign', 26),\n",
       "             ('perdition', 26),\n",
       "             ('finally', 26),\n",
       "             ('wholesome', 26),\n",
       "             ('exercise', 26),\n",
       "             ('pure', 26),\n",
       "             ('air', 104),\n",
       "             ('fore', 26),\n",
       "             ('castle', 26),\n",
       "             ('deck', 52),\n",
       "             ('far', 104),\n",
       "             ('prevalent', 26),\n",
       "             ('astern', 26),\n",
       "             ('violate', 26),\n",
       "             ('pythagorean', 26),\n",
       "             ('maxim', 26),\n",
       "             ('quarter', 52),\n",
       "             ('gets', 26),\n",
       "             ('atmosphere', 26),\n",
       "             ('second', 104),\n",
       "             ('sailors', 78),\n",
       "             ('breathes', 26),\n",
       "             ('commonalty', 26),\n",
       "             ('leaders', 52),\n",
       "             ('many', 104),\n",
       "             ('things', 130),\n",
       "             ('suspect', 26),\n",
       "             ('wherefore', 26),\n",
       "             ('after', 234),\n",
       "             ('repeatedly', 26),\n",
       "             ('smelt', 52),\n",
       "             ('merchant', 26),\n",
       "             ('whaling', 234),\n",
       "             ('invisible', 26),\n",
       "             ('police', 26),\n",
       "             ('fates', 52),\n",
       "             ('has', 104),\n",
       "             ('constant', 26),\n",
       "             ('surveillance', 26),\n",
       "             ('secretly', 26),\n",
       "             ('dogs', 26),\n",
       "             ('influences', 26),\n",
       "             ('unaccountable', 104),\n",
       "             ('answer', 130),\n",
       "             ('doubtless', 52),\n",
       "             ('formed', 52),\n",
       "             ('grand', 104),\n",
       "             ('programme', 26),\n",
       "             ('providence', 26),\n",
       "             ('drawn', 26),\n",
       "             ('came', 286),\n",
       "             ('brief', 26),\n",
       "             ('interlude', 26),\n",
       "             ('solo', 26),\n",
       "             ('extensive', 26),\n",
       "             ('performances', 26),\n",
       "             ('bill', 26),\n",
       "             ('run', 52),\n",
       "             ('contested', 26),\n",
       "             ('election', 26),\n",
       "             ('presidency', 26),\n",
       "             ('united', 26),\n",
       "             ('states', 26),\n",
       "             ('bloody', 26),\n",
       "             ('battle', 26),\n",
       "             ('affghanistan', 26),\n",
       "             ('exactly', 78),\n",
       "             ('stage', 52),\n",
       "             ('managers', 26),\n",
       "             ('put', 208),\n",
       "             ('shabby', 52),\n",
       "             ('others', 52),\n",
       "             ('magnificent', 26),\n",
       "             ('parts', 130),\n",
       "             ('tragedies', 26),\n",
       "             ('short', 78),\n",
       "             ('easy', 78),\n",
       "             ('genteel', 26),\n",
       "             ('comedies', 26),\n",
       "             ('jolly', 104),\n",
       "             ('farces', 26),\n",
       "             ('recall', 26),\n",
       "             ('circumstances', 52),\n",
       "             ('springs', 26),\n",
       "             ('motives', 52),\n",
       "             ('cunningly', 26),\n",
       "             ('presented', 26),\n",
       "             ('various', 52),\n",
       "             ('disguises', 26),\n",
       "             ('induced', 26),\n",
       "             ('performing', 26),\n",
       "             ('cajoling', 26),\n",
       "             ('delusion', 26),\n",
       "             ('choice', 26),\n",
       "             ('resulting', 26),\n",
       "             ('unbiased', 26),\n",
       "             ('freewill', 26),\n",
       "             ('discriminating', 26),\n",
       "             ('judgment', 26),\n",
       "             ('overwhelming', 26),\n",
       "             ('idea', 182),\n",
       "             ('whale', 260),\n",
       "             ('portentous', 78),\n",
       "             ('mysterious', 52),\n",
       "             ('monster', 26),\n",
       "             ('roused', 26),\n",
       "             ('curiosity', 52),\n",
       "             ('wild', 130),\n",
       "             ('seas', 156),\n",
       "             ('rolled', 156),\n",
       "             ('island', 78),\n",
       "             ('bulk', 26),\n",
       "             ('undeliverable', 26),\n",
       "             ('nameless', 78),\n",
       "             ('perils', 26),\n",
       "             ('attending', 26),\n",
       "             ('marvels', 26),\n",
       "             ('patagonian', 26),\n",
       "             ('sights', 26),\n",
       "             ('sounds', 78),\n",
       "             ('helped', 26),\n",
       "             ('sway', 26),\n",
       "             ('wish', 26),\n",
       "             ('inducements', 26),\n",
       "             ('tormented', 52),\n",
       "             ('everlasting', 52),\n",
       "             ('itch', 26),\n",
       "             ('remote', 26),\n",
       "             ('love', 26),\n",
       "             ('forbidden', 26),\n",
       "             ('barbarous', 26),\n",
       "             ('coasts', 26),\n",
       "             ('ignoring', 26),\n",
       "             ('good', 442),\n",
       "             ('quick', 26),\n",
       "             ('perceive', 26),\n",
       "             ('horror', 26),\n",
       "             ('social', 26),\n",
       "             ('since', 78),\n",
       "             ('friendly', 26),\n",
       "             ('terms', 26),\n",
       "             ('inmates', 26),\n",
       "             ('place', 390),\n",
       "             ('lodges', 26),\n",
       "             ('reason', 130),\n",
       "             ('welcome', 26),\n",
       "             ('flood', 26),\n",
       "             ('gates', 26),\n",
       "             ('wonder', 52),\n",
       "             ('swung', 26),\n",
       "             ('open', 104),\n",
       "             ('conceits', 26),\n",
       "             ('swayed', 26),\n",
       "             ('purpose', 52),\n",
       "             ('floated', 52),\n",
       "             ('inmost', 26),\n",
       "             ('endless', 26),\n",
       "             ('processions', 26),\n",
       "             ('mid', 26),\n",
       "             ('hooded', 26),\n",
       "             ('snow', 78),\n",
       "             ('stuffed', 52),\n",
       "             ('shirt', 104),\n",
       "             ('carpet', 26),\n",
       "             ('bag', 182),\n",
       "             ('tucked', 26),\n",
       "             ('arm', 286),\n",
       "             ('started', 26),\n",
       "             ('cape', 104),\n",
       "             ('horn', 52),\n",
       "             ('pacific', 26),\n",
       "             ('quitting', 26),\n",
       "             ('manhatto', 26),\n",
       "             ('duly', 26),\n",
       "             ('arrived', 52),\n",
       "             ('bedford', 104),\n",
       "             ('saturday', 78),\n",
       "             ('night', 624),\n",
       "             ('december', 26),\n",
       "             ('disappointed', 26),\n",
       "             ('learning', 26),\n",
       "             ('packet', 26),\n",
       "             ('nantucket', 182),\n",
       "             ('had', 858),\n",
       "             ('already', 26),\n",
       "             ('sailed', 26),\n",
       "             ('offer', 52),\n",
       "             ('till', 156),\n",
       "             ('following', 52),\n",
       "             ('monday', 26),\n",
       "             ('young', 130),\n",
       "             ('candidates', 26),\n",
       "             ('pains', 26),\n",
       "             ('penalties', 26),\n",
       "             ('stop', 208),\n",
       "             ('embark', 52),\n",
       "             ('related', 26),\n",
       "             ('doing', 26),\n",
       "             ('made', 338),\n",
       "             ('craft', 130),\n",
       "             ('fine', 104),\n",
       "             ('boisterous', 26),\n",
       "             ('everything', 26),\n",
       "             ('connected', 26),\n",
       "             ('famous', 26),\n",
       "             ('amazingly', 26),\n",
       "             ('pleased', 52),\n",
       "             ('late', 182),\n",
       "             ('gradually', 26),\n",
       "             ('monopolising', 26),\n",
       "             ('business', 130),\n",
       "             ('matter', 78),\n",
       "             ('behind', 26),\n",
       "             ('original', 52),\n",
       "             ('tyre', 26),\n",
       "             ('carthage', 26),\n",
       "             ('dead', 130),\n",
       "             ('stranded', 52),\n",
       "             ('aboriginal', 26),\n",
       "             ('whalemen', 26),\n",
       "             ('red', 78),\n",
       "             ('sally', 26),\n",
       "             ('canoes', 26),\n",
       "             ('chase', 26),\n",
       "             ('leviathan', 52),\n",
       "             ('too', 364),\n",
       "             ('adventurous', 26),\n",
       "             ('sloop', 26),\n",
       "             ('forth', 26),\n",
       "             ('partly', 78),\n",
       "             ('laden', 26),\n",
       "             ('imported', 26),\n",
       "             ('cobblestones', 26),\n",
       "             ('throw', 26),\n",
       "             ('whales', 52),\n",
       "             ('discover', 26),\n",
       "             ('risk', 26),\n",
       "             ('harpoon', 135),\n",
       "             ('bowsprit', 26),\n",
       "             ('day', 156),\n",
       "             ('another', 130),\n",
       "             ('ere', 78),\n",
       "             ('destined', 26),\n",
       "             ('port', 26),\n",
       "             ('became', 78),\n",
       "             ('concernment', 26),\n",
       "             ('eat', 26),\n",
       "             ('meanwhile', 78),\n",
       "             ('dubious', 26),\n",
       "             ('nay', 52),\n",
       "             ('dark', 208),\n",
       "             ('dismal', 52),\n",
       "             ('bitingly', 26),\n",
       "             ('cold', 182),\n",
       "             ('cheerless', 26),\n",
       "             ('anxious', 26),\n",
       "             ('grapnels', 26),\n",
       "             ('sounded', 26),\n",
       "             ('pocket', 78),\n",
       "             ('only', 364),\n",
       "             ('brought', 26),\n",
       "             ('pieces', 26),\n",
       "             ('wherever', 52),\n",
       "             ('said', 494),\n",
       "             ('stood', 260),\n",
       "             ('middle', 130),\n",
       "             ('dreary', 52),\n",
       "             ('shouldering', 26),\n",
       "             ('comparing', 26),\n",
       "             ('gloom', 26),\n",
       "             ('darkness', 78),\n",
       "             ('wisdom', 26),\n",
       "             ('conclude', 26),\n",
       "             ('lodge', 26),\n",
       "             ('dear', 26),\n",
       "             ('sure', 130),\n",
       "             ('inquire', 26),\n",
       "             ('price', 26),\n",
       "             ('halting', 26),\n",
       "             ('steps', 26),\n",
       "             ('paced', 26),\n",
       "             ('sign', 156),\n",
       "             ('crossed', 52),\n",
       "             ('harpoons', 78),\n",
       "             ('\"--', 78),\n",
       "             ('looked', 156),\n",
       "             ('expensive', 52),\n",
       "             ('further', 104),\n",
       "             ('bright', 52),\n",
       "             ('windows', 52),\n",
       "             ('fish', 104),\n",
       "             ('inn', 104),\n",
       "             ('fervent', 26),\n",
       "             ('rays', 26),\n",
       "             ('seemed', 416),\n",
       "             ('melted', 26),\n",
       "             ('packed', 52),\n",
       "             ('ice', 104),\n",
       "             ('house', 286),\n",
       "             ('everywhere', 26),\n",
       "             ('congealed', 26),\n",
       "             ('frost', 104),\n",
       "             ('lay', 234),\n",
       "             ('inches', 52),\n",
       "             ('thick', 52),\n",
       "             ...])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want this to be a matrix\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,   14,  265, ..., 2704,   14,   24],\n",
       "       [  14,  265,   51, ...,   14,   24,  965],\n",
       "       [ 265,   51,  263, ...,   24,  965,    5],\n",
       "       ...,\n",
       "       [ 960,   12,  168, ...,  264,   53,    2],\n",
       "       [  12,  168, 2703, ...,   53,    2, 2709],\n",
       "       [ 168, 2703,    3, ...,    2, 2709,   26]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all columns but the last\n",
    "X = sequences[:,:-1]\n",
    "# Grab only the last column\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note the syntax for factor selection\n",
    "w = np.array([[1,2,3],[4,5,6]])\n",
    "display(w)\n",
    "display(w[:,:-1])\n",
    "display(w[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11368, 25)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size,seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # turns positive integers into dense vectors of a fixed size\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=seq_len, input_length=seq_len))\n",
    "    # Common to make this some multiple of your sequence length: seq_len*10 = units = #neurons\n",
    "    model.add(LSTM(units=seq_len*10, return_sequences=True))\n",
    "    model.add(LSTM(units=seq_len*10))\n",
    "    model.add(Dense(units=seq_len*10, activation='relu'))\n",
    "    \n",
    "    # output of network - a word\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 25)            67750     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 250)           276000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 250)               501000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2710)              680210    \n",
      "=================================================================\n",
      "Total params: 1,587,710\n",
      "Trainable params: 1,587,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11368/11368 [==============================] - 113s 10ms/sample - loss: 6.8042 - acc: 0.0464\n",
      "Epoch 2/2\n",
      "11368/11368 [==============================] - 107s 9ms/sample - loss: 6.3800 - acc: 0.0527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5cf76090>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text based off of a seeded input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate new text\n",
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    \"\"\"\n",
    "    Function that takes in some text and returns some predicted words to complete the sentence\n",
    "    \n",
    "    model: trained DL model\n",
    "    tokenizer: association of vocabulary and what ID number goes with what word\n",
    "    seq_len: length of the sequence of expected texted\n",
    "    seed_text: input text (preferably the same length as the sequence)\n",
    "    num_gen_words: number of words to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len,truncating='pre')\n",
    "        \n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        \n",
    "        input_text += ' '+pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'throwing',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'to',\n",
       " 'one',\n",
       " 'side',\n",
       " 'he',\n",
       " 'really',\n",
       " 'did',\n",
       " 'this',\n",
       " 'in',\n",
       " 'not',\n",
       " 'only',\n",
       " 'a',\n",
       " 'civil',\n",
       " 'but',\n",
       " 'a',\n",
       " 'really',\n",
       " 'kind',\n",
       " 'and',\n",
       " 'charitable',\n",
       " 'way',\n",
       " 'i',\n",
       " 'stood',\n",
       " 'looking']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and throwing the clothes to one side he really did this in not only a civil but a really kind and charitable way i stood looking'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the the the the the the the the the the the the the the the the the the the the the the the the the'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load in a much more deeply trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('epochBIG.h5')\n",
    "tokenizer = load(open('epochBIG','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board existence ha often massa parcel how a thing alone touching broken shy cried sir free on boys my hail here rings a special here'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=text_sequences[0],num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
