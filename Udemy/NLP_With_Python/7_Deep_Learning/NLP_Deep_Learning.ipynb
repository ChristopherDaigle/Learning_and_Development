{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use one-hot-encoding to use in Keras\n",
    "# Class_0 --> [1,0,0]\n",
    "# Class_1 --> [0,1,0]\n",
    "# Class_2 --> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array([5,10,15,20])/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only fit to the training features\n",
    "scaler_obj.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_obj.transform(X_train)\n",
    "scaled_X_test = scaler_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mbair/anaconda3/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      " - 2s - loss: 1.2513 - acc: 0.3100\n",
      "Epoch 2/150\n",
      " - 0s - loss: 1.2316 - acc: 0.3100\n",
      "Epoch 3/150\n",
      " - 0s - loss: 1.2108 - acc: 0.3100\n",
      "Epoch 4/150\n",
      " - 0s - loss: 1.1914 - acc: 0.3000\n",
      "Epoch 5/150\n",
      " - 0s - loss: 1.1726 - acc: 0.3000\n",
      "Epoch 6/150\n",
      " - 0s - loss: 1.1540 - acc: 0.3000\n",
      "Epoch 7/150\n",
      " - 0s - loss: 1.1365 - acc: 0.3100\n",
      "Epoch 8/150\n",
      " - 0s - loss: 1.1197 - acc: 0.3100\n",
      "Epoch 9/150\n",
      " - 0s - loss: 1.1027 - acc: 0.5300\n",
      "Epoch 10/150\n",
      " - 0s - loss: 1.0862 - acc: 0.6300\n",
      "Epoch 11/150\n",
      " - 0s - loss: 1.0710 - acc: 0.6400\n",
      "Epoch 12/150\n",
      " - 0s - loss: 1.0558 - acc: 0.6400\n",
      "Epoch 13/150\n",
      " - 0s - loss: 1.0417 - acc: 0.6400\n",
      "Epoch 14/150\n",
      " - 0s - loss: 1.0279 - acc: 0.6400\n",
      "Epoch 15/150\n",
      " - 0s - loss: 1.0147 - acc: 0.6400\n",
      "Epoch 16/150\n",
      " - 0s - loss: 1.0014 - acc: 0.6400\n",
      "Epoch 17/150\n",
      " - 0s - loss: 0.9888 - acc: 0.6400\n",
      "Epoch 18/150\n",
      " - 0s - loss: 0.9763 - acc: 0.6500\n",
      "Epoch 19/150\n",
      " - 0s - loss: 0.9639 - acc: 0.6500\n",
      "Epoch 20/150\n",
      " - 0s - loss: 0.9517 - acc: 0.6500\n",
      "Epoch 21/150\n",
      " - 0s - loss: 0.9395 - acc: 0.6500\n",
      "Epoch 22/150\n",
      " - 0s - loss: 0.9270 - acc: 0.6500\n",
      "Epoch 23/150\n",
      " - 0s - loss: 0.9149 - acc: 0.6500\n",
      "Epoch 24/150\n",
      " - 0s - loss: 0.9028 - acc: 0.6500\n",
      "Epoch 25/150\n",
      " - 0s - loss: 0.8913 - acc: 0.6500\n",
      "Epoch 26/150\n",
      " - 0s - loss: 0.8802 - acc: 0.6500\n",
      "Epoch 27/150\n",
      " - 0s - loss: 0.8696 - acc: 0.6700\n",
      "Epoch 28/150\n",
      " - 0s - loss: 0.8585 - acc: 0.6800\n",
      "Epoch 29/150\n",
      " - 0s - loss: 0.8479 - acc: 0.6800\n",
      "Epoch 30/150\n",
      " - 0s - loss: 0.8372 - acc: 0.6800\n",
      "Epoch 31/150\n",
      " - 0s - loss: 0.8268 - acc: 0.6800\n",
      "Epoch 32/150\n",
      " - 0s - loss: 0.8169 - acc: 0.6800\n",
      "Epoch 33/150\n",
      " - 0s - loss: 0.8067 - acc: 0.6800\n",
      "Epoch 34/150\n",
      " - 0s - loss: 0.7964 - acc: 0.6900\n",
      "Epoch 35/150\n",
      " - 0s - loss: 0.7869 - acc: 0.6900\n",
      "Epoch 36/150\n",
      " - 0s - loss: 0.7768 - acc: 0.6900\n",
      "Epoch 37/150\n",
      " - 0s - loss: 0.7676 - acc: 0.6900\n",
      "Epoch 38/150\n",
      " - 0s - loss: 0.7584 - acc: 0.6800\n",
      "Epoch 39/150\n",
      " - 0s - loss: 0.7493 - acc: 0.6800\n",
      "Epoch 40/150\n",
      " - 0s - loss: 0.7404 - acc: 0.6800\n",
      "Epoch 41/150\n",
      " - 0s - loss: 0.7315 - acc: 0.6800\n",
      "Epoch 42/150\n",
      " - 0s - loss: 0.7228 - acc: 0.6800\n",
      "Epoch 43/150\n",
      " - 0s - loss: 0.7143 - acc: 0.6800\n",
      "Epoch 44/150\n",
      " - 0s - loss: 0.7062 - acc: 0.6800\n",
      "Epoch 45/150\n",
      " - 0s - loss: 0.6980 - acc: 0.6800\n",
      "Epoch 46/150\n",
      " - 0s - loss: 0.6900 - acc: 0.6800\n",
      "Epoch 47/150\n",
      " - 0s - loss: 0.6822 - acc: 0.6800\n",
      "Epoch 48/150\n",
      " - 0s - loss: 0.6742 - acc: 0.6900\n",
      "Epoch 49/150\n",
      " - 0s - loss: 0.6677 - acc: 0.7000\n",
      "Epoch 50/150\n",
      " - 0s - loss: 0.6595 - acc: 0.7100\n",
      "Epoch 51/150\n",
      " - 0s - loss: 0.6525 - acc: 0.7500\n",
      "Epoch 52/150\n",
      " - 0s - loss: 0.6455 - acc: 0.7800\n",
      "Epoch 53/150\n",
      " - 0s - loss: 0.6382 - acc: 0.7800\n",
      "Epoch 54/150\n",
      " - 0s - loss: 0.6320 - acc: 0.7900\n",
      "Epoch 55/150\n",
      " - 0s - loss: 0.6258 - acc: 0.8200\n",
      "Epoch 56/150\n",
      " - 0s - loss: 0.6197 - acc: 0.8300\n",
      "Epoch 57/150\n",
      " - 0s - loss: 0.6136 - acc: 0.8500\n",
      "Epoch 58/150\n",
      " - 0s - loss: 0.6074 - acc: 0.8400\n",
      "Epoch 59/150\n",
      " - 0s - loss: 0.6015 - acc: 0.8100\n",
      "Epoch 60/150\n",
      " - 0s - loss: 0.5958 - acc: 0.8000\n",
      "Epoch 61/150\n",
      " - 0s - loss: 0.5904 - acc: 0.7900\n",
      "Epoch 62/150\n",
      " - 0s - loss: 0.5851 - acc: 0.7400\n",
      "Epoch 63/150\n",
      " - 0s - loss: 0.5804 - acc: 0.7200\n",
      "Epoch 64/150\n",
      " - 0s - loss: 0.5754 - acc: 0.7200\n",
      "Epoch 65/150\n",
      " - 0s - loss: 0.5706 - acc: 0.7200\n",
      "Epoch 66/150\n",
      " - 0s - loss: 0.5662 - acc: 0.7200\n",
      "Epoch 67/150\n",
      " - 0s - loss: 0.5616 - acc: 0.7200\n",
      "Epoch 68/150\n",
      " - 0s - loss: 0.5570 - acc: 0.7200\n",
      "Epoch 69/150\n",
      " - 0s - loss: 0.5528 - acc: 0.7200\n",
      "Epoch 70/150\n",
      " - 0s - loss: 0.5484 - acc: 0.7200\n",
      "Epoch 71/150\n",
      " - 0s - loss: 0.5447 - acc: 0.7200\n",
      "Epoch 72/150\n",
      " - 0s - loss: 0.5409 - acc: 0.7100\n",
      "Epoch 73/150\n",
      " - 0s - loss: 0.5368 - acc: 0.7200\n",
      "Epoch 74/150\n",
      " - 0s - loss: 0.5332 - acc: 0.7200\n",
      "Epoch 75/150\n",
      " - 0s - loss: 0.5291 - acc: 0.7300\n",
      "Epoch 76/150\n",
      " - 0s - loss: 0.5257 - acc: 0.7700\n",
      "Epoch 77/150\n",
      " - 0s - loss: 0.5222 - acc: 0.8100\n",
      "Epoch 78/150\n",
      " - 0s - loss: 0.5184 - acc: 0.8200\n",
      "Epoch 79/150\n",
      " - 0s - loss: 0.5156 - acc: 0.8600\n",
      "Epoch 80/150\n",
      " - 0s - loss: 0.5123 - acc: 0.8600\n",
      "Epoch 81/150\n",
      " - 0s - loss: 0.5094 - acc: 0.8600\n",
      "Epoch 82/150\n",
      " - 0s - loss: 0.5063 - acc: 0.8700\n",
      "Epoch 83/150\n",
      " - 0s - loss: 0.5029 - acc: 0.8600\n",
      "Epoch 84/150\n",
      " - 0s - loss: 0.5001 - acc: 0.8300\n",
      "Epoch 85/150\n",
      " - 0s - loss: 0.4970 - acc: 0.8300\n",
      "Epoch 86/150\n",
      " - 0s - loss: 0.4942 - acc: 0.8200\n",
      "Epoch 87/150\n",
      " - 0s - loss: 0.4915 - acc: 0.8200\n",
      "Epoch 88/150\n",
      " - 0s - loss: 0.4889 - acc: 0.8200\n",
      "Epoch 89/150\n",
      " - 0s - loss: 0.4859 - acc: 0.8200\n",
      "Epoch 90/150\n",
      " - 0s - loss: 0.4834 - acc: 0.8400\n",
      "Epoch 91/150\n",
      " - 0s - loss: 0.4805 - acc: 0.8500\n",
      "Epoch 92/150\n",
      " - 0s - loss: 0.4780 - acc: 0.8600\n",
      "Epoch 93/150\n",
      " - 0s - loss: 0.4754 - acc: 0.8500\n",
      "Epoch 94/150\n",
      " - 0s - loss: 0.4727 - acc: 0.8500\n",
      "Epoch 95/150\n",
      " - 0s - loss: 0.4701 - acc: 0.8500\n",
      "Epoch 96/150\n",
      " - 0s - loss: 0.4679 - acc: 0.8600\n",
      "Epoch 97/150\n",
      " - 0s - loss: 0.4655 - acc: 0.8600\n",
      "Epoch 98/150\n",
      " - 0s - loss: 0.4632 - acc: 0.8600\n",
      "Epoch 99/150\n",
      " - 0s - loss: 0.4609 - acc: 0.8600\n",
      "Epoch 100/150\n",
      " - 0s - loss: 0.4584 - acc: 0.8600\n",
      "Epoch 101/150\n",
      " - 0s - loss: 0.4562 - acc: 0.8600\n",
      "Epoch 102/150\n",
      " - 0s - loss: 0.4538 - acc: 0.8600\n",
      "Epoch 103/150\n",
      " - 0s - loss: 0.4515 - acc: 0.8700\n",
      "Epoch 104/150\n",
      " - 0s - loss: 0.4492 - acc: 0.8700\n",
      "Epoch 105/150\n",
      " - 0s - loss: 0.4469 - acc: 0.8700\n",
      "Epoch 106/150\n",
      " - 0s - loss: 0.4445 - acc: 0.8900\n",
      "Epoch 107/150\n",
      " - 0s - loss: 0.4425 - acc: 0.8900\n",
      "Epoch 108/150\n",
      " - 0s - loss: 0.4401 - acc: 0.8900\n",
      "Epoch 109/150\n",
      " - 0s - loss: 0.4378 - acc: 0.8800\n",
      "Epoch 110/150\n",
      " - 0s - loss: 0.4359 - acc: 0.8700\n",
      "Epoch 111/150\n",
      " - 0s - loss: 0.4338 - acc: 0.8700\n",
      "Epoch 112/150\n",
      " - 0s - loss: 0.4316 - acc: 0.8700\n",
      "Epoch 113/150\n",
      " - 0s - loss: 0.4292 - acc: 0.8900\n",
      "Epoch 114/150\n",
      " - 0s - loss: 0.4271 - acc: 0.8900\n",
      "Epoch 115/150\n",
      " - 0s - loss: 0.4248 - acc: 0.8900\n",
      "Epoch 116/150\n",
      " - 0s - loss: 0.4234 - acc: 0.9100\n",
      "Epoch 117/150\n",
      " - 0s - loss: 0.4212 - acc: 0.9100\n",
      "Epoch 118/150\n",
      " - 0s - loss: 0.4191 - acc: 0.9100\n",
      "Epoch 119/150\n",
      " - 0s - loss: 0.4168 - acc: 0.9100\n",
      "Epoch 120/150\n",
      " - 0s - loss: 0.4144 - acc: 0.8900\n",
      "Epoch 121/150\n",
      " - 0s - loss: 0.4129 - acc: 0.8700\n",
      "Epoch 122/150\n",
      " - 0s - loss: 0.4132 - acc: 0.8500\n",
      "Epoch 123/150\n",
      " - 0s - loss: 0.4125 - acc: 0.8500\n",
      "Epoch 124/150\n",
      " - 0s - loss: 0.4118 - acc: 0.8500\n",
      "Epoch 125/150\n",
      " - 0s - loss: 0.4098 - acc: 0.8500\n",
      "Epoch 126/150\n",
      " - 0s - loss: 0.4067 - acc: 0.8500\n",
      "Epoch 127/150\n",
      " - 0s - loss: 0.4031 - acc: 0.8600\n",
      "Epoch 128/150\n",
      " - 0s - loss: 0.4002 - acc: 0.9000\n",
      "Epoch 129/150\n",
      " - 0s - loss: 0.3984 - acc: 0.8900\n",
      "Epoch 130/150\n",
      " - 0s - loss: 0.3971 - acc: 0.9100\n",
      "Epoch 131/150\n",
      " - 0s - loss: 0.3947 - acc: 0.9100\n",
      "Epoch 132/150\n",
      " - 0s - loss: 0.3932 - acc: 0.9100\n",
      "Epoch 133/150\n",
      " - 0s - loss: 0.3916 - acc: 0.9100\n",
      "Epoch 134/150\n",
      " - 0s - loss: 0.3903 - acc: 0.9100\n",
      "Epoch 135/150\n",
      " - 0s - loss: 0.3877 - acc: 0.9100\n",
      "Epoch 136/150\n",
      " - 0s - loss: 0.3860 - acc: 0.9100\n",
      "Epoch 137/150\n",
      " - 0s - loss: 0.3841 - acc: 0.9100\n",
      "Epoch 138/150\n",
      " - 0s - loss: 0.3821 - acc: 0.9200\n",
      "Epoch 139/150\n",
      " - 0s - loss: 0.3803 - acc: 0.9200\n",
      "Epoch 140/150\n",
      " - 0s - loss: 0.3786 - acc: 0.9100\n",
      "Epoch 141/150\n",
      " - 0s - loss: 0.3767 - acc: 0.9300\n",
      "Epoch 142/150\n",
      " - 0s - loss: 0.3750 - acc: 0.9300\n",
      "Epoch 143/150\n",
      " - 0s - loss: 0.3729 - acc: 0.9300\n",
      "Epoch 144/150\n",
      " - 0s - loss: 0.3711 - acc: 0.9300\n",
      "Epoch 145/150\n",
      " - 0s - loss: 0.3693 - acc: 0.9300\n",
      "Epoch 146/150\n",
      " - 0s - loss: 0.3672 - acc: 0.9300\n",
      "Epoch 147/150\n",
      " - 0s - loss: 0.3651 - acc: 0.9300\n",
      "Epoch 148/150\n",
      " - 0s - loss: 0.3638 - acc: 0.9200\n",
      "Epoch 149/150\n",
      " - 0s - loss: 0.3625 - acc: 0.9400\n",
      "Epoch 150/150\n",
      " - 0s - loss: 0.3614 - acc: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3e124190>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs is the number of times the network \"runs\" through the entire training dataset\n",
    "# verbose is the amount of info to output\n",
    "model.fit(scaled_X_train,y_train,epochs=150,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)\n",
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred0</th>\n",
       "      <th>Pred1</th>\n",
       "      <th>Pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pred0  Pred1  Pred2\n",
       "True0     19      0      0\n",
       "True1      0     13      2\n",
       "True2      0      1     15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.93      0.87      0.90        15\n",
      "           2       0.88      0.94      0.91        16\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        50\n",
      "   macro avg       0.94      0.93      0.94        50\n",
      "weighted avg       0.94      0.94      0.94        50\n",
      "\n",
      "Accuracy Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(confusion_matrix(y_test.argmax(axis=1),predictions), \n",
    "                            index=['True0','True1', 'True2'], \n",
    "                            columns=['Pred0','Pred1', 'Pred2']))\n",
    "print(classification_report(y_test.argmax(axis=1),predictions))\n",
    "print('Accuracy Score:', accuracy_score(y_test.argmax(axis=1),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the model to be re-used\n",
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "> Specifically designed to work with sequence data\n",
    "* Time series\n",
    "* Sentences (NLP)\n",
    "* Audio\n",
    "* Car Trajectories\n",
    "\n",
    "<img src='feed_forward_neuron.png'>\n",
    "<img src='recur_neuron.png'>\n",
    "<img src='recur_neuron_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Cells:** Nodes that are a function of inputs from previous time steps\n",
    "\n",
    "<img src='recur_layer.png'>\n",
    "<img src='recur_layer_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM & GRU\n",
    "* Long-short term memory\n",
    "* \n",
    "\n",
    "> Help to reduce the \"forgetfullness\" of RNNs as what was input at $t_{0}$ may lose its influence at $t_{i+j}$\n",
    "\n",
    "<img src='lstm_cell.png'>\n",
    "\n",
    "**Input:**\n",
    "1. $C_{t-1}$: Cell state at $t-1$\n",
    "2. $h_{t-1}$: outcome of previous cell\n",
    "3. $x_{t}$: factor of relevance\n",
    "\n",
    "### First Step: Forget Gate Layer\n",
    "* We decide what information we are going to \"forget\", or \"throw away\", from the cell state\n",
    "<img src='forget_gate_layer.png'>\n",
    "\n",
    "### Second Step:\n",
    "* We decide what to store\n",
    "<img src='lstm_stage_2.png'>\n",
    "\n",
    "### Third Step:\n",
    "* We update the cell state (to send to the next cell state)\n",
    "<img src='lstm_stage_3.png'>\n",
    "\n",
    "### Fourth Step:\n",
    "* We output $h_{t}$\n",
    "<img src='lstm_stage_4.png'>\n",
    "\n",
    "**Different kind of LSTMs:**\n",
    "1. Peephole\n",
    "2. Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Peephole**\n",
    "<img src='lstm_peep.png'>\n",
    "**GRU**\n",
    "<img src='lstm_GRU.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTMs with Keras and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "        \n",
    "        return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize the load by specifiying what to disable\n",
    "nlp = spacy.load('en',disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    import string\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "#     return [token.text.lower() for token in nlp(doc_text) if token.text not in [string.punctuation, '\\n\\n \\n\\n\\n!\"']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 words --> network predict #26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on\n",
      "\n",
      "me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore\n",
      "\n",
      "ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i\n",
      "\n",
      "some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought\n",
      "\n",
      "years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each item in the list is composed of one word to the right of the previous, and less the first\n",
    "# - shift the text one word right\n",
    "for i in range(5):\n",
    "    print(' '.join(text_sequences[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c879ee320cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[964,\n",
       " 14,\n",
       " 265,\n",
       " 51,\n",
       " 263,\n",
       " 416,\n",
       " 87,\n",
       " 222,\n",
       " 129,\n",
       " 111,\n",
       " 962,\n",
       " 262,\n",
       " 50,\n",
       " 43,\n",
       " 37,\n",
       " 321,\n",
       " 7,\n",
       " 23,\n",
       " 555,\n",
       " 3,\n",
       " 150,\n",
       " 261,\n",
       " 6,\n",
       " 2704,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[964]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964: call\n",
      "14: me\n",
      "265: ishmael\n",
      "51: some\n",
      "263: years\n",
      "416: ago\n",
      "87: never\n",
      "222: mind\n",
      "129: how\n",
      "111: long\n",
      "962: precisely\n",
      "262: having\n",
      "50: little\n",
      "43: or\n",
      "37: no\n",
      "321: money\n",
      "7: in\n",
      "23: my\n",
      "555: purse\n",
      "3: and\n",
      "150: nothing\n",
      "261: particular\n",
      "6: to\n",
      "2704: interest\n",
      "14: me\n",
      "24: on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i}: {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('call', 27),\n",
       "             ('me', 2471),\n",
       "             ('ishmael', 133),\n",
       "             ('some', 758),\n",
       "             ('years', 135),\n",
       "             ('ago', 84),\n",
       "             ('never', 449),\n",
       "             ('mind', 164),\n",
       "             ('how', 321),\n",
       "             ('long', 374),\n",
       "             ('precisely', 37),\n",
       "             ('having', 142),\n",
       "             ('little', 767),\n",
       "             ('or', 950),\n",
       "             ('no', 1029),\n",
       "             ('money', 120),\n",
       "             ('in', 5647),\n",
       "             ('my', 1812),\n",
       "             ('purse', 71),\n",
       "             ('and', 9646),\n",
       "             ('nothing', 281),\n",
       "             ('particular', 152),\n",
       "             ('to', 6497),\n",
       "             ('interest', 24),\n",
       "             ('on', 1716),\n",
       "             ('shore', 26),\n",
       "             ('i', 7176),\n",
       "             ('thought', 676),\n",
       "             ('would', 702),\n",
       "             ('sail', 104),\n",
       "             ('about', 1014),\n",
       "             ('a', 10377),\n",
       "             ('see', 442),\n",
       "             ('the', 15566),\n",
       "             ('watery', 26),\n",
       "             ('part', 234),\n",
       "             ('of', 8287),\n",
       "             ('world', 234),\n",
       "             ('it', 4394),\n",
       "             ('is', 1950),\n",
       "             ('way', 390),\n",
       "             ('have', 806),\n",
       "             ('driving', 26),\n",
       "             ('off', 416),\n",
       "             ('spleen', 26),\n",
       "             ('regulating', 26),\n",
       "             ('circulation', 26),\n",
       "             ('whenever', 130),\n",
       "             ('find', 78),\n",
       "             ('myself', 416),\n",
       "             ('growing', 26),\n",
       "             ('grim', 26),\n",
       "             ('mouth', 130),\n",
       "             ('damp', 78),\n",
       "             ('drizzly', 26),\n",
       "             ('november', 26),\n",
       "             ('soul', 78),\n",
       "             ('involuntarily', 52),\n",
       "             ('pausing', 52),\n",
       "             ('before', 364),\n",
       "             ('coffin', 130),\n",
       "             ('warehouses', 52),\n",
       "             ('bringing', 26),\n",
       "             ('up', 1237),\n",
       "             ('rear', 26),\n",
       "             ('every', 182),\n",
       "             ('funeral', 26),\n",
       "             ('meet', 26),\n",
       "             ('especially', 104),\n",
       "             ('hypos', 26),\n",
       "             ('get', 364),\n",
       "             ('such', 572),\n",
       "             ('an', 806),\n",
       "             ('upper', 26),\n",
       "             ('hand', 312),\n",
       "             ('that', 3770),\n",
       "             ('requires', 52),\n",
       "             ('strong', 78),\n",
       "             ('moral', 26),\n",
       "             ('principle', 26),\n",
       "             ('prevent', 26),\n",
       "             ('from', 1508),\n",
       "             ('deliberately', 26),\n",
       "             ('stepping', 26),\n",
       "             ('into', 988),\n",
       "             ('street', 104),\n",
       "             ('methodically', 26),\n",
       "             ('knocking', 26),\n",
       "             ('people', 52),\n",
       "             (\"'s\", 1691),\n",
       "             ('hats', 26),\n",
       "             ('then', 832),\n",
       "             ('account', 78),\n",
       "             ('high', 130),\n",
       "             ('time', 520),\n",
       "             ('sea', 546),\n",
       "             ('as', 2366),\n",
       "             ('soon', 234),\n",
       "             ('can', 338),\n",
       "             ('this', 2184),\n",
       "             ('substitute', 26),\n",
       "             ('for', 1820),\n",
       "             ('pistol', 26),\n",
       "             ('ball', 26),\n",
       "             ('with', 2392),\n",
       "             ('philosophical', 26),\n",
       "             ('flourish', 26),\n",
       "             ('cato', 26),\n",
       "             ('throws', 26),\n",
       "             ('himself', 338),\n",
       "             ('upon', 780),\n",
       "             ('his', 3139),\n",
       "             ('sword', 78),\n",
       "             ('quietly', 78),\n",
       "             ('take', 260),\n",
       "             ('ship', 182),\n",
       "             ('there', 1456),\n",
       "             ('surprising', 26),\n",
       "             ('if', 728),\n",
       "             ('they', 728),\n",
       "             ('but', 2704),\n",
       "             ('knew', 130),\n",
       "             ('almost', 286),\n",
       "             ('all', 1872),\n",
       "             ('men', 130),\n",
       "             ('their', 390),\n",
       "             ('degree', 78),\n",
       "             ('other', 494),\n",
       "             ('cherish', 26),\n",
       "             ('very', 494),\n",
       "             ('nearly', 52),\n",
       "             ('same', 312),\n",
       "             ('feelings', 26),\n",
       "             ('towards', 260),\n",
       "             ('ocean', 52),\n",
       "             ('now', 1040),\n",
       "             ('your', 442),\n",
       "             ('insular', 26),\n",
       "             ('city', 104),\n",
       "             ('manhattoes', 26),\n",
       "             ('belted', 26),\n",
       "             ('round', 364),\n",
       "             ('by', 962),\n",
       "             ('wharves', 26),\n",
       "             ('indian', 52),\n",
       "             ('isles', 26),\n",
       "             ('coral', 26),\n",
       "             ('reefs', 26),\n",
       "             ('commerce', 26),\n",
       "             ('surrounds', 26),\n",
       "             ('her', 156),\n",
       "             ('surf', 26),\n",
       "             ('right', 156),\n",
       "             ('left', 78),\n",
       "             ('streets', 208),\n",
       "             ('you', 2210),\n",
       "             ('waterward', 52),\n",
       "             ('its', 156),\n",
       "             ('extreme', 26),\n",
       "             ('downtown', 26),\n",
       "             ('battery', 52),\n",
       "             ('where', 364),\n",
       "             ('noble', 52),\n",
       "             ('mole', 26),\n",
       "             ('washed', 52),\n",
       "             ('waves', 26),\n",
       "             ('cooled', 26),\n",
       "             ('breezes', 26),\n",
       "             ('which', 572),\n",
       "             ('few', 104),\n",
       "             ('hours', 130),\n",
       "             ('previous', 104),\n",
       "             ('were', 962),\n",
       "             ('out', 956),\n",
       "             ('sight', 104),\n",
       "             ('land', 208),\n",
       "             ('look', 156),\n",
       "             ('at', 2184),\n",
       "             ('crowds', 52),\n",
       "             ('water', 260),\n",
       "             ('gazers', 26),\n",
       "             ('circumambulate', 26),\n",
       "             ('dreamy', 26),\n",
       "             ('sabbath', 52),\n",
       "             ('afternoon', 52),\n",
       "             ('go', 494),\n",
       "             ('corlears', 26),\n",
       "             ('hook', 26),\n",
       "             ('coenties', 26),\n",
       "             ('slip', 26),\n",
       "             ('thence', 52),\n",
       "             ('whitehall', 26),\n",
       "             ('northward', 26),\n",
       "             ('what', 1170),\n",
       "             ('do', 702),\n",
       "             ('?--', 182),\n",
       "             ('posted', 26),\n",
       "             ('like', 732),\n",
       "             ('silent', 52),\n",
       "             ('sentinels', 26),\n",
       "             ('around', 78),\n",
       "             ('town', 182),\n",
       "             ('stand', 182),\n",
       "             ('thousands', 52),\n",
       "             ('mortal', 26),\n",
       "             ('fixed', 78),\n",
       "             ('reveries', 52),\n",
       "             ('leaning', 52),\n",
       "             ('against', 234),\n",
       "             ('spiles', 26),\n",
       "             ('seated', 52),\n",
       "             ('pier', 26),\n",
       "             ('heads', 338),\n",
       "             ('looking', 312),\n",
       "             ('over', 702),\n",
       "             ('bulwarks', 52),\n",
       "             ('ships', 78),\n",
       "             ('china', 26),\n",
       "             ('aloft', 52),\n",
       "             ('rigging', 26),\n",
       "             ('striving', 26),\n",
       "             ('still', 364),\n",
       "             ('better', 208),\n",
       "             ('seaward', 26),\n",
       "             ('peep', 26),\n",
       "             ('these', 494),\n",
       "             ('are', 416),\n",
       "             ('landsmen', 26),\n",
       "             ('week', 52),\n",
       "             ('days', 104),\n",
       "             ('pent', 26),\n",
       "             ('lath', 26),\n",
       "             ('plaster', 52),\n",
       "             ('tied', 26),\n",
       "             ('counters', 26),\n",
       "             ('nailed', 26),\n",
       "             ('benches', 26),\n",
       "             ('clinched', 26),\n",
       "             ('desks', 26),\n",
       "             ('green', 130),\n",
       "             ('fields', 26),\n",
       "             ('gone', 52),\n",
       "             ('here', 624),\n",
       "             ('come', 338),\n",
       "             ('more', 494),\n",
       "             ('pacing', 26),\n",
       "             ('straight', 104),\n",
       "             ('seemingly', 26),\n",
       "             ('bound', 52),\n",
       "             ('dive', 26),\n",
       "             ('strange', 182),\n",
       "             ('will', 260),\n",
       "             ('content', 52),\n",
       "             ('them', 442),\n",
       "             ('extremest', 26),\n",
       "             ('limit', 26),\n",
       "             ('loitering', 26),\n",
       "             ('under', 260),\n",
       "             ('shady', 26),\n",
       "             ('lee', 26),\n",
       "             ('yonder', 52),\n",
       "             ('not', 1534),\n",
       "             ('suffice', 26),\n",
       "             ('must', 442),\n",
       "             ('just', 390),\n",
       "             ('nigh', 104),\n",
       "             ('possibly', 26),\n",
       "             ('without', 182),\n",
       "             ('falling', 52),\n",
       "             ('miles', 78),\n",
       "             ('leagues', 26),\n",
       "             ('inlanders', 26),\n",
       "             ('lanes', 26),\n",
       "             ('alleys', 26),\n",
       "             ('avenues', 26),\n",
       "             ('north', 52),\n",
       "             ('east', 26),\n",
       "             ('south', 156),\n",
       "             ('west', 26),\n",
       "             ('yet', 416),\n",
       "             ('unite', 26),\n",
       "             ('tell', 442),\n",
       "             ('does', 156),\n",
       "             ('magnetic', 26),\n",
       "             ('virtue', 26),\n",
       "             ('needles', 26),\n",
       "             ('compasses', 26),\n",
       "             ('those', 234),\n",
       "             ('attract', 26),\n",
       "             ('thither', 26),\n",
       "             ('once', 208),\n",
       "             ('say', 286),\n",
       "             ('country', 78),\n",
       "             ('lakes', 26),\n",
       "             ('any', 364),\n",
       "             ('path', 26),\n",
       "             ('please', 52),\n",
       "             ('ten', 52),\n",
       "             ('one', 1300),\n",
       "             ('carries', 26),\n",
       "             ('down', 468),\n",
       "             ('dale', 26),\n",
       "             ('leaves', 52),\n",
       "             ('pool', 26),\n",
       "             ('stream', 78),\n",
       "             ('magic', 52),\n",
       "             ('let', 156),\n",
       "             ('most', 468),\n",
       "             ('absent', 26),\n",
       "             ('minded', 26),\n",
       "             ('be', 1716),\n",
       "             ('plunged', 52),\n",
       "             ('deepest', 26),\n",
       "             ('man', 572),\n",
       "             ('legs', 104),\n",
       "             ('set', 156),\n",
       "             ('feet', 182),\n",
       "             ('going', 260),\n",
       "             ('he', 3273),\n",
       "             ('infallibly', 26),\n",
       "             ('lead', 78),\n",
       "             ('region', 26),\n",
       "             ('should', 286),\n",
       "             ('ever', 338),\n",
       "             ('athirst', 26),\n",
       "             ('great', 376),\n",
       "             ('american', 78),\n",
       "             ('desert', 26),\n",
       "             ('try', 104),\n",
       "             ('experiment', 26),\n",
       "             ('caravan', 26),\n",
       "             ('happen', 26),\n",
       "             ('supplied', 26),\n",
       "             ('metaphysical', 52),\n",
       "             ('professor', 26),\n",
       "             ('yes', 104),\n",
       "             ('knows', 26),\n",
       "             ('meditation', 26),\n",
       "             ('wedded', 26),\n",
       "             ('artist', 78),\n",
       "             ('desires', 26),\n",
       "             ('paint', 26),\n",
       "             ('dreamiest', 26),\n",
       "             ('shadiest', 26),\n",
       "             ('quietest', 26),\n",
       "             ('enchanting', 26),\n",
       "             ('bit', 130),\n",
       "             ('romantic', 26),\n",
       "             ('landscape', 26),\n",
       "             ('valley', 26),\n",
       "             ('saco', 26),\n",
       "             ('chief', 52),\n",
       "             ('element', 26),\n",
       "             ('employs', 26),\n",
       "             ('trees', 26),\n",
       "             ('each', 78),\n",
       "             ('hollow', 26),\n",
       "             ('trunk', 52),\n",
       "             ('hermit', 26),\n",
       "             ('crucifix', 26),\n",
       "             ('within', 130),\n",
       "             ('sleeps', 26),\n",
       "             ('meadow', 52),\n",
       "             ('sleep', 416),\n",
       "             ('cattle', 26),\n",
       "             ('cottage', 26),\n",
       "             ('goes', 78),\n",
       "             ('sleepy', 26),\n",
       "             ('smoke', 52),\n",
       "             ('deep', 78),\n",
       "             ('distant', 78),\n",
       "             ('woodlands', 26),\n",
       "             ('winds', 78),\n",
       "             ('mazy', 26),\n",
       "             ('reaching', 52),\n",
       "             ('overlapping', 26),\n",
       "             ('spurs', 26),\n",
       "             ('mountains', 26),\n",
       "             ('bathed', 26),\n",
       "             ('hill', 52),\n",
       "             ('side', 286),\n",
       "             ('blue', 78),\n",
       "             ('though', 546),\n",
       "             ('picture', 130),\n",
       "             ('lies', 26),\n",
       "             ('thus', 52),\n",
       "             ('tranced', 26),\n",
       "             ('pine', 52),\n",
       "             ('tree', 26),\n",
       "             ('shakes', 26),\n",
       "             ('sighs', 52),\n",
       "             ('shepherd', 52),\n",
       "             ('head', 624),\n",
       "             ('vain', 26),\n",
       "             ('unless', 104),\n",
       "             ('eye', 26),\n",
       "             ('him', 1092),\n",
       "             ('visit', 26),\n",
       "             ('prairies', 26),\n",
       "             ('june', 52),\n",
       "             ('when', 650),\n",
       "             ('scores', 52),\n",
       "             ('wade', 26),\n",
       "             ('knee', 26),\n",
       "             ('among', 78),\n",
       "             ('tiger', 26),\n",
       "             ('lilies', 26),\n",
       "             ('charm', 26),\n",
       "             ('wanting', 26),\n",
       "             ('drop', 26),\n",
       "             ('niagara', 26),\n",
       "             ('cataract', 26),\n",
       "             ('sand', 26),\n",
       "             ('travel', 26),\n",
       "             ('thousand', 52),\n",
       "             ('why', 286),\n",
       "             ('did', 572),\n",
       "             ('poor', 104),\n",
       "             ('poet', 26),\n",
       "             ('tennessee', 26),\n",
       "             ('suddenly', 78),\n",
       "             ('receiving', 26),\n",
       "             ('two', 338),\n",
       "             ('handfuls', 26),\n",
       "             ('silver', 52),\n",
       "             ('deliberate', 26),\n",
       "             ('whether', 182),\n",
       "             ('buy', 26),\n",
       "             ('coat', 104),\n",
       "             ('sadly', 52),\n",
       "             ('needed', 26),\n",
       "             ('invest', 26),\n",
       "             ('pedestrian', 26),\n",
       "             ('trip', 26),\n",
       "             ('rockaway', 26),\n",
       "             ('beach', 26),\n",
       "             ('robust', 52),\n",
       "             ('healthy', 52),\n",
       "             ('boy', 52),\n",
       "             ('crazy', 52),\n",
       "             ('first', 494),\n",
       "             ('voyage', 208),\n",
       "             ('passenger', 104),\n",
       "             ('yourself', 156),\n",
       "             ('feel', 78),\n",
       "             ('mystical', 26),\n",
       "             ('vibration', 26),\n",
       "             ('told', 130),\n",
       "             ('old', 754),\n",
       "             ('persians', 26),\n",
       "             ('hold', 52),\n",
       "             ('holy', 52),\n",
       "             ('greeks', 26),\n",
       "             ('give', 208),\n",
       "             ('separate', 26),\n",
       "             ('deity', 26),\n",
       "             ('own', 286),\n",
       "             ('brother', 52),\n",
       "             ('jove', 26),\n",
       "             ('surely', 26),\n",
       "             ('meaning', 78),\n",
       "             ('deeper', 26),\n",
       "             ('story', 130),\n",
       "             ('narcissus', 26),\n",
       "             ('who', 416),\n",
       "             ('because', 182),\n",
       "             ('could', 650),\n",
       "             ('grasp', 26),\n",
       "             ('tormenting', 26),\n",
       "             ('mild', 26),\n",
       "             ('image', 156),\n",
       "             ('saw', 156),\n",
       "             ('fountain', 26),\n",
       "             ('was', 2886),\n",
       "             ('drowned', 26),\n",
       "             ('we', 286),\n",
       "             ('ourselves', 52),\n",
       "             ('rivers', 26),\n",
       "             ('oceans', 26),\n",
       "             ('ungraspable', 26),\n",
       "             ('phantom', 78),\n",
       "             ('life', 78),\n",
       "             ('key', 26),\n",
       "             ('am', 156),\n",
       "             ('habit', 26),\n",
       "             ('begin', 52),\n",
       "             ('grow', 52),\n",
       "             ('hazy', 26),\n",
       "             ('eyes', 182),\n",
       "             ('conscious', 26),\n",
       "             ('lungs', 26),\n",
       "             ('mean', 130),\n",
       "             ('inferred', 52),\n",
       "             ('needs', 52),\n",
       "             ('rag', 26),\n",
       "             ('something', 312),\n",
       "             ('besides', 156),\n",
       "             ('passengers', 78),\n",
       "             ('sick', 26),\n",
       "             ('quarrelsome', 26),\n",
       "             (\"don't\", 52),\n",
       "             ('nights', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('themselves', 52),\n",
       "             ('much', 442),\n",
       "             ('general', 26),\n",
       "             ('thing', 130),\n",
       "             (';--', 104),\n",
       "             ('nor', 78),\n",
       "             ('salt', 26),\n",
       "             ('commodore', 52),\n",
       "             ('captain', 52),\n",
       "             ('cook', 78),\n",
       "             ('abandon', 26),\n",
       "             ('glory', 52),\n",
       "             ('distinction', 26),\n",
       "             ('offices', 26),\n",
       "             ('abominate', 26),\n",
       "             ('honourable', 26),\n",
       "             ('respectable', 26),\n",
       "             ('toils', 26),\n",
       "             ('trials', 26),\n",
       "             ('tribulations', 26),\n",
       "             ('kind', 78),\n",
       "             ('whatsoever', 52),\n",
       "             ('quite', 78),\n",
       "             ('care', 78),\n",
       "             ('taking', 104),\n",
       "             ('barques', 26),\n",
       "             ('brigs', 26),\n",
       "             ('schooners', 26),\n",
       "             (',--', 130),\n",
       "             ('confess', 52),\n",
       "             ('considerable', 26),\n",
       "             ('being', 390),\n",
       "             ('sort', 494),\n",
       "             ('officer', 52),\n",
       "             ('board', 78),\n",
       "             ('somehow', 78),\n",
       "             ('fancied', 26),\n",
       "             ('broiling', 26),\n",
       "             ('fowls', 26),\n",
       "             ('broiled', 78),\n",
       "             ('judiciously', 26),\n",
       "             ('buttered', 26),\n",
       "             ('judgmatically', 26),\n",
       "             ('salted', 26),\n",
       "             ('peppered', 26),\n",
       "             ('speak', 130),\n",
       "             ('respectfully', 52),\n",
       "             ('reverentially', 26),\n",
       "             ('fowl', 26),\n",
       "             ('than', 390),\n",
       "             ('idolatrous', 26),\n",
       "             ('dotings', 26),\n",
       "             ('egyptians', 26),\n",
       "             ('ibis', 26),\n",
       "             ('roasted', 26),\n",
       "             ('river', 26),\n",
       "             ('horse', 52),\n",
       "             ('mummies', 26),\n",
       "             ('creatures', 26),\n",
       "             ('huge', 52),\n",
       "             ('bake', 26),\n",
       "             ('houses', 52),\n",
       "             ('pyramids', 26),\n",
       "             ('simple', 26),\n",
       "             ('sailor', 156),\n",
       "             ('mast', 78),\n",
       "             ('plumb', 26),\n",
       "             ('forecastle', 52),\n",
       "             ('royal', 26),\n",
       "             ('true', 104),\n",
       "             ('rather', 260),\n",
       "             ('order', 130),\n",
       "             ('make', 260),\n",
       "             ('jump', 52),\n",
       "             ('spar', 52),\n",
       "             ('grasshopper', 26),\n",
       "             ('may', 312),\n",
       "             ('unpleasant', 26),\n",
       "             ('enough', 338),\n",
       "             ('touches', 26),\n",
       "             ('sense', 78),\n",
       "             ('honour', 26),\n",
       "             ('particularly', 52),\n",
       "             ('established', 26),\n",
       "             ('family', 26),\n",
       "             ('van', 26),\n",
       "             ('rensselaers', 26),\n",
       "             ('randolphs', 26),\n",
       "             ('hardicanutes', 26),\n",
       "             ('putting', 52),\n",
       "             ('tar', 52),\n",
       "             ('pot', 26),\n",
       "             ('been', 468),\n",
       "             ('lording', 26),\n",
       "             ('schoolmaster', 52),\n",
       "             ('making', 130),\n",
       "             ('tallest', 26),\n",
       "             ('boys', 52),\n",
       "             ('awe', 26),\n",
       "             ('transition', 52),\n",
       "             ('keen', 26),\n",
       "             ('assure', 26),\n",
       "             ('decoction', 26),\n",
       "             ('seneca', 26),\n",
       "             ('stoics', 26),\n",
       "             ('enable', 26),\n",
       "             ('grin', 52),\n",
       "             ('bear', 52),\n",
       "             ('even', 130),\n",
       "             ('wears', 26),\n",
       "             ('hunks', 52),\n",
       "             ('orders', 26),\n",
       "             ('broom', 26),\n",
       "             ('sweep', 52),\n",
       "             ('decks', 26),\n",
       "             ('indignity', 26),\n",
       "             ('amount', 26),\n",
       "             ('weighed', 52),\n",
       "             ('scales', 26),\n",
       "             ('new', 286),\n",
       "             ('testament', 26),\n",
       "             ('think', 182),\n",
       "             ('archangel', 26),\n",
       "             ('gabriel', 26),\n",
       "             ('thinks', 182),\n",
       "             ('anything', 52),\n",
       "             ('less', 52),\n",
       "             ('promptly', 26),\n",
       "             ('obey', 26),\n",
       "             ('instance', 26),\n",
       "             ('ai', 104),\n",
       "             (\"n't\", 624),\n",
       "             ('slave', 26),\n",
       "             ('well', 208),\n",
       "             ('however', 208),\n",
       "             ('captains', 26),\n",
       "             ('thump', 52),\n",
       "             ('punch', 26),\n",
       "             ('satisfaction', 26),\n",
       "             ('knowing', 78),\n",
       "             ('everybody', 26),\n",
       "             ('else', 208),\n",
       "             ('served', 26),\n",
       "             ('either', 78),\n",
       "             ('physical', 26),\n",
       "             ('point', 52),\n",
       "             ('view', 52),\n",
       "             ('so', 1092),\n",
       "             ('universal', 26),\n",
       "             ('passed', 78),\n",
       "             ('hands', 78),\n",
       "             ('rub', 26),\n",
       "             ('shoulder', 26),\n",
       "             ('blades', 26),\n",
       "             ('again', 286),\n",
       "             ('always', 104),\n",
       "             ('paying', 78),\n",
       "             ('trouble', 26),\n",
       "             ('whereas', 26),\n",
       "             ('pay', 78),\n",
       "             ('single', 52),\n",
       "             ('penny', 78),\n",
       "             ('heard', 208),\n",
       "             ('contrary', 26),\n",
       "             ('difference', 52),\n",
       "             ('between', 234),\n",
       "             ('paid', 52),\n",
       "             ('act', 52),\n",
       "             ('perhaps', 130),\n",
       "             ('uncomfortable', 52),\n",
       "             ('infliction', 26),\n",
       "             ('orchard', 26),\n",
       "             ('thieves', 26),\n",
       "             ('entailed', 26),\n",
       "             ('us', 104),\n",
       "             ('compare', 52),\n",
       "             ('urbane', 26),\n",
       "             ('activity', 26),\n",
       "             ('receives', 26),\n",
       "             ('really', 104),\n",
       "             ('marvellous', 104),\n",
       "             ('considering', 26),\n",
       "             ('earnestly', 26),\n",
       "             ('believe', 26),\n",
       "             ('root', 26),\n",
       "             ('earthly', 52),\n",
       "             ('ills', 26),\n",
       "             ('monied', 26),\n",
       "             ('enter', 52),\n",
       "             ('heaven', 104),\n",
       "             ('ah', 26),\n",
       "             ('cheerfully', 26),\n",
       "             ('consign', 26),\n",
       "             ('perdition', 26),\n",
       "             ('finally', 26),\n",
       "             ('wholesome', 26),\n",
       "             ('exercise', 26),\n",
       "             ('pure', 26),\n",
       "             ('air', 104),\n",
       "             ('fore', 26),\n",
       "             ('castle', 26),\n",
       "             ('deck', 52),\n",
       "             ('far', 104),\n",
       "             ('prevalent', 26),\n",
       "             ('astern', 26),\n",
       "             ('violate', 26),\n",
       "             ('pythagorean', 26),\n",
       "             ('maxim', 26),\n",
       "             ('quarter', 52),\n",
       "             ('gets', 26),\n",
       "             ('atmosphere', 26),\n",
       "             ('second', 104),\n",
       "             ('sailors', 78),\n",
       "             ('breathes', 26),\n",
       "             ('commonalty', 26),\n",
       "             ('leaders', 52),\n",
       "             ('many', 104),\n",
       "             ('things', 130),\n",
       "             ('suspect', 26),\n",
       "             ('wherefore', 26),\n",
       "             ('after', 234),\n",
       "             ('repeatedly', 26),\n",
       "             ('smelt', 52),\n",
       "             ('merchant', 26),\n",
       "             ('whaling', 234),\n",
       "             ('invisible', 26),\n",
       "             ('police', 26),\n",
       "             ('fates', 52),\n",
       "             ('has', 104),\n",
       "             ('constant', 26),\n",
       "             ('surveillance', 26),\n",
       "             ('secretly', 26),\n",
       "             ('dogs', 26),\n",
       "             ('influences', 26),\n",
       "             ('unaccountable', 104),\n",
       "             ('answer', 130),\n",
       "             ('doubtless', 52),\n",
       "             ('formed', 52),\n",
       "             ('grand', 104),\n",
       "             ('programme', 26),\n",
       "             ('providence', 26),\n",
       "             ('drawn', 26),\n",
       "             ('came', 286),\n",
       "             ('brief', 26),\n",
       "             ('interlude', 26),\n",
       "             ('solo', 26),\n",
       "             ('extensive', 26),\n",
       "             ('performances', 26),\n",
       "             ('bill', 26),\n",
       "             ('run', 52),\n",
       "             ('contested', 26),\n",
       "             ('election', 26),\n",
       "             ('presidency', 26),\n",
       "             ('united', 26),\n",
       "             ('states', 26),\n",
       "             ('bloody', 26),\n",
       "             ('battle', 26),\n",
       "             ('affghanistan', 26),\n",
       "             ('exactly', 78),\n",
       "             ('stage', 52),\n",
       "             ('managers', 26),\n",
       "             ('put', 208),\n",
       "             ('shabby', 52),\n",
       "             ('others', 52),\n",
       "             ('magnificent', 26),\n",
       "             ('parts', 130),\n",
       "             ('tragedies', 26),\n",
       "             ('short', 78),\n",
       "             ('easy', 78),\n",
       "             ('genteel', 26),\n",
       "             ('comedies', 26),\n",
       "             ('jolly', 104),\n",
       "             ('farces', 26),\n",
       "             ('recall', 26),\n",
       "             ('circumstances', 52),\n",
       "             ('springs', 26),\n",
       "             ('motives', 52),\n",
       "             ('cunningly', 26),\n",
       "             ('presented', 26),\n",
       "             ('various', 52),\n",
       "             ('disguises', 26),\n",
       "             ('induced', 26),\n",
       "             ('performing', 26),\n",
       "             ('cajoling', 26),\n",
       "             ('delusion', 26),\n",
       "             ('choice', 26),\n",
       "             ('resulting', 26),\n",
       "             ('unbiased', 26),\n",
       "             ('freewill', 26),\n",
       "             ('discriminating', 26),\n",
       "             ('judgment', 26),\n",
       "             ('overwhelming', 26),\n",
       "             ('idea', 182),\n",
       "             ('whale', 260),\n",
       "             ('portentous', 78),\n",
       "             ('mysterious', 52),\n",
       "             ('monster', 26),\n",
       "             ('roused', 26),\n",
       "             ('curiosity', 52),\n",
       "             ('wild', 130),\n",
       "             ('seas', 156),\n",
       "             ('rolled', 156),\n",
       "             ('island', 78),\n",
       "             ('bulk', 26),\n",
       "             ('undeliverable', 26),\n",
       "             ('nameless', 78),\n",
       "             ('perils', 26),\n",
       "             ('attending', 26),\n",
       "             ('marvels', 26),\n",
       "             ('patagonian', 26),\n",
       "             ('sights', 26),\n",
       "             ('sounds', 78),\n",
       "             ('helped', 26),\n",
       "             ('sway', 26),\n",
       "             ('wish', 26),\n",
       "             ('inducements', 26),\n",
       "             ('tormented', 52),\n",
       "             ('everlasting', 52),\n",
       "             ('itch', 26),\n",
       "             ('remote', 26),\n",
       "             ('love', 26),\n",
       "             ('forbidden', 26),\n",
       "             ('barbarous', 26),\n",
       "             ('coasts', 26),\n",
       "             ('ignoring', 26),\n",
       "             ('good', 442),\n",
       "             ('quick', 26),\n",
       "             ('perceive', 26),\n",
       "             ('horror', 26),\n",
       "             ('social', 26),\n",
       "             ('since', 78),\n",
       "             ('friendly', 26),\n",
       "             ('terms', 26),\n",
       "             ('inmates', 26),\n",
       "             ('place', 390),\n",
       "             ('lodges', 26),\n",
       "             ('reason', 130),\n",
       "             ('welcome', 26),\n",
       "             ('flood', 26),\n",
       "             ('gates', 26),\n",
       "             ('wonder', 52),\n",
       "             ('swung', 26),\n",
       "             ('open', 104),\n",
       "             ('conceits', 26),\n",
       "             ('swayed', 26),\n",
       "             ('purpose', 52),\n",
       "             ('floated', 52),\n",
       "             ('inmost', 26),\n",
       "             ('endless', 26),\n",
       "             ('processions', 26),\n",
       "             ('mid', 26),\n",
       "             ('hooded', 26),\n",
       "             ('snow', 78),\n",
       "             ('stuffed', 52),\n",
       "             ('shirt', 104),\n",
       "             ('carpet', 26),\n",
       "             ('bag', 182),\n",
       "             ('tucked', 26),\n",
       "             ('arm', 286),\n",
       "             ('started', 26),\n",
       "             ('cape', 104),\n",
       "             ('horn', 52),\n",
       "             ('pacific', 26),\n",
       "             ('quitting', 26),\n",
       "             ('manhatto', 26),\n",
       "             ('duly', 26),\n",
       "             ('arrived', 52),\n",
       "             ('bedford', 104),\n",
       "             ('saturday', 78),\n",
       "             ('night', 624),\n",
       "             ('december', 26),\n",
       "             ('disappointed', 26),\n",
       "             ('learning', 26),\n",
       "             ('packet', 26),\n",
       "             ('nantucket', 182),\n",
       "             ('had', 858),\n",
       "             ('already', 26),\n",
       "             ('sailed', 26),\n",
       "             ('offer', 52),\n",
       "             ('till', 156),\n",
       "             ('following', 52),\n",
       "             ('monday', 26),\n",
       "             ('young', 130),\n",
       "             ('candidates', 26),\n",
       "             ('pains', 26),\n",
       "             ('penalties', 26),\n",
       "             ('stop', 208),\n",
       "             ('embark', 52),\n",
       "             ('related', 26),\n",
       "             ('doing', 26),\n",
       "             ('made', 338),\n",
       "             ('craft', 130),\n",
       "             ('fine', 104),\n",
       "             ('boisterous', 26),\n",
       "             ('everything', 26),\n",
       "             ('connected', 26),\n",
       "             ('famous', 26),\n",
       "             ('amazingly', 26),\n",
       "             ('pleased', 52),\n",
       "             ('late', 182),\n",
       "             ('gradually', 26),\n",
       "             ('monopolising', 26),\n",
       "             ('business', 130),\n",
       "             ('matter', 78),\n",
       "             ('behind', 26),\n",
       "             ('original', 52),\n",
       "             ('tyre', 26),\n",
       "             ('carthage', 26),\n",
       "             ('dead', 130),\n",
       "             ('stranded', 52),\n",
       "             ('aboriginal', 26),\n",
       "             ('whalemen', 26),\n",
       "             ('red', 78),\n",
       "             ('sally', 26),\n",
       "             ('canoes', 26),\n",
       "             ('chase', 26),\n",
       "             ('leviathan', 52),\n",
       "             ('too', 364),\n",
       "             ('adventurous', 26),\n",
       "             ('sloop', 26),\n",
       "             ('forth', 26),\n",
       "             ('partly', 78),\n",
       "             ('laden', 26),\n",
       "             ('imported', 26),\n",
       "             ('cobblestones', 26),\n",
       "             ('throw', 26),\n",
       "             ('whales', 52),\n",
       "             ('discover', 26),\n",
       "             ('risk', 26),\n",
       "             ('harpoon', 135),\n",
       "             ('bowsprit', 26),\n",
       "             ('day', 156),\n",
       "             ('another', 130),\n",
       "             ('ere', 78),\n",
       "             ('destined', 26),\n",
       "             ('port', 26),\n",
       "             ('became', 78),\n",
       "             ('concernment', 26),\n",
       "             ('eat', 26),\n",
       "             ('meanwhile', 78),\n",
       "             ('dubious', 26),\n",
       "             ('nay', 52),\n",
       "             ('dark', 208),\n",
       "             ('dismal', 52),\n",
       "             ('bitingly', 26),\n",
       "             ('cold', 182),\n",
       "             ('cheerless', 26),\n",
       "             ('anxious', 26),\n",
       "             ('grapnels', 26),\n",
       "             ('sounded', 26),\n",
       "             ('pocket', 78),\n",
       "             ('only', 364),\n",
       "             ('brought', 26),\n",
       "             ('pieces', 26),\n",
       "             ('wherever', 52),\n",
       "             ('said', 494),\n",
       "             ('stood', 260),\n",
       "             ('middle', 130),\n",
       "             ('dreary', 52),\n",
       "             ('shouldering', 26),\n",
       "             ('comparing', 26),\n",
       "             ('gloom', 26),\n",
       "             ('darkness', 78),\n",
       "             ('wisdom', 26),\n",
       "             ('conclude', 26),\n",
       "             ('lodge', 26),\n",
       "             ('dear', 26),\n",
       "             ('sure', 130),\n",
       "             ('inquire', 26),\n",
       "             ('price', 26),\n",
       "             ('halting', 26),\n",
       "             ('steps', 26),\n",
       "             ('paced', 26),\n",
       "             ('sign', 156),\n",
       "             ('crossed', 52),\n",
       "             ('harpoons', 78),\n",
       "             ('\"--', 78),\n",
       "             ('looked', 156),\n",
       "             ('expensive', 52),\n",
       "             ('further', 104),\n",
       "             ('bright', 52),\n",
       "             ('windows', 52),\n",
       "             ('fish', 104),\n",
       "             ('inn', 104),\n",
       "             ('fervent', 26),\n",
       "             ('rays', 26),\n",
       "             ('seemed', 416),\n",
       "             ('melted', 26),\n",
       "             ('packed', 52),\n",
       "             ('ice', 104),\n",
       "             ('house', 286),\n",
       "             ('everywhere', 26),\n",
       "             ('congealed', 26),\n",
       "             ('frost', 104),\n",
       "             ('lay', 234),\n",
       "             ('inches', 52),\n",
       "             ('thick', 52),\n",
       "             ...])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want this to be a matrix\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,   14,  265, ..., 2704,   14,   24],\n",
       "       [  14,  265,   51, ...,   14,   24,  965],\n",
       "       [ 265,   51,  263, ...,   24,  965,    5],\n",
       "       ...,\n",
       "       [ 960,   12,  168, ...,  264,   53,    2],\n",
       "       [  12,  168, 2703, ...,   53,    2, 2709],\n",
       "       [ 168, 2703,    3, ...,    2, 2709,   26]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,   14,  265, ...,    6, 2704,   14],\n",
       "       [  14,  265,   51, ..., 2704,   14,   24],\n",
       "       [ 265,   51,  263, ...,   14,   24,  965],\n",
       "       ...,\n",
       "       [ 960,   12,  168, ...,   11,  264,   53],\n",
       "       [  12,  168, 2703, ...,  264,   53,    2],\n",
       "       [ 168, 2703,    3, ...,   53,    2, 2709]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24,  965,    5, ...,    2, 2709,   26])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11368, 25)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size,seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # turns positive integers into dense vectors of a fixed size\n",
    "    model.add(Embedding(vocabulary_size,seq_len,input_length=seq_len))\n",
    "    # Common to make this some multiple of your sequence length: seq_len*10 = units = #neurons\n",
    "    model.add(LSTM(units=seq_len*10,return_sequences=True))\n",
    "    model.add(LSTM(units=seq_len*10))\n",
    "    model.add(Dense(units=seq_len*10, activation='relu'))\n",
    "    \n",
    "    # output of network\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mbair/anaconda3/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            67750     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 250)           276000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 250)               501000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2710)              680210    \n",
      "=================================================================\n",
      "Total params: 1,587,710\n",
      "Trainable params: 1,587,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mbair/anaconda3/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
