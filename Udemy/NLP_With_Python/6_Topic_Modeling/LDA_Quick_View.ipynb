{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "npr = pd.read_csv('npr.csv')\n",
    "npr.head()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# discard words that appear in 90% of the document\n",
    "# only include the word if it appears in at least 2 documents\n",
    "# use 'english' stop_words\n",
    "cv = CountVectorizer(max_df=0.9,min_df=2,stop_words='english')\n",
    "\n",
    "dtm = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Grab the vocab words & Step 2: Grab the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=7, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Use n_components as the number of topics to be identified\n",
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "# This may take a while\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '00000', '000s', '000th', '002', '004', '007', '009', '00s']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this holds an instance of every word in each document\n",
    "cv.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Grab the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n",
       "        1.43006821e-01, 1.42902042e-01, 1.42861626e-01],\n",
       "       [2.76191749e+01, 5.36394437e+02, 1.42857148e-01, ...,\n",
       "        1.42861973e-01, 1.42857147e-01, 1.42906875e-01],\n",
       "       [7.22783888e+00, 8.24033986e+02, 1.42857148e-01, ...,\n",
       "        6.14236247e+00, 2.14061364e+00, 1.42923753e-01],\n",
       "       ...,\n",
       "       [3.11488651e+00, 3.50409655e+02, 1.42857147e-01, ...,\n",
       "        1.42859912e-01, 1.42857146e-01, 1.42866614e-01],\n",
       "       [4.61486388e+01, 5.14408600e+01, 3.14281373e+00, ...,\n",
       "        1.43107628e-01, 1.43902481e-01, 2.14271779e+00],\n",
       "       [4.93991422e-01, 4.18841042e+02, 1.42857151e-01, ...,\n",
       "        1.42857146e-01, 1.43760101e-01, 1.42866201e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This holds the topics:\n",
    "display(LDA.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Step 1 and 2:\n",
    "> See topics and their top words (not necessary for computation, but helpful for dictating subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['says', 'said', 'health', 'people', 'care', 'million', 'company', 'government', 'percent', 'new', '000', 'federal', 'year', 'money', 'companies']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['said', 'trump', 'president', 'police', 'told', 'people', 'news', 'says', 'reports', 'npr', 'government', 'russia', 'security', 'house', 'military']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['says', 'like', 'people', 'just', 'food', 'years', 'new', 'city', 'water', 'time', 'day', 'home', 'family', 'world', 'way']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['says', 'people', 'health', 'women', 'like', 'study', 'children', 'just', 'patients', 'disease', 'medical', 'years', 'don', 'new', 'time']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['trump', 'said', 'clinton', 'president', 'state', 'people', 'campaign', 'republican', 'court', 'obama', 'new', 'party', 'election', 'vote', 'voters']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['like', 'just', 'people', 'think', 'know', 'time', 'really', 'music', 'way', 'new', 'don', 'life', 've', 'going', 'years']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['says', 'school', 'students', 'like', 'new', 'education', 'just', 'schools', 'time', 'people', 'university', 'science', 'data', 'years', 'student']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]][::-1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Grab the highest probability words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize: Assign the classes/topics to the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "npr.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
