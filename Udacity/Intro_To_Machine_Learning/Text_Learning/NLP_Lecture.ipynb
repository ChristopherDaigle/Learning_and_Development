{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Learning With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words: vector representation of a word counts of a document\n",
    "\n",
    "In SKLearn, we use \"Count Vectorizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"hi Katie the self driving car will be late Best Sebastian\"\n",
    "string2 = \"Hi Sebastian the machine learning class will be great great great Best Katie\"\n",
    "string3 = \"Hi Katie the machine learning class will be most excellent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_list = [string1, string2, string3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what the words in the corpus are and assign list indices to each:\n",
    "bag_of_words = vectorizer.fit(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many counts of each word:\n",
    "bag_of_words = vectorizer.transform(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t3\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words[0])\n",
    "print()\n",
    "print(bag_of_words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first document, document 0, the first word, word 0 == \"hi\", occurs 1 time; ending with the 17th word, word 16 == \"will\", occuring one time\n",
    "\n",
    "In the second document, document 1, the seventh word, word 6 == \"great\", occurs 3 times; ending with the 17th word, word 16 == \"will\", occuring 1 time\n",
    "\n",
    "**(a, b)   x:**\n",
    "* a = # of document\n",
    "* b = # of word\n",
    "* x = frequency of word b in document a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_.get(\"great\"))\n",
    "print()\n",
    "print(vectorizer.vocabulary_.get(\"will\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting stopwords from NLTK\n",
    "* NLTK: National Language Tool Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(sw[0:7])\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "* Showing some kind of root to words\n",
    "> Idea: not all words are equally important, permutations of similar words may be more meaningful (e.g.:repsond, responsive, unresponsive -> respon)\n",
    "\n",
    "Helps to lower the dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'respons'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'respons'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'unrespons'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(stemmer.stem(\"responsiveness\"))\n",
    "display(stemmer.stem(\"responsivity\"))\n",
    "display(stemmer.stem(\"unresponsive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of order of operations when processing text:\n",
    "1. Stemming\n",
    "2. Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency (TFIDF)\n",
    "* TF (Term Frequency) - like bag of words, each term is weighted by how often it occurs in a document\n",
    "> e.g. Word that occurs 10 times has 10 times more weight than a word that appears only once\n",
    "* IDF (Inverse Documnet Frequency): weighting by how often it occurs in the corpus as a whole (all the documents put together)\n",
    "> e.g. rate rare words more highly than common words (think signal VS noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project\n",
    "Construct your own version of that preprocessing step, so that you are going directly from raw data to processed features.\n",
    "\n",
    "You will be given two text files: one contains the locations of all the emails from Sara, the other has emails from Chris. You will also have access to the `parseOutText()` function, which accepts an opened email as an argument and returns a string containing all the (stemmed) words in the email.\n",
    "\n",
    "Start with a warmup exercise to get acquainted with `parseOutText()`. Go to the tools directory and run `parse_out_email_text.py`, which contains parseOutText() and a test email to run this function over.\n",
    "\n",
    "`parseOutText()` takes the opened email and returns only the text part, stripping away any metadata that may occur at the beginning of the email, so what's left is the text of the message. We currently have this script set up so that it will print the text of the email to the screen, what is the text that you get when you run `parseOutText()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code required work to port to python 3\n",
    "import parse_out_email_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without stemming\n",
    "# parse_out_email_text.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "# With stemming\n",
    "parse_out_email_text.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
